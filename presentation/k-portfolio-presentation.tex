% Customized "beamer" class
% Template cloned from https://git.scc.kit.edu/i43/dokumentvorlagen/praesentationen/beamer | commit: 5c6fe51d431425942a350e8a439bb4cef08f0275 (2022-06-28)
% Due to unclear licensing, we do not provide the files "sdqbeamer.cls" and "logos/kitlogo_en_rgb.pdf" (need to be added manually)
\documentclass[en]{sdqbeamer}
% - remove animation roll-out: handout (general "beamer" option, not specific for this class)
% - layout options: 16:9 (default), 16:10, 4:3
% - footer font size options: bigfoot (default), smallfoot (KIT layout)
% - navigation bar options: navbarinline (default), navbarinfooter, navbarside, navbaroff, navbarkit (off + smallfoot)
% - language: de (default), en

\titleimage{}

\grouplogo{}

\groupname{}
%\groupnamewidth{50mm} % default

\title[A Comprehensive Study of k-Portfolios of Recent SAT Solvers]{A Comprehensive Study of k-Portfolios of Recent SAT Solvers} % [footer]{title slide}
\subtitle{SAT 2022 | Haifa, Israel}
\author[\underline{Jakob Bach}, Markus Iser, and Klemens Böhm]{\underline{Jakob Bach}, Markus Iser, and Klemens Böhm} % [footer]{title slide}
\date[2022-08-02]{August 2, 2022} % [footer]{title slide}

%\usepackage{amsmath} % mathematical symbols and equations; apparently pre-loaded
%\usepackage{amssymb} % mathematical symbols; apparently pre-loaded
\usepackage[style=numeric, backend=bibtex]{biblatex}  % original template uses "biber" as backend
%\usepackage{graphicx} % plots; apparently pre-loaded
%\usepackage{hyperref} % links and URLs; apparently pre-loaded
\usepackage{subcaption} % figures with multiple sub-figures; just used for \caption* here

\addbibresource{k-portfolio-presentation.bib}

\setlength{\leftmargini}{0.2cm} % change default identation (so items are left-aligned to boxes)
\setlength{\leftmarginii}{0.3cm} % 2nd level identation
\setlength{\leftmarginiii}{0.3cm} % 3rd level identation

\setbeamercovered{invisible} % use "transparent" to show later content of animated slide in gray
\setbeamertemplate{enumerate items}[default] % do not use the ugly colored circles

\begin{document}

\KITtitleframe

\section{Experiments}

\begin{frame}[t]{Experimental Design}
	\begin{itemize}
		\item Two datasets (from Main Tracks of recent SAT Competitions):
		\begin{enumerate}[1)]
			\item \emph{SC2020} (316 instances, 48 solvers)~\cite{balyo2020proceedings}
			\item \emph{SC2021} (325 instances, 46 solvers)~\cite{balyo2021proceedings}
			%JB: took all solvers, but removed instances not solved any of them
		\end{enumerate}
		\begin{itemize}
			\item 138 features from feature extractor of SATzilla~2012~\cite{xu2012features, xu2012satzilla2012}
			%JB: features from twelve categories, simple ones (like number of instances) to complicated ones (like variable-clause graph node degree)
			%JB: missing values due to timeouts and memouts replaced with out-of-range value
			\item Instance features and solver runtimes retrieved from GBD~\cite{iser2020collaborative}
		\end{itemize}
		\pause
		\vspace{\baselineskip}
		\item Four solution approaches:
		%JB: all of them run for both datasets and all k (from 1 to number of solvers)
		\begin{itemize}
			\item \emph{Optimal solution} via MIP solving~\cite{python-mip}
			%JB: that's the exact approach, others are heuristics
			%JB: package "mip" uses the solver "COIN-OR branch-and-cut" (Cbc) internally
			%JB: Cbc was very fast, mean runtime of 15 s, max ~5 min
			\item \emph{Beam search} with beam width $w \in \{1, 2, 3, \dots, 10, 20, 30, \dots, 100\}$
			\item \emph{K-best}
			\item \emph{Random sampling} with 1000 repetitions
		\end{itemize}
		\pause
		\vspace{\baselineskip}
		\item Two multi-class prediction models: Random forests~\cite{breiman2001random, scikit-learn} and XGBoost~\cite{xgboost} with 100 trees each
		%JB: ensemble tree models: powerful and can learn non-linear dependencies (RFs also used in SATzilla 2012)
		%JB: in preliminary experiments, also tried other models (e.g., kNN, untuned neural network) -> worse performance
		%JB: in preliminary experiments, also regression, instance-weighted classififcation, one-vs-one classification -> worse performance
	\end{itemize}
\end{frame}

\begin{frame}[t]{Results -- Portfolio Search (VBS on Training Set)}
	\begin{figure}[htb]
		\centering
		\includegraphics[width=\textwidth]{plots/search-train-objective.pdf}
		\caption*{Training-set VBS performance for different datasets, values of $k$, and portfolio-search approaches.}
		%JB: conducted five-fold cross-validation; portfolios search and model training on training set only
		%JB: figures are average over CV folds, in case of random sampling also over sampling repetitions
		%JB: all approaches: strong decrease with k; optimal 10-portfolio only 25% (SC2020) / 17% (SC2021) worse than portfolio out of all solvers
		%JB: greedy search is beam search with k=1; already very close to optimum (though problem NP-complete); slight improvement by increasing w
		%JB: also looked at composition of optimal portfolio and found that usually just 1 or 2 solvers added from k to k+1, which is nice scenario for greedy search
		%JB: upper bound follows from submodularity; far away from greedy search
		%JB: k-best between greedy and random; depends on datasets to which closer; gaps widens after first few k
	\end{figure}
\end{frame}

\begin{frame}[t]{Results -- Portfolio Search (VBS on Test Set)}
	\begin{figure}[htb]
		\centering
		\includegraphics[width=\textwidth]{plots/search-test-objective.pdf}
		\caption*{Test-set VBS performance for different datasets, values of $k$, and portfolio-search approaches.}
		%JB: overall, rather similar trends as on training set
		%JB: k-best closer to greedy/optimal than on training set; there might be slight overfitting (best potrfolio on train instances not necessarily best on test instances)
	\end{figure}
\end{frame}

\begin{frame}[t]{Results -- Recommending Solvers (MCC)}
	\begin{figure}[htb]
		\centering
		\includegraphics[width=\textwidth]{plots/prediction-test-mcc.pdf}
		\caption*{Test-set prediction performance in terms of Matthews correlation coefficient (MCC)~\cite{matthews1975comparison,gorodkin2004comparing} for different datasets, values of $k$, and prediction models. Randomly sampled portfolios.}
		%JB: that's direct way of evaluating predictions; does not consider how bad wrongly recommended solver is; indirect evaluation on next slide
		%JB: MCC is suitable for imbalanced classes; is in [-1, 1]; higher is better; 0 for random guessing and constant prediction
		%JB: random sampling to have many portfolios; prediction performance for beam-search/optimal porfolios similar
		%JB: training-set MCC close to 1, as typical for unpruned tree models (overfitting)
		%JB: prediction performance not good, but better than random guessing (apparently there is some information in features)
		%JB: slightly larger MCC and clearly more variation for small k
		%JB: random forest and XGBoost similar; focus on former in the following
	\end{figure}
\end{frame}

\begin{frame}[t]{Results -- Recommending Solvers (PAR-2 Score)}
	\begin{figure}[htb]
		\centering
		\includegraphics[width=\textwidth]{plots/prediction-test-objective-beam.pdf}
		\caption*{Test-set solver performance for different datasets, values of $k$, and solver-recommendation approaches. Portfolios from \emph{beam search} with $w=100$. Random forests for predictions.}
		%JB: beam search with w=10o to predict for a bunch of good portfolios (similar resutls for optimal portfolios)
		%JB: VBS corresponds to optimal prediction
		%JB: SBS corresponds to sensible baseline (best constant prediction), though results could even be worse
		%JB: horizontal line is global SBS, "SBS" boxes are portfolio-specific
		%JB: while VBS decreases, predictions cannot leverage new solvers (decrease from k=2 to k=3 or k=4, but not beyond)
		%JB: at least score of predicted solver (for k > 2) better than (global, and thus also portfolio) SBS
	\end{figure}
\end{frame}

\section{Summary}

\begin{frame}[t]{Summary and Future Work}
	\begin{itemize}
		\item Evaluated solver portfolios on data from SAT Competitions 2020 and 2021
		\pause
		\item Small portfolios already show high runtime improvement
		\pause
		\item Greedy portfolio search already close to optimal portfolio
		\pause
		\item Our prediction approach does not benefit from increased portfolio size
		\pause
		\vspace{\baselineskip}
		\item Exemplary directions for future work:
		\begin{itemize}
			\item Improve prediction performance, e.g., by using new features like community-based ones~\cite{Ansotegui:2019:CommunityStructure, Li:2021:HCS}
			%JB: did not perform deep analysis of feature importance, but RF's built-in one spread over many features (no single or small set of very useful features)
			%JB: features might also be redundant (which, however, should not impact prediction performance of tree-based models negatively)
			\item Use more specialized solvers and solver configurations
			%JB: while the solvers submitted to SAT Competition's Main Track rather are general-purpose solvers
			\item Compare to sophisticated portfolio approaches like SATzilla~\cite{xu2008satzilla, xu2012satzilla2012}
			%JB: our focus was not on creating best overall portfolio approach, but analyzing portfolio construction/size, while we kept prediction approach simple
		\end{itemize}
	\end{itemize}
\end{frame}

\appendix
\beginbackup % subsequent slides do not impact overall slide count

\begin{frame}[t, allowframebreaks]{References}
	\printbibliography
\end{frame}

\backupend

\end{document}
