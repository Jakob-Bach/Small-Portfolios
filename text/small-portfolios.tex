\documentclass[runningheads]{llncs}

%\usepackage[style=ieee, backend=bibtex]{biblatex} % handled by LNCS template at the moment
\usepackage[ruled,linesnumbered,vlined]{algorithm2e} % pseudo-code
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
%\usepackage{amsthm} % mathematical theorems; handled by LNCS template at the moment
\usepackage{graphicx} % plots
\usepackage[misc]{ifsym} % letter symbol to mark corresponding author
\usepackage{xcolor} % for link coloring as advised by LNCS template
\usepackage{hyperref} % links and URLs; should be loaded last

%\addbibresource{references.bib} % handled by LNCS template at the moment

%\newtheorem{example}{Example} % also defined in LNCS template

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\renewcommand\UrlFont{\color{blue}\rmfamily} % URL formatting as advised by LNCS template

\begin{document}

\title{A Comprehensive Study of k-Portfolios\\ of Recent SAT Solvers}

%\titlerunning{Abbreviated paper title} % currently not necessary, but possible in LNCS template

\author{
	Jakob Bach\orcidID{0000-0003-0301-2798} (\Letter) \and
	Markus Iser\orcidID{0000-0003-2904-232X} \and
	Klemens BÃ¶hm
}

\authorrunning{J. Bach et al.} % should use "et al." for more than two authors

\institute{
	Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany\\
	\email{\{jakob.bach,markus.iser,klemens.boehm\}@kit.edu}
}

\maketitle

\begin{abstract}
Hard combinatorial problems such as propositional satisfiability are ubiquitous. 
The holy grail are methods that show good performance on all problem instances. 
However, new approaches emerge regularly, some of which are complementary to such solvers in that they only run faster on some instances, but not on many others. 
While portfolios, i.e., sets of solvers, have been touted as useful, putting together such portfolios also needs to be efficient. 
In particular, it remains an open question how well portfolios can exploit the complementarity of existing solvers. 
This paper features a comprehensive analysis of portfolios of recent SAT solvers, the ones from the SAT Competitions~2020 and~2021. 
We determine optimal portfolios with exact and approximate approaches and study the impact of portfolio size $k$ on performance. 
We also investigate how effective off-the-shelf prediction models are for instance-specific solver recommendations.
One result is that the portfolios found with an approximate approach are as good as the optimal solution in practice. 
We also observe that marginal returns decrease very quickly with larger $k$, and our prediction models do not give way to better performance beyond very small portfolio sizes. 

\keywords{Propositional satisfiability \and Solver portfolios \and Runtime prediction \and Machine Learning}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\subsubsection{Motivation}

SAT solving is the archetypal NP-complete problem.
Its practical applications abound, e.g., verification of hardware and software~\cite{Kaufmann:2021:Amulet,Buning:2020:QPRVerify}, product configuration~\cite{Janota:2014:Configuration}, cryptanalysis~\cite{Nejati:2020:CDCLCrypto}, or planning~\cite{Schreiber:2021:Lilotane}.
SAT solvers are not only used to solve hard combinatorial problems in industry, but also previously open problems in mathematics~\cite{Heule:2016:Pyth,Heule:2018:Schur}. 

Nowadays, one can observe continuous progress regarding SAT solving methods, heuristics, and their implementations in SAT solvers. 
Evaluation of solvers is commonly based on compilations of benchmark instances which represent diverse interesting application scenarios. 
An important design objective behind new SAT solvers is stability, i.e., good performance on many types of instances. 
At the same time, we observe a recurrent emergence of new heuristics and methods which improve performance on only a narrow subset of instances. 
Such approaches have a merit nevertheless -- one can see them as complementary to solvers with good average behavior. 

One way to leverage complementarity of solvers is with portfolios, i.e., sets of solvers.
One can combine a solver portfolio with a perfect oracle which then selects the fastest solver from the portfolio for each instance in the benchmark set. 
This construction is commonly referred to as the \emph{Virtual Best Solver} (VBS). 
One can then derive a ranking of solvers based on their marginal contribution to the VBS~\cite{Xu:2012:EvalContribVBS}. 

\begin{example}[Special Price in SAT Competition 2021]
In the SAT Competition 2021, the performance of the VBS of all portfolios of size two over the entirety of participating solvers has been evaluated. 
The best pair of solvers in terms of their VBS contains the solver \emph{CaDiCaL\_PriPro}.
This solver has not been competitive in the overall evaluation.
But due to that achievement, it has been awarded the new \emph{Special Innovation Price}.
\end{example}

Yet another tweak when it comes to performance gains from sets of solvers is as follows:
Combining a portfolio with a solver-selection process that includes runtime predictions usually outperforms individual best solvers by much~\cite{xu2008satzilla}. 
However, the relative performance gains of such approaches on instances in the very diverse \emph{application} category are usually much smaller than the gains in the categories of \emph{crafted} or \emph{randomly generated} instances~\cite{Xu:2012:EvalContribVBS,Collautti:2013:SNNAP}. 
We argue that evaluations confined to only a few, very specific sets of instances, such as the one in \cite{Kadioglu:2010:ISAC}, let the solvers appear in a better light than ones covering various domains. 
Moreover, the construction of a prediction-based solver-selection process is complex and quite time-consuming.
In consequence, more often than not, comparative studies in SAT solving do not include implementations applying this principle to state-of-the-art solvers and recent benchmarks of interest.

Given the circumstances sketched so far, the following questions are important when it comes to the design of portfolios and their evaluation. 
First, one must decide how many solvers to put in a portfolio, and which ones. 
The \emph{K-Portfolio Problem} is to determine an optimal $k$-portfolio (cf.\ Section~\ref{sec:approach}). 
Searching for a solution to the \emph{K-Portfolio Problem} is an NP-complete optimization problem~\cite{nof2020real}. 
Second, one must decide how to select a solver for each instance. 
In particular, this includes the choice of instance features used to learn a prediction model as well as of the model type. 

\subsubsection{Contributions}

This current article features a study which captures and evaluates the state-of-the-art in SAT solving in terms of portfolio performance. 
Our study is unique in that it addresses all points raised so far, as follows:
We systematically construct $k$-portfolios and analyze the impact of the portfolio size $k$ on portfolio performance. 
For exact search, we encode the \emph{K-Portfolio Problem} as an integer optimization problem (which is faster than alternative approaches, according to preliminary experiments of ours). 
Spurred by the complexity of the problem, we also analyze the quality of various approximate and random solutions. 

In a subsequent step, we analyze the quality of solver runtime prediction which one can achieve with off-the-shelf models and instance features -- without any tuning.
For each portfolio size $k$, we train random forests and boosted trees as prediction models, using the widely known instance feature records of SATzilla~2012~\cite{xu2012satzilla2012}, and then use the models to select portfolio members automatically for each instance. 

Our study is based on very recent \emph{competition} datasets which contain the runtimes of solvers having participated in SAT~Competitions~2020 and~2021. 
The two sets of benchmarks in these SAT Competitions are a balanced mix of families of interest which have been drawn from the application and crafted instance categories. 
Our code and data are available online (cf. Section~\ref{sec:experimental-design:impl}).

\subsubsection{Results}

Our evaluation yields insightful results regarding the various questions. 
While it is expected that the marginal utility decreases with portfolio size, we did not expect the decrease to be so early. 
Portfolios of size five already achieve more than 70\% of the maximum possible performance gain over the single best solver, so marginal performance gains from adding single additional solvers become very small in each case. 
Another interesting observation is that well-known \emph{beam search} has given way to good approximate solutions to the \emph{K-Portfolio Problem}. 
These solutions happen to perform very similarly to the optimal ones, even for a minimal beam width of one.

Relying on an off-the-shelf prediction model has been a mixed bag. 
On the negative side, prediction performance has been somewhat low. 
The actual runtime of portfolios with prediction models is not even close to the theoretical improvement one could have with a perfect oracle.
We take this as an indication that more research is necessary; our hypothesis is that the instance features available in our dataset are not sufficiently discriminative. 
On the positive side, we have observed considerable performance gains over single-best solver performance with runtime prediction for $3$-portfolios in both competition datasets. 
For the solvers in the SAT Competition~2021 dataset, we observe a further gain in performance for $k = 4$. 
For larger $k$, the runtimes of portfolios with prediction models do not improve further. 
We take this as a further indication that it might not be beneficial to have many solvers in a portfolio. 

\subsubsection{Outline}

In Section~\ref{sec:related-work}, we review related work. 
Preliminaries are given in Section~\ref{sec:preliminaries}. 
In Section~\ref{sec:approach}, we introduce the \emph{K-Portfolio Problem} and several solution approaches. 
We describe the experimental design in Section~\ref{sec:experimental-design} and present the experimental results in Section~\ref{sec:evaluation}. 
We conclude with Section~\ref{sec:conclusion}. 

\section{Related Work}
\label{sec:related-work}

In this section, we provide an overview of approaches that deal with portfolios of SAT solvers. 
We begin with approaches using portfolios to evaluate the state-of-the-art of SAT solving and continue with instance-specific solver selection, including approaches to automatically configure and analyze such techniques. 
Finally, we discuss related work on systematic investigation of $k$-portfolios. 

Solver complementarity regarding the runtime of solver portfolios has been examined by Xu et al.~\cite{Xu:2012:EvalContribVBS}. 
They show that significant contributions to the runtime of a portfolio often come from solvers which are not competitive when evaluated as a standalone solver. 
This however depends on the dataset under evaluation. 
In fact, the best performing $2$-portfolio of solvers in SAT~Competition~2020 are the winners of the SAT and UNSAT Tracks.
This is the expected result for a benchmark set which contains similar amounts of SAT and UNSAT instances. 
Results have been markedly different in SAT~Competition~2021, in which an otherwise not competitive solver is part of the best performing $2$-portfolio. 
All this underlines the importance of portfolio analysis for a complete evaluation of the state-of-the-art. 
This in turn can also support the development of better stable solvers.

Instance-specific solver-selection approaches bring in another perspective on solver complementarity.
One of the best-known complex solver-selection approaches is SATzilla, of which multiple versions exist~\cite{xu2008satzilla,xu2012satzilla2012}. 
In SATZilla, multiple prediction models guide the selection of a solver from their portfolio. 
Additional components of SATzilla are so-called \emph{pre-solvers}, which are only run for a short time to solve easy instances, and a \emph{backup solver}, which is responsible for instances where computing features for the prediction models takes too long.
Other instance-specific solver selection approaches are ISAC, which is based on clustering in the instance feature space~\cite{Kadioglu:2010:ISAC}, and SNNAP, which combines the ISAC approach with runtime prediction models~\cite{Collautti:2013:SNNAP}.
A recent survey on algorithm-selection approaches -- also beyond SAT -- is provided by Kerschke et al.~\cite{kerschke2019automated}.

Many portfolio approaches have several stages and configuration options, which makes configuring and selecting the best portfolio approach difficult.
To cope with this, Autofolio by Lindauer et al.\ provides an automatic configurator~\cite{lindauer2015autofolio}.
One can also analyze the impact of configuration options with tools such as CAVE~\cite{biedenkapp2018cave}.

A systematic analysis of portfolios with varying size~$k$, which has some similarities to our study, is conducted by Amadini et al.~\cite{amadini2014empirical,amadini2016extensive}.
They evaluate $k$-portfolios of CSP solvers in terms of their average runtime and solved instances. 
They compare several classification methods within portfolios, next to adapting complex SAT portfolio approaches like SATzilla. 
Amadini et al.\ focus on different instances and solvers than this current study, though the evaluation procedure has some similarities.
While they only use heuristic search algorithms, our study features a broad comparison of different search strategies for portfolios, including an \emph{exact} solution, two \emph{approximate} solutions, and \emph{random sampling}. 

Nof and Strichman formalize the \emph{K-Portfolio Problem} in the form of two maximization problems with different objective functions and prove their submodularity and NP-completeness~\cite{nof2020real}. 
They solve the \emph{K-Portfolio Problem} optimally with an SMT solver and use \emph{greedy search} to generate approximate solutions.
They evaluate their approach on anytime algorithms for an allocation problem, focusing on solution quality after a timeout of one second or less. 
While we build on the theoretical results of Nof and Strichman, our evaluation is broader. 
In addition to including more solution approaches, we also analyze portfolios with prediction models.

Our study is based on two most recent datasets of the SAT~Competitions~2020  and~2021~\cite{balyo2020proceedings,balyo2021proceedings}. 
They contain runtimes of actual state-of-the-art SAT solvers for a very diverse set of hard SAT instances, which are measured with a timeout of 5000~seconds.

\section{Preliminaries}
\label{sec:preliminaries}

Let a set of solvers $S = \{s_1, \dots, s_n\}$, a set of SAT instances $I = \{i_1, \dots, i_l\}$, and solver runtimes $r: I \times S \rightarrow [0, T]$ with a fixed timeout $T$ be given.
A scoring function $c_T: S \rightarrow \mathbb{R}$ estimates solver performance. 
To score a solver, we use the popular penalized-average-runtime measure with a penalization factor of two (PAR-2 Score). 
This score is a trade-off between solver runtime and the number of solved instances.
It is defined as follows:%
\begin{align}
r_T(i,s) &:= \begin{cases}
	2 \cdot T & \text{if }r(i,s) = T\\
	r(i,s) & \text{otherwise}
\end{cases} \tag*{Penalized Runtimes}\\[.5em]
c_T(s) &:= \frac{1}{|I|} \sum_{i \in I}{r_T(i,s)} \tag*{PAR-2 Score}
\end{align}

A portfolio $P \subseteq S$ is a non-empty set of solvers.
To score a solver portfolio $P$, we assume to have an oracle that always selects the fastest solver for each instance. 
This construction is commonly referred to as the virtual best solver (VBS) for $P$. 
Accordingly, we extend the scoring function as follows:%
$$
	c_{T}(P) := \frac{1}{|I|} \cdot \sum\limits_{i \in I}{\min\{r_T(i,s) \mid s \in P\}}
$$
In the following, we refer to $c_{T}(P)$ as the \emph{cost} of $P$. 

In reality, one may train a prediction model $m : I \rightarrow P$ for a solver portfolio $P$ that recommends a solver for each instance, using features of the instance. 
The cost of such a prediction model is given by the following function:%
$$
	c_{T}(m,P) := \frac{1}{|I|} \cdot \sum\limits_{i \in I}{r_T(m(i),i)}
$$

Clearly, the portfolio cost $c_{T}(P)$ is a lower bound for the actual cost of a portfolio $P$ which uses a prediction model.

\section{Optimal \texorpdfstring{$k$}{k}-Portfolios} % hyperref does not want a math symbol here
\label{sec:approach}

The \emph{K-Portfolio Problem} is to find a portfolio $P$ of size $|P| = k$ with minimum cost:%
$$
\argmin\limits_{P \subseteq S, |P| = k} c_{T}(P)
$$
Note that the portfolio cost function decreases monotonically under the addition of solvers: $\forall s \in S, c_{T}(P \cup \{s\}) \leq c_{T}(P)$. 
In the following, we outline three solution approaches, an exact one based on \emph{integer programming} and two approximate methods, namely \emph{beam search} and \emph{k-best}. 

\subsection{Integer Programming}
\label{sec:approach:ip}

The \emph{K-Portfolio Problem} is not linear due to the use of the $\min$ function.
However, one can make it an integer linear problem by introducing additional variables.
This allows to obtain an exact solution for the problem with an off-the-shelf integer-programming solver.

We introduce two sets of binary decision variables. 
The binary variables $y_s$ indicate whether a solver $s \in S$ is in the portfolio, and 
the binary variables $x_{i,s}$ indicate whether solver $s \in S$ is selected for instance $i \in I$. 
Equation~\ref{eq:ip1} specifies the cardinality constraint on the number of solvers. 
Equation~\ref{eq:ip2} stipulates that exactly one solver is chosen for each instance. 
Equation~\ref{eq:ip3} ensures that a solver can only be chosen for an instance if it is part of the portfolio. 
Ultimately, Equation~\ref{eq:ip4} specifies the optimization target.%
\begin{align}
	\sum_{s \in S} y_s &\leq k \label{eq:ip1}\\
	\forall i\in I:~\sum_{s \in S} x_{i,s} &= 1 \label{eq:ip2}\\
	\forall s \in S:~\sum_{i \in I} x_{i,s} &\leq |I| \cdot y_s \label{eq:ip3}\\
	\min_{x,y} \quad & \frac{1}{|I|} \cdot \sum_{i \in I} \sum_{s \in S} r_T(i,s) \cdot x_{i,s} \label{eq:ip4}
\end{align}

\subsection{Beam Search}
\label{sec:approach:beam}
%
\begin{algorithm}[htb]
	\DontPrintSemicolon
	\KwIn{Solvers $S$, Portfolio size $k$, Portfolio cost $c_T$}
	\KwIn{Beam width $w$}
	\KwOut{Portfolio $P$ with $|P|=k$}
	\BlankLine
	$T_0 \leftarrow \{\emptyset\}$\;
	\For{$i \leftarrow 1$ \KwTo $k$}{
		\tcp{Candidate $i$-portfolios:}
		$U \leftarrow \emptyset$\;
		\ForEach{$P \in T_{i-1}$}{
			\ForEach{$s \in S \setminus P$}{
				$U \leftarrow U \cup \{ P \cup \{ s \} \}$\;
			}
		}
		\tcp{Select $w$ best $i$-portfolios:}
		$T_i \leftarrow \emptyset$\;
		\For{$j \leftarrow 1$ \KwTo $w$}{
			$T_i \leftarrow T_i \cup \{\argmin\limits_{P \in U \setminus T_i}{c_T(P)}\}$\;
		}
	}
	\Return $\argmin\limits_{P \in T_k}{c_{T}(P)}$\;
	\caption{\emph{Beam search} algorithm}
	\label{al:beam-search}
\end{algorithm}%
%
\emph{Beam search} is an approximate method that finds portfolios in an iterative manner.
Algorithm~\ref{al:beam-search} specifies the approach. 
In each iteration, the algorithm combines the portfolios from the previous iteration with individual solvers which are not part of these portfolios. 
In other words, the algorithm expands existing portfolios by adding individual solvers. 
Before the next iteration, only the $w$ portfolios with the lowest cost are retained.
The beam width $w$ is an input parameter.
For $w=1$, only one portfolio remains at the end of each iteration.
We refer to this special case as \emph{greedy search}.
For reasonably small $w$, \emph{beam search} has a clear runtime advantage compared to an exhaustive search over all $k$-portfolios.
In particular, \emph{beam search} only evaluates $O(|S| \cdot w)$ out of $\binom{|S|}{k}$ possible portfolios per iteration.

Though the algorithm does not necessarily find the optimal solution to the \emph{K-Portfolio Problem}, there is a bound on the cost of a portfolio found by \emph{greedy search}. 
To this end, one can use a result from~\cite{nemhauser1978analysis}, which applies to greedy algorithms on non-negative monotone submodular set functions.
Nof and Strichman show that their \emph{K-Algorithms Max-Sum Problem} for portfolios is submodular, and thus a bound on greedy algorithms holds~\cite{nof2020real}. 
We can transform the \emph{K-Portfolio Problem} (minimization) into the \emph{K-Algorithms Max-Sum Problem} (maximization) by replacing the cost with utility as follows:%
$$
u_{T}(P) := c_W - c_{T}(P)
$$
In this transformation, $c_W$ is an upper bound on portfolio cost, the single worst solver:%
$$
c_W := \max\{c_T(s) \mid s \in S\}
$$
As $u_{T}(P)$ is non-negative, monotone, and submodular, we get a lower bound on the utility of a portfolio found by \emph{greedy search} $P_k^{greedy}$~\cite{nemhauser1978analysis,krause2014submodular}:%
$$
	u_{T}(P_k^{greedy}) \geq (1 - \frac{1}{e}) \cdot \max_{|P| \leq k}{u_{T}(P)}
$$
This can be transformed into an upper bound on the cost of a portfolio found by \emph{greedy search}:%
\begin{equation}
	c_{T}(P_k^{greedy}) \leq (1 - \frac{1}{e}) \cdot \min_{|P| \leq k}{c_{T}(P)} + \frac{1}{e} \cdot c_W
	\label{eq:upper-bound}
\end{equation}

\subsection{K-Best}
\label{sec:approach:k-best}

This is a baseline used in~\cite{nof2020real} and is one of the most obvious naive approaches. 
It sorts all solvers by their individual performance and then picks the top $k$ from this list.
Unlike \emph{beam search}, it does not consider how solvers within a portfolio interact, i.e., if they complement each other.

\section{Experimental Design}
\label{sec:experimental-design}

In our experiments, we evaluate the exact and approximate solution approaches just presented.
Also, we combine the found $k$-portfolios with prediction models for instance-specific solver selection. 
In both cases, we are interested in the influence of the portfolio size $k$ on performance. 

For evaluation purposes, we conduct five-fold cross-validation over SAT instances.
This means we only use SAT instances from the training folds to search for $k$-portfolios and to subsequently train prediction models.
We compute all evaluation metrics on training instances as well as on test instances.
We average evaluation metrics over the cross-validation folds.

\subsection{Solution Approaches}

We employ the three solution approaches from Section~\ref{sec:approach} to determine $k$-portfolios for each $k \in \{1, \dots, |S|\}$. 
We also generate an additional baseline via random sampling of $k$-portfolios.

\begin{itemize}
	\item \emph{Optimal solution}:
	We solve the \emph{K-Portfolio Problem} as an integer optimization problem to exactly determine the best portfolio.
	\item \emph{Beam search}: 
	We use this approach to search for good portfolios heuristically with varying beam widths $w \in \{1, 2, 3, \dots, 10, 20, 30, \dots, 100\}$.
	\item \emph{K-best}:
	We use this approach to have a simple baseline for \emph{beam search}.
	\item \emph{Random sampling}:
	To get an idea how the performance of arbitrary portfolios is distributed, we randomly sample 1000 portfolios for each $k$.
\end{itemize}

The optimization goal for all approaches is the PAR-2 score $c_T(P)$.
In preliminary experiments, we also analyzed two slightly different objectives, namely the number of unsolved instances and the PAR-2 score normalized for each instance.
However, general trends in the results were similar to those with PAR-2 score, so we stick to the latter.

\subsection{Prediction Approaches}

We also analyze the performance of the previously determined $k$-portfolios with prediction models to select a solver, rather than choosing the VBS. 
For each instance, the prediction target is the best solver out of the given $k$-portfolio.
As model types, we leverage two powerful ensemble methods.
First, we use random forests~\cite{breiman2001random}, which are also part of the well-known portfolio approach SATzilla2012~\cite{xu2012satzilla2012}.
Second, we use XGBoost~\cite{xgboost}, a popular implementation of gradient boosting.
In both cases, we train models with 100 trees.

In preliminary experiments, we also tried two adaptations of the prediction approach that are part of SATzilla2012:
First, one can train a classifier for each pair of solvers, instead of training one classifier that makes a multi-class prediction.
Second, one can weight instances based on the runtime difference between solvers, instead of using an unweighted training set.
However, none of these changes has helped to improve prediction performance in our preliminary experiments.

We evaluate prediction performance in two ways:

\begin{itemize}
	\item \emph{Objective value}:
	We evaluate the prediction models with respect to the objective value, i.e., the runtime cost function $c_{T}(m,P)$ (cf. Section~\ref{sec:preliminaries}). 
	\item \emph{MCC}:
	We evaluate the predictions with Matthews correlation coefficient (MCC)~\cite{matthews1975comparison,gorodkin2004comparing}.
	This does not take into account how fast the recommended solvers actually are, but only if the fastest solver is recommended or not.
	We use MCC instead of simpler metrics like accuracy, as the class labels might be imbalanced, i.e., one solver might be the fastest for most of the instances, and always predicting this solver would already yield a high accuracy.
	MCC has a range of $[-1,1]$. 
	The value zero occurs with both random guessing and always guessing the same solver.
\end{itemize}

\subsection{Dataset}

In our experiments, we use two datasets. 
The first one, \texttt{SC2020}, contains the runtime data of $48$ solvers on $400$ instances from the Main Track of SAT~Competition~2020~\cite{balyo2020proceedings,SC2020:AIJ}.
The second dataset, \texttt{SC2021}, contains the runtime data of $46$ solvers on $400$ instances from the Main Track of SAT~Competition~2021. 
In both datasets, we filtered out those instances where no solver finished in time, such that runtimes for $316$ instances remained in \texttt{SC2020} and for $325$ instances in \texttt{SC2021}.

For predictions, we use $138$ features to characterize instances, which are from the feature extractor of SATzilla~2012~\cite{xu2008satzilla,xu2012satzilla2012}. 
Feature values that are missing due to the feature extractor having exceeded time- or memory limits are replaced with a constant value that is out of the range of the features.
In both datasets, the number of features is relatively large compared to the number of instances.
However, the prediction models in our experiments are tree-based and therefore implicitly select features during their training.
Further, these kinds of models are not affected by monotonic transformations of features, which makes the experimental results more robust.

\subsection{Implementation}
\label{sec:experimental-design:impl}

We implement our experimental design in Python and make our code available online\footnote{\url{https://github.com/Jakob-Bach/Small-Portfolios}}.
The code also allows to download and prepare the datasets.
Additionally, we publish the full experimental data, including results\footnote{\url{https://bwsyncandshare.kit.edu/s/yKtJ34KTyqBtcJn}; will be moved to a public repository after review}.
To create the datasets, we use the package~\emph{gbd-tools}~\cite{iser2020collaborative}.
For predictions, we use the package \emph{scikit-learn}~\cite{scikit-learn}.
To solve the \emph{K-Portfolio Problem} exactly, we use the package \emph{mip}~\cite{python-mip}.

\section{Evaluation}
\label{sec:evaluation}

We evaluate the solution approaches for the \emph{K-Portfolio Problem} first, and the use of prediction models for solver recommendation second.

\subsection{Optimization Results}

The datasets seem promising for portfolios, as in both datasets, there is no single solver which is fastest for all or even for a majority of the instances.
For the $316$ instances in \texttt{SC2020}, the three overall fastest solvers win on only 46, 38, and 26 instances, respectively.
For the $325$ instances in \texttt{SC2021}, the three overall fastest solvers win on only 25, 22, and 20 instances, respectively.
This indicates that combining solvers in portfolios can improve overall runtime.

\subsubsection{General Trend}

\begin{figure}[htb]
	\centering
	\includegraphics[width=\columnwidth]{plots/search-train-objective.pdf}
	\caption{Training-set VBS performance of $k$-portfolios determined by different approaches for the \texttt{SC2020} dataset (left) and the \texttt{SC2021} dataset (right)}
	\label{fig:search-train-objective}
\end{figure}

Figure~\ref{fig:search-train-objective} displays the cost in terms of the PAR-2 score of the best $k$-portfolios for the different solution approaches. 
The \emph{optimal solution} is the exact optimum.
\emph{Greedy search} denotes \emph{beam search} with the smallest beam width $w=1$. 
\emph{K-best} stands for portfolios comprised of the top $k$ single-best solvers. 
For \emph{random sampling}, we average over repeated samples.
The \emph{upper bound} limits the cost of \emph{greedy search} according to Equation~\ref{eq:upper-bound}.
If we report numbers for the solution approaches in the following, we refer to the training set, where all solution approaches conduct their search.

For all approaches and both datasets, the PAR-2 score improves rapidly for the first few $k$, but marginal gains become smaller with increasing $k$.
For the dataset \texttt{SC2020}, the optimal $1$-portfolio has a penalized runtime $5.51$ times as high as the $48$-portfolio.
This ratio reduces to $1.89$ for the best $5$-portfolio and $1.25$ for the best $10$-portfolio.
For \texttt{SC2021}, the respective ratios are $4.74$ for $k=1$, $1.58$ for $k=5$ and $1.17$ for $k=10$.

\subsubsection{Beam Search}

Figure~\ref{fig:search-train-objective} also shows the cost of the best $k$-portfolios found by \emph{greedy search} to be very close to that of the \emph{optimal solution}.
These results are remarkable considering that the runtime of \emph{greedy search} is linear in $k$ as well as the total number of solvers $n$, while finding the \emph{optimal solution} is NP-complete.
In contrast, the theoretical, submodularity-based \emph{upper bound} for \emph{greedy search} clearly is higher and too loose to serve as a good estimate.

To bring the \emph{beam search} solution even closer to the \emph{optimal solution}, one can increase $w$.
For example, the best $2$-portfolio found by \emph{greedy search} has a $14.7\%$ higher cost than the \emph{optimal solution} for \texttt{SC2020}.
For all other $k$, the \emph{greedy search} portfolio has less than $4\%$ higher cost than the \emph{optimal solution}.
In comparison, for $w=10$ and for all $k$, cost is never more than $2\%$ higher than for the \emph{optimal solution}.

Regarding the set of the $w$ best portfolios in each iteration of \emph{beam search}, we observe a convergence of portfolio performance with increasing $k$. 
There is a large variance in the PAR-2 score of portfolios in the beam for small $k$, and this variance becomes smaller in later iterations, i.e., the top $w$ portfolios become more similar in performance.
A similar phenomenon also occurs for \emph{random sampling} portfolios:
With increasing $k$, the standard deviation of the PAR-2 score in a sample of portfolios decreases.
The expected value of portfolio performance improves as well.
Thus, carefully selecting the solvers for a portfolio matters most for small $k$.

\subsubsection{K-Best}

The baseline \emph{k-best} is worse than \emph{greedy search} on the training set, but better than \emph{random sampling}.
While \emph{greedy search} always is quite close to the \emph{optimal solution}, the performance gap between \emph{k-best} and the \emph{optimal solution} widens after the first few $k$ and only becomes smaller for large $k$ later.
The performance of \emph{k-best} relative to the other approaches also differs between \texttt{SC2020} and \texttt{SC2021}.
For \texttt{SC2020}, \emph{k-best} is closer to the optimal solution, while for \texttt{SC2021}, \emph{k-best} is closer to the expected value of \emph{random sampling}.
This indicates that building a portfolio of complementary solvers, rather than picking the best individual solvers, may be more important for \texttt{SC2021}. 

\subsubsection{Test-Set Performance}

\begin{figure}[htb]
	\centering
	\includegraphics[width=\columnwidth]{plots/search-test-objective.pdf}
	\caption{Test-set VBS performance of $k$-portfolios determined by different approaches for the \texttt{SC2020} dataset (left) and the \texttt{SC2021} dataset (right)}
	\label{fig:search-test-objective}
\end{figure}

Figure~\ref{fig:search-test-objective} shows test-set portfolio cost for the different solution approaches.
Here, we take the portfolios found on the training set, but evaluate them with the test-set instances.
The overall trends are the same as on the training set in Figure~\ref{fig:search-train-objective}.
A notable exception is that \emph{beam search}, \emph{k-best}, and the \emph{optimal solution} show similar performance on the test set of \texttt{SC2020}.
In particular, there is no clear winner, and the \emph{optimal solution} can even perform worse than those found by the approximate approaches. 
This is because the best portfolio on the training set is not necessarily the best portfolio on the test set.
For the test set of \texttt{SC2021}, \emph{k-best} is markedly worse than \emph{beam search} and the \emph{optimal solution}. 
The larger performance gap already on the training set might have caused this effect.

\subsubsection{Portfolio Composition}

While \emph{beam search} adds solvers iteratively, the \emph{optimal solution} might differ in more than one solver from $k-1$ to $k$, i.e., existing solvers from the portfolio can be replaced.
Indeed, we observe this phenomenon in our results.
For example, the optimal $2$-portfolios do not contain the optimal $1$-portfolios for both datasets.
However, this non-monotonous behavior is not strong.
On average, one or two new solvers become part of the optimal portfolio when increasing $k$ by one.
I.e., zero or one solver are replaced on average when increasing $k$ by one.
In addition, the replaced solver might only be slightly worse than its substitute.
This might explain the good performance of \emph{beam search}, which pursues a monotonous approach when building portfolios.

\subsubsection{Impact of Single Solvers on Portfolios}

We have carried out a correlation analysis on the 1000 portfolios which are the result of \emph{random sampling} for a certain value of $k$. 
First, we encode the absence or presence of each solver in a portfolio with 0 or 1 respectively.
Next, we compute the Spearman rank correlation between this occurrence vector and the PAR-2 score.
Our analysis highlights the interaction between solvers. 
For $k=5$, all correlations are in $[-0.40,0.17]$ for \texttt{SC2020}, and $[-0.24,0.23]$ for \texttt{SC2021}.
The mean correlation is zero in both cases.
Similar correlation behavior occurs for other $k$.

These results indicate that only the presence of some solvers has a moderately negative correlation to the PAR-2 score of the whole portfolio, i.e., only some solvers can improve the portfolio performance of our minimization problem on their own.
This effect is stronger for \texttt{SC2021}.
The positive correlations are even weaker, i.e., there are no solvers which influence portfolio performance in a strongly negative manner.
Overall, this means that the influence of single solvers on portfolio performance is limited; one needs a combination of solvers to clearly influence the PAR-2 score in either direction.

\subsection{Prediction Results}

In this section, we evaluate the prediction performance for instance-specific solver recommendations in the portfolios found earlier. 
We also evaluate the portfolio performance, i.e., the PAR-2 score of the predicted solvers.

\subsubsection{Matthews Correlation Coefficient}

\begin{figure}[htb]
	\centering
	\includegraphics[width=\columnwidth]{plots/prediction-test-mcc.pdf}
	\caption{
		Prediction performance (test-set MCC) for randomly sampled portfolios, using random forests and XGBoost as models, for the \texttt{SC2020} dataset (left) and the \texttt{SC2021} dataset (right)
	}
	\label{fig:prediction-test-mcc}
\end{figure}

Figure~\ref{fig:prediction-test-mcc} graphs the test-set classification performance of random forests and XGBoost, using the portfolios from \emph{random sampling}.
We use \emph{random sampling} results here, to show the variation of prediction performance over many portfolios.
However, classification results are similar for \emph{beam search} with $w=100$ and for the \emph{optimal solution}.
We do not display training-set performance, since it is close to 1.0.

As Figure~\ref{fig:prediction-test-mcc} shows, test-set prediction performance is rather low for all $k$ and for both types of prediction models.
Prediction performance is clearly higher than with random guessing, which would yield an MCC of 0.0, but clearly lower than the optimal MCC of 1.0.
With a nearly perfect training-set performance, but a low test set performance, the models seem to overfit to the training set.
This could cause a bad \mbox{PAR-2} value for model-based portfolios, and we will analyze this next.
For small $k$, the prediction performance varies stronger between portfolios than for larger $k$, and is slightly higher on average.
As random forests and XGBoost perform very similar, we will only use random-forest results in the following analyses.

\subsubsection{Portfolio Performance}

\begin{figure}[htb]
	\centering
	\includegraphics[width=\columnwidth]{plots/prediction-test-objective-beam.pdf}
	\caption{
		Performance of prediction models, VBS and SBS for \emph{beam search} portfolios with $w=100$ for the \texttt{SC2020} dataset (left) and the \texttt{SC2021} dataset (right).
		The performance of the the global SBS is shown as a horizontal line.}
	\label{fig:prediction-test-objective-beam}
\end{figure}

Figure~\ref{fig:prediction-test-objective-beam} shows test-set PAR-2 scores for portfolios from \emph{beam search} with $w=100$.
The plot compares the PAR-2 score of portfolios with prediction model to two competitors without prediction model, computed on the same portfolios.
The virtual best solver (VBS) provides a lower bound on cost, as its portfolio performance can only be achieved with perfect prediction.
The single best solver (SBS) of each portfolio serves as a baseline, corresponding to always predicting the same solver.
Note that the portfolio performance can even be worse than the SBS, e.g., when always predicting the slowest solver for each instance.
However, we do not show this upper bound on the PAR-2 score here.

As discussed earlier, the VBS score decreases with $k$.
In theory, this also allows portfolios with prediction model to improve their performance.
However, in our case, the \mbox{PAR-2} score of model-based portfolios remains rather stable with increasing values of $k$, with the biggest improvement from $k=2$ to $k=3$.
This implies that the prediction model does not improve its selection of solvers even if the portfolio grows.
Given the low prediction performance in terms of MCC, as seen in Figure~\ref{fig:prediction-test-mcc}, this has been expected.
As a consequence, the gap between VBS and the predicted solvers grows with $k$.
However, model-based portfolios tend to be better than the single best solver from these portfolios, i.e., the prediction models can discriminate between solvers to some extent.
In addition, for $k > 2$ the model-based portfolios are better than the global single-best solver, at least on average.

\begin{figure}[htb]
	\centering
	\includegraphics[width=\columnwidth]{plots/prediction-test-objective-optimal.pdf}
	\caption{
		Performance of prediction models, VBS and SBS for the optimal solution of the $k$-Portfolio Problem for the \texttt{SC2020} dataset (left) and the \texttt{SC2021} dataset (right).
		The performance of the the global SBS is shown as a horizontal line.
	}
	\label{fig:prediction-test-objective-optimal}
\end{figure}

Figure~\ref{fig:prediction-test-objective-optimal} repeats the same comparison as before for the \emph{optimal solution} portfolios.
The overall trends remain the same.
In contrast to Figure~\ref{fig:prediction-test-objective-beam}, here we have only five portfolios for each $k$, one for each fold of cross-validation.
We still see a considerable variance for each $k$, i.e., the test set performance also depends on the current split of instances.

\subsubsection{Feature Importance}

Averaging importance over all trained random-forest models, the most important feature has an importance of roughly 1.66\%, the least important one an importance of roughly 0.01\%.
To reach a cumulated average importance of 50\%, one needs 38 out of 138 features.
On average, random forests use 135 features, i.e., nearly all of them.
We conclude that no single feature or even small set of features drives prediction performance.
Together with the low prediction performance, this indicates that our dataset lacks discriminating features which are suitable to decide which solver should be used on an instance.

\section{Conclusions and Future Work}
\label{sec:conclusion}

Solution methods for the propositional satisfiability problem are an active area of research, with continuous advances in methods, heuristics, and their implementations in SAT solvers. 
New SAT solvers that improve performance only on a few benchmark instances can be assets to portfolios nevertheless. 
In principle, runtime prediction models can help to find out which solvers are complementary. 

This article has been a comparative study of portfolios that goes beyond previous work in several respects.
One feature of our study is that it includes the runtime data of the latest SAT solvers from the SAT~Competitions~2020 and~2021. 
It has addressed the important question of how large portfolios should actually be.
There has been a strong improvement in the objective value for small portfolios, but a small impact once a certain size is reached. 
We also found that \emph{beam search}, an approximate solution approach, yields close-to-optimal solutions.

In a next step, we have combined portfolios with prediction models to recommend solvers specific to problem instances.
However, these models have not performed satisfactorily on the given datasets:
The objective value of portfolios with predictions did not continuously improve with portfolio size.

We see room for improved prediction performance, by adapting the set of instance features in particular,
e.g., by using features describing the community structure of graph representations of SAT instances.
Such new features have correlated well with solver performance on application instances in recent studies~\cite{Ansotegui:2009:StructureIndustrial,Ansotegui:2017:StructureFeatures,Ansotegui:2019:CommunityStructure,Li:2021:HCS}. 
Efficient implementations of feature extractors and studying feature importance on datasets which represent the state-of-the-art are subject to future work. 
In order to facilitate portfolio analysis for evaluation purposes and for ad-hoc portfolio generation, we also want to integrate our exact portfolio-search approach into the Python package~\emph{gbd-tools}, so one can directly use it in queries to the database GBD~\cite{iser2020collaborative}. 

\bibliographystyle{splncs04}
\bibliography{references}

\end{document}
