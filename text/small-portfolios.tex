\documentclass[conference]{IEEEtran}

%\usepackage[style=ieee, backend=bibtex]{biblatex}
\usepackage[linesnumbered,vlined]{algorithm2e} % pseudo-code; use of package discouraged by IEEE, but hacked in in a (hopefully acceptable) way
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{balance} % balance columns on the last page
\usepackage{booktabs} % nicely formatted tables
\usepackage{graphicx} % plots
\usepackage[caption=false,font=footnotesize]{subfig} % figures with multiple sub-figures and sub-captions; use of (newer) package "subcaption" discouraged by IEEE
\usepackage{hyperref} % links and URLs
%\addbibresource{references.bib}

\newtheorem{definition}{Definition}

\begin{document}

\title{Analysis of Optimal Small Portfolios}
%TODO change title, as we do no only analyze optimal portfolios

\author{\IEEEauthorblockN{Jakob Bach}
\IEEEauthorblockA{\textit{KIT Department of Informatics} \\
\textit{Karlsruhe Institute of Technology}\\
Karlsruhe, Germany \\
jakob.bach@kit.edu}
\and
\IEEEauthorblockN{Markus Iser}
\IEEEauthorblockA{\textit{KIT Department of Informatics} \\
\textit{Karlsruhe Institute of Technology}\\
Karlsruhe, Germany \\
markus.iser@kit.edu}
}

\maketitle

\begin{abstract}
Successful approaches that tackle hard combinatorial problems such as propositional satisfiability are often in some sense complementary, i.e., each approach is only the best on some problem instances. 
Parallel portfolios and instance-specific algorithm selection take advantage of this. 
In this paper, we present a systematic analysis of solver portfolios using runtime measurements from the SAT~Competition~2020. 
First, we present and compare search approaches to find theoretically optimal portfolios of limited size. 
Second, we use prediction models to make instance-specific solver recommendations within the portfolios and evaluate this approach as well.
\end{abstract}

\begin{IEEEkeywords}
Propositional Satisfiability, Solver Portfolios, Runtime Prediction Models, Machine Learning
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

\paragraph{Motivation}

SAT solving is the archetypal NP-complete problem and has lots of practical applications, e.g., verification of hard- and software~\cite{Kaufmann:2021:Amulet,Buning:2020:QPRVerify}, product configuration~\cite{Janota:2014:Configuration}, cryptanalysis~\cite{Nejati:2020:CDCLCrypto}, or planning~\cite{Schreiber:2021:Lilotane}. 
Today, SAT solvers are not only used to solve hard combinatorial problems in industry but also to solve previously open problems in mathematics~\cite{Heule:2016:Pyth,Heule:2018:Schur}. 

We can observe continuous progress in SAT solving methods, heuristics, and their implementations in SAT solvers. 
The SAT Competition series, which is organized yearly as in international open event, aims to support and provide further incentives for maintaining this progress~\cite{balyo2020proceedings}. 

Solvers are usually evaluated based on compilations of benchmark instances representing diverse interesting applications scenarios of SAT solvers. 
As it is also common in other areas of computer science, no SAT solving method is the best on all SAT instances. 
In order to create an additional incentive for developing methods that target specific applications, SAT~Competition~2020 introduced an application track where SAT solvers are evaluated on instances stemming from one specific application. 
In~2020, that was the Planning track, and this was followed in~2021 by the Crypto track~\cite{SC2020:AIJ}. 

Another promising approach to decrease runtime is combing solvers to portfolios for instance-specific algorithm selection, as in SATzilla~\cite{xu2008satzilla, xu2012satzilla2012}. 
Within a portfolio, usually one or more prediction models help to decide which solver to use for a particular SAT instance. 
Additional components of a portfolio can be pre-solvers, which are only run for a short time to solve easy instances, or a backup-solver, which is responsible for instances where computing features for the prediction models takes too long.
Overall, portfolio approaches can become quite complex and it becomes unclear which parts of the portfolio solution are really necessary to achieve a good performance.

\paragraph{Problem Statement}

In this paper, we study small portfolios of SAT solvers.
\emph{Small} primarily refers to the number of solvers in the portfolio, which we limit to certain values $k$.
Also, we strive to keep the overall approach simple by only using one prediction model per portfolio, and no further portfolio components. 
Instead, we analyze the impact of the portfolio size $k$. 
In particular, we want to find out whether small portfolios are already sufficient to achieve a good solving performance. 

\paragraph{Contributions}

We empirically analyze the performance of small portfolios on runtime data from the SAT~Competition~2020. 
In particular, our data comprise 48 solvers and 400 instances from the Main track of the competition. 
To find small portfolios, we apply beam search, a heuristic method, as well as an exact search. 
For the latter, we encode the small-portfolio problem as an integer optimization problem. 
In our experiments, we analyze all possible portfolio sizes $k \in \{1, \dots, 48\}$. 
Also, we systematically vary the only hyper-parameter of beam search, the beam width. 
We analyze two variants of the small-portfolio problems, one using penalized solver runtimes and the other one using the number of unsolved instances. 
For evaluation, we consider both the theoretically optimal performance of portfolios as well as the performance if a prediction model makes instance-specific solver recommendations. 

\paragraph{Results}

We observe that adding solvers has diminishing returns, i.e., portfolio performance improves most when adding the first few solvers. 
Additionally, the portfolios found heuristically by beam search have very similar performance as the optimal solution. 
Unfortunately, we find that the prediction models exhibit a rather low prediction performance in our experiments. 
Thus, actual portfolio performance cannot benefit from the theoretical improvement over $k$. 

\paragraph{Outline}

In Section~\ref{sec:related-work}, we review related work. 
In Section~\ref{sec:approach}, we introduce the small-portfolio problem and multiple solution approaches for it. 
We describe the experimental design in Section~\ref{sec:experimental-design} and present the experimental results in Section~\ref{sec:evaluation}. 
We conclude with Section~\ref{sec:conclusion}. 

\section{Related Work}
\label{sec:related-work}

%TODO conduct deeper literature research
%TODO expand a bit, if necessary; currently, description of references is rather short
%TODO add "SAT Competition 2020" paper, which does a lot of analysis on single solvers and some analysis on portfolios; might also cite it in section "Dataset" below

\cite{kerschke2019automated} provides an overview of algorithm selection.
Portfolios of SAT solvers are a sub-category of this.
Many portfolio approaches have several stages and configuration options, which makes configuring and selecting the best portfolio approach difficult.
To this end, \cite{lindauer2015autofolio} provides an automatic configurator.
Also, one can analyze the impact of configuration options, e.g., with the tool CAVE~\cite{biedenkapp2018cave}.

One of the most well-known complex portfolio approaches is SATzilla, of which multiple versions exist~\cite{xu2008satzilla, xu2012satzilla2012}.
Besides complex portfolio approaches, there are also simple ones, e.g., selecting a solver nearest-neighbor-based, without training a prediction model~\cite{malitsky2011non, nikolic2013simple, samulowitz2013snappy}.

\cite{amadini2014empirical, amadini2016extensive} analyze portfolios of CSP solvers in terms of average runtime and solved instances.
Thus, they focus on different instances and solvers than we, though the evaluation procedure has some similarities.
Similar to us, they vary the portfolio size $k$.
They compare several classification methods in portfolios, besides adapting complex SAT portfolio approaches like SATzilla.
In contrast, we focus on one simple classification model, vary its complexity, and analyze feature importance.
Also, we analyze parametrization of beam search.
Further, we compare against random portfolios of size $k$.
Finally, we solve the small-portfolio-problem exactly, while \cite{amadini2014empirical} only use heuristic search algorithms.

\cite{nof2020real} formalizes the small-portfolio problem as maximization problem.
They show submodularity and NP-completeness.
Also, they provide an SMT encoding to exactly solve the problem.
In their evaluation, they use beam search with a beam width of one.
In contrast, we analyze parametrization of beam search.
Further, we compare against random portfolios of size $k$.
Also, \cite{nof2020real} does not analyze portfolios with prediction models.
For the exact solution, we provide an encoding as integer-problem.
Finally, their domain is different than ours, as they focus on instances of an allocation problem whose solution is evaluated already after 0.1 s.
In comparison, we allows 5000 s for solvers on our SAT instances.

\section{Approach}
\label{sec:approach}

First, we introduce the small-portfolio problem.
Next, we present multiple approaches to solve it.

\subsection{Small-Portfolio Problem}

\subsubsection{Given Data}
\label{sec:approach:problem:data}

\begin{align*}
	S &= \{s_1, \dots, s_n\} \tag*{Solvers}\\
	I &= \{i_1, \dots, i_m\} \tag*{Instances}\\
	r &: S \times I \rightarrow \mathbb{N} \cup \{\bot\} \tag*{Runtimes (censored)}\\
	T &\in \mathbb{N} \tag*{Timeout}\\
	r_T(s,i) &:= \begin{cases}
		2*T & \text{if }r(s,i) = \bot\\
		r(s,i) & \text{otherwise}
	\end{cases} \tag*{Penalized Runtimes}
\end{align*}

Let $S$ be a set of solvers and $I$ be a set of SAT instances.
Let $r(s,i)$ be the runtimes of the solvers on the instances.
If a solver encounters an error, e.g., runs out of memory, or takes longer than time $T$ on an instance, the runtime is set to $\bot$ first.
Next, we penalize these missing runtimes with the double timeout, i.e., we use a PAR2 score.
We use these penalized runtimes $r_T(s,i)$ to score the solvers.

\subsubsection{Target Function}
\label{sec:approach:problem:target}

\begin{align*}
	c_{T} &: 2^S \rightarrow \mathbb{N}\\
	c_{T}(P) &:= \begin{cases}
		2*T & \text{if }P=\emptyset\\
		\frac{1}{|I|} * \sum_{i \in I}{\min\{r_T(s,i) \mid s \in P\}} & \text{otherwise}
	\end{cases} \tag*{Portfolio Cost}
\end{align*}

The cost of a solver equals its penalized runtime, averaged over all instances.
A portfolio $P \subseteq S$ is a set of solvers.
To compute the cost of a portfolio $c_{T}(P)$, we assume to have an oracle that always chooses the fastest solver for each instance.
This is the \emph{virtual-best solver} (VBS).

In reality, one may train a prediction model that recommends a solver for each instance.
Such a prediction model uses features of the SAT instances to make its recommendations.
Only if the prediction model always recommends the fastest solver out of $P$ for each instance, the portfolio has the same costs as the VBS for $P$.
Else, the costs are higher, i.e., $c_{T}(P)$ is a lower bound for the actual portfolio costs.
In the worst case, the prediction model always recommends the slowest solver out of $P$ for each instance.
This serves as an upper bound for actual portfolio costs and it the \emph{virtual worst solver} (VWS).

\subsubsection{Optimization Problem}

\begin{equation}
	\label{eq:small-portfolio-problem}
	\begin{aligned}
		\min_P \quad & c_{T}(P)\\
		s.t. \quad & |P| \leq k
	\end{aligned}
   \tag{Small-Portfolio Problem}
\end{equation}

The \ref{eq:small-portfolio-problem} is to find a portfolio with minimum costs that contains at most $k$ solvers.
For each solver out of $S$, one needs to decide whether it becomes part of the portfolio $P$ or not.

\subsection{Solution Approaches}
\label{sec:approach:solution}

\subsubsection{Exhaustive Search}

The simplest way to solve the \ref{eq:small-portfolio-problem} is by exhaustively searching all portfolios with at most $k$ solvers.
Due to the use of a VBS, adding a solver to the portfolio cannot increase costs.
Thus, it is sufficient to search over all portfolios with exactly $k$ solvers, ignoring smaller portfolios.
Still, there are $\binom{n}{k}$ possible portfolios to search.
This can become infeasible, given sufficiently large $n$ and $k \gg 1$ as well as $k \ll n$.
For example, with $n=48$ solvers in our dataset, there are already 1128 portfolios for $k=2$, but roughly $6.54 * 10^9$ portfolios for $k=10$.

\subsubsection{Exact Search: Integer Programming}

The \ref{eq:small-portfolio-problem} is not linear due to the use of the $\min$ function.
However, one can make it an integer linear problem by introducing additional variables.
This allows to obtain an exact solution for the problem with a standard integer-programming solver.

\begin{equation}
	\label{eq:small-portfolio-integer-problem}
	\begin{aligned}
		\min_{x,y} \quad & \frac{1}{|I|} * \sum_{i \in I} \sum_{s \in S} r_T(s,i) * x_{s,i} \\
		s.t. \quad & \begin{aligned}
			\forall_{i\in I} \sum_{s \in S} x_{s,i} &= 1\\
			\forall_{s \in S} \sum_{i \in I} x_{s,i} &\leq |I| * y_s\\
			\sum_{s \in S} y_s &\leq k\\
			\forall_{i\in I} \forall_{s \in S}~x_{s,i} &\in \{0, 1\}\\
			\forall_{s \in S}~y_s &\in \{0,1\}
		\end{aligned}
	\end{aligned}
	\tag{Small-Portfolio Integer Problem}
\end{equation}

There are two sets of decision variables.
$x_{s,i}$ denotes whether solver $s$ is used for instance $i$.
$y_s$ denotes whether a solver is in the portfolio.
Besides the cardinality constraint on the number of solvers, there are two additional constraints.
First, exactly one solver is chosen for each instance.
Second, a solver can only be chosen for an instance if it is part of the portfolio.

\subsubsection{Beam Search}

Beam search is a greedy algorithm that finds portfolios in an iterative manner.
Figure~\ref{al:beam-search} shows the approach:

% IEEE wants algorithms as figures, not separate floats (as "algorithm2e" does), so we need some hacking: https://tex.stackexchange.com/questions/147598/how-to-use-the-algorithm2e-package-with-ieeetran-class
\begin{figure}[t]
\makeatletter
\let\@latex@error\@gobble
\makeatother
\begin{algorithm}[H]
	\KwIn{Penalized runtimes $r_T(s,i)$, portfolio size $k$, beam width $w$}
	\KwOut{Portfolio $P$ with $|P|=k$}
	$\mathbb{P}^0$ $\leftarrow$ \{$\emptyset$\}\tcp*[r]{portfolios of size 0}
	\For{$i \leftarrow 1$ \KwTo $k$}{
		$\mathbb{P}^i \leftarrow \emptyset$\tcp*[r]{portfolios of size $i$}
		\ForEach{$P \in \mathbb{P}^{i-1}$}{
			\ForEach{$s \in S$}{
				\If{$s \notin P$}{
					$\mathbb{P}^i \leftarrow \mathbb{P}^i \cup \{ P \cup \{ s \} \}$\tcp*[r]{add new portfolio}
				}
			}
		}
		$\mathbb{P}^i \leftarrow$ sort($\mathbb{P}_i$, $r_T$)\tcp*[r]{sort portfolios by costs}
		$\mathbb{P}^i \leftarrow \{\mathbb{P}_j^i \in \mathbb{P}^i \mid j \leq w \}$\tcp*[r]{best $w$ portfolios of size $i$}
	}
	\Return $\underset{P \in \mathbb{P}^k}{\arg\min}~c_{T}(P)$\;
\end{algorithm}
\caption{Beam search.}
\label{al:beam-search}
\end{figure}

The algorithm starts with an empty portfolio.
In each iteration, the algorithm creates all combinations of portfolios from the previous iteration and individual solvers which are not part of these portfolios.
I.e., the algorithm expands existing portfolios by adding single solvers.
Before going to the next iteration, only the $w$ portfolios with the lowest cost are retained.
The beam-width $w$ is an input parameter.
For $w=1$, only one portfolio remains at the end of each iteration.
For maximal $w$, all portfolios remain.

The algorithm is a heuristic that does not necessarily find the optimal solution to the \ref{eq:small-portfolio-problem}.
In return, its only needs to check $O(n \cdot w)$ portfolios per iteration, yielding a runtime advantage compared to exhaustive search.
Additionally, we can provide a bound for the costs of a portfolio found by beam search with $w=1$.
To this end, we use a result from \cite{nemhauser1978analysis} for greedy algorithms on non-negative monotone submodular set functions.
In the following, we show that a slightly transformed version of the objective function in the \ref{eq:small-portfolio-problem} fulfills the requirements for the bound.
We start with the following definitions~\cite{krause2014submodular}:

\begin{definition}[Submodular set function]
	A function $f : 2^V \rightarrow \mathbb{R}$ is submodular if for every $A, B \subseteq V$, $f(A \cap B) + f(A \cup B) \leq f(A) + f(B)$.
	\label{def:submodular}
\end{definition}

\begin{definition}[Monotone set function]
	A function $f : 2^V \rightarrow \mathbb{R}$ is monotone if for every $A \subseteq B \subseteq V$, $f(A) \leq f(B)$.
	\label{def:monotone}
\end{definition}

Our cost function $c_{T}: 2^S \rightarrow \mathbb{N}$ is a set function, but it is non-increasing instead of non-decreasing (monotone).
Thus, we define the following transformed version:

\begin{align*}
	u_{T} &: 2^S \rightarrow \mathbb{N}\\
	u_{T}(P) &:= \begin{cases}
		0 & \text{if }P=\emptyset\\
		c_W - c_{T}(P) & \text{otherwise}
	\end{cases} \tag*{Portfolio Utility}
\end{align*}

with the cost of the virtual-worst solver defined as:

\begin{align*}
	c_W := \frac{1}{|I|} * \sum_{i \in I}{\max_{s \in S}{r_T(s,i)}} \tag*{Virtual Worst Solver}
\end{align*}

Our definition of portfolio utility is non-negative, as $c_{T}(P) \leq c_W$.
Additionally, $u_{T}(P)$ is monotone.
In general, for $A \subseteq B \subseteq \mathbb{R}$, it holds that $\min(A) \geq \min(B)$.
For $A \subseteq B \subseteq S$, we obtain $\frac{1}{|I|} * \sum_{i \in I}{\min_{s \in A}{r_T(s,i)}} \geq \frac{1}{|I|} * \sum_{i \in I}{\min_{s \in B}{r_T(s,i)}}$.
Thus, $c_{T}(A) \geq c_{T}(B)$ and therefore $u_{T}(A) \leq u_{T}(B)$. \hfill$\square$

Next, we show that $u_{T}(P)$ is submodular.
Let $A,B \subseteq S$, $A,B \neq \emptyset$ be two portfolios.
For our proof, we split the set of instances $I$ in two subsets.
$I_A := \{i \in I \mid \min_{s \in A}{r_T(s,i)} < \min_{s \in B}{r_T(s,i)} \}$ contains the instances where portfolio A is faster.
$I_B := I \setminus I_A$ contains the remaining instances.
We now show that Definition~\ref{def:submodular} holds:

%TODO Make width narrower and define multi-line format in a way that permits page breaks
%TODO if this is not removed anyway, "\frac{1}{|I|} *" has to be added a lot of times (since we normalized the objective)
\begin{align*}
	& u_{T}(A) + u_{T}(B) - u_{T}(A \cap B) - u_{T}(A \cup B)\\
	= & c_W - c_{T}(A) + c_W - c_{T}(B) - c_W + c_{T}(A \cap B) - c_W + c_{T}(A \cup B)\\
	= & - \sum_{i \in I}{\min_{s \in A}{r_T(s,i)}} - \sum_{i \in I}{\min_{s \in B}{r_T(s,i)}} + \sum_{i \in I}{\min_{s \in A \cap B}{r_T(s,i)}} + \sum_{i \in I}{\min_{s \in A \cup B}{r_T(s,i)}}\\
	= & - \sum_{i \in I_A}{\min_{s \in A}{r_T(s,i)}} - \sum_{i \in I_B}{\min_{s \in A}{r_T(s,i)}} - \sum_{i \in I_A}{\min_{s \in B}{r_T(s,i)}} - \sum_{i \in I_B}{\min_{s \in B}{r_T(s,i)}}\\
	& + \sum_{i \in I_A}{\min_{s \in A \cap B}{r_T(s,i)}} + \sum_{i \in I_B}{\min_{s \in A \cap B}{r_T(s,i)}} + \sum_{i \in I_A}{\min_{s \in A \cup B}{r_T(s,i)}} + \sum_{i \in I_B}{\min_{s \in A \cup B}{r_T(s,i)}}\\
	= & - \sum_{i \in I_B}{\min_{s \in A}{r_T(s,i)}} - \sum_{i \in I_A}{\min_{s \in B}{r_T(s,i)}} + \sum_{i \in I_A}{\min_{s \in A \cap B}{r_T(s,i)}} + \sum_{i \in I_B}{\min_{s \in A \cap B}{r_T(s,i)}}\\
	\geq & 0 \Leftrightarrow u_{T}(A \cap B) + u_{T}(A \cup B) \leq u_{T}(A) + u_{T}(B)
\end{align*}\hfill$\square$

We made use of the fact that $\sum_{i \in I_A}{\min_{s \in A}{r_T(s,i)}} = \sum_{i \in I_A}{\min_{s \in A \cup B}{r_T(s,i)}}$ and $\sum_{i \in I_B}{\min_{s \in B}{r_T(s,i)}} = \sum_{i \in I_B}{\min_{s \in A \cup B}{r_T(s,i)}}$.
Also, $\sum_{i \in I_A}{\min_{s \in A \cap B}{r_T(s,i)}} \geq \sum_{i \in I_A}{\min_{s \in B}{r_T(s,i)}}$ and $\sum_{i \in I_B}{\min_{s \in A \cap B}{r_T(s,i)}} \geq \sum_{i \in I_B}{\min_{s \in A}{r_T(s,i)}}$, since $A \subseteq A \cap B$ and $B \subseteq A \cap B$.
Note the we stated $A,B \neq \emptyset$.
However, in these cases, the inequality simply holds with equality.
E.g., let $B = \emptyset$:

\begin{align*}
	& u_{T}(A) + u_{T}(B) - u_{T}(A \cap B) - u_{T}(A \cup B)\\
	= & u_{T}(A) + u_{T}(\emptyset) - u_{T}(\emptyset) - u_{T}(A)\\
	= & 0
\end{align*}

Having shown that $u_{T}(P)$ is non-negative, monotone and submodular, we can give the following bound on a beam-search result $P_{beam}^k$ with $w=1$~\cite{nemhauser1978analysis, krause2014submodular}:

\begin{equation}
	u_{T}(P_{beam}^k) \geq (1 - \frac{1}{e}) * \max_{|P| \leq k}{u_{T}(P)}
\end{equation}

I.e., there is a lower bound on the utility of a portfolio found by beam search.
With a minor transformation, this is an upper bound on the cost of a portfolio found by beam search:

\begin{equation}
	c_{T}(P_{beam}^k) \leq (1 - \frac{1}{e}) * \min_{|P| \leq k}{c_{T}(P)} + \frac{1}{e} * c_W
	\label{eq:upper-bound}
\end{equation}

\subsubsection{K-Best Search}

This is a baseline used in~\cite{nof2020real}.
It sorts all solvers by their individual performance and then picks the top $k$ from this list.
Thus, opposed to beam search, it does not consider how solvers within a portfolio interact, e.g., if they complement each other.

\section{Experimental Design}
\label{sec:experimental-design}

\subsection{Optimization Approaches}

We analyze the \ref{eq:small-portfolio-problem} for two definitions of runtime in the objective function:

\begin{itemize}
	\item \emph{PAR2}:
	The penalized runtimes as described in Section~\ref{sec:approach:problem:data}, with a penalty of $2*T$ for runtimes over the timeout $T$:
	\begin{align*}
		r_T(s,i) &:= \begin{cases}
			2*T & \text{if }r(s,i) = \bot\\
			r(s,i) & \text{otherwise}
		\end{cases}
	\end{align*}
	\item \emph{Unsolved}:
	A heavily discretized version of the runtimes, only stating whether a solver did not finish within $T$.
	\begin{align*}
		r_T(s,i) &:= \begin{cases}
			1 & \text{if }r(s,i) = \bot\\
			0 & \text{otherwise}
		\end{cases}
	\end{align*}
\end{itemize}

To test generalization of search and prediction approaches, we conduct five-fold cross-validation.
I.e., we split the instances of each of the two problems five times into a training set and a set set.
We search for portfolios and train prediction models only on the training sets.
We employ the four solution approaches from Section~\ref{sec:approach:solution} to search for portfolios:

\begin{itemize}
	\item \emph{Random search}:
	Exhaustively evaluating all portfolios for all sizes $k$ is too expensive in our scenario, in particular, when training prediction models for each portfolio, as there are $2^{48}$ portfolios overall.
	To still get an idea how the performance of arbitrary portfolios is distributed, we randomly sample 1000 portfolios for each $k$.
	\item \emph{Exact search}:
	We solve the \ref{eq:small-portfolio-integer-problem} to exactly determine the best portfolio for each $k \in \{1, \dots, |S|\}$.
	\item \emph{Beam search}:
	We use this approach to search for good portfolios heuristically.
	We evaluate all $k \in \{1, \dots, |S|\}$ and also vary the beam width $w \in \{1, 2, \dots, 10, 20, \dots, 100\}$.
	\item \emph{K-best search}:
	We use this approach to have a simple baseline for beam search.
	We evaluate all $k \in \{1, \dots, |S|\}$.
\end{itemize}

\subsection{Prediction Approaches}

All the solution approaches from the previous section use an oracle, the VBS, to choose a solver from the portfolio for an instance.
We also analyze the more realistic scenario of using a prediction model to recommend a solver.
To this end, we take the portfolios found by the solution approaches and train a prediction model.
For each instance, the prediction target is the best solver out of the portfolio.
As models, we use random forests~\cite{breiman2001random}, which are ensembles of decision trees.
Preliminary experiments with single decision trees~\cite{breiman1984classification} of different depths yielded unsatisfactory prediction performance.
To analyze if prediction performance improves due to ensembling, we train models with one, ten, and 100 trees.
We evaluate prediction performance in two ways:

\begin{itemize}
	\item \emph{Objective value}:
	We take the solvers recommended by the prediction model and average their costs, i.e., penalized runtimes or how many instances remain unsolved.
	The resulting objective value cannot be better than for the VBS, but also cannot be worse than for the VWS; see Section~\ref{sec:approach:problem:target}.
	\item \emph{MCC}:
	We evaluate the predictions directly with Matthews correlation coefficient~\cite{matthews1975comparison, gorodkin2004comparing}.
	This does not take into account how fast the recommended solvers actually are, but only if the fastest solver is recommended or not.
	We use MCC instead of simpler metrics like accuracy, as the class labels might be imbalanced.
	I.e., one solver might be the fastest for most of the instances, and always predicting that solver would already yield a high accuracy.
	MCC has a range of $[-1,1]$, being zero for both random guessing and always guessing the same solver.
\end{itemize}

\subsection{Dataset}

We obtain a dataset from the \emph{Global Benchmark Database}~\cite{iser2020collaborative}.
The dataset has $|S| = 48$ solvers and $|I| = 316$ instances.
We take the instances from the main track of the SAT Competition 2020\footnote{\url{https://satcompetition.github.io/2020/}}~\cite{balyo2020proceedings}, but discard the instances where no solver finished in time.
For predictions, we make use of 143 features to characterize instances:
138 from SATzilla~\cite{xu2008satzilla, xu2012satzilla2012} and five from a gate-recognition approach~\cite{iser2020recognition}.
We replace missing values of features with their training-set mean.
%TODO @Markus from which SATzilla version exactly?
%TODO @Markus vielleicht hast du noch eine bessere Referenz, ich habe erst mal deine Diss genommen


\subsection{Implementation}

We implement our experimental design in Python and make our code available online\footnote{\url{https://github.com/Jakob-Bach/Small-Portfolios}}.
To create the dataset of solver runtimes, we use the package~\emph{gbd-tools}~\cite{iser2020collaborative}.
For predictions, we use the package \emph{scikit-learn}~\cite{scikit-learn}.
To solve the small portfolio-problem exactly, we use the package \emph{mip}~\cite{python-mip}.

\section{Evaluation}
\label{sec:evaluation}
% TODO make consistent to adaptations of pipeline after 93f7a2513e

\subsection{Optimization Results}

\subsubsection{Performance of Single Solvers}

The dataset seems promising for portfolios.
Regarding PAR2 score, here is no single solver which is fastest for all or even a majority of the 316 instances.
Instead, the top three solvers in terms of being fastest (\emph{kissat\_unsat}, \emph{kissat\_sat}, and \emph{kissat}) `win' on only 46, 38, and 26 instances respectively.
This indicates that combing solvers in portfolios can improve overall runtime.
Also, the three best single solvers (\emph{kissat\_sat}, \emph{kissat}, and \emph{relaxed\_newtech}) leave 52, 55, and 61 instances unsolved.

\subsubsection{Objective Value of Portfolios}

\paragraph{Exhaustive Search}

Table~\ref{tab:objective-exhaustive} displays statistics for the objective value in exhaustive search over $k$.
As expected, objective value gets better with larger portfolio size $k$.
Also, the standard deviation becomes smaller, i.e., portfolios become more similar with increasing $k$.
Thus, choosing an appropriate portfolio-search strategy matters the most for small values of $k$.

\begin{table}[t]
	\centering
	\caption{Statistics of objective value for exhaustive search over $k$.}
	\label{tab:objective-exhaustive}
	\begin{tabular}{llrrrr}
		\toprule
		&   &     min &     mean &      max &     std \\
		problem & k &         &          &          &         \\
		\midrule
		PAR2 & 1 &  730476 &  1328323 &  2947616 &  408661 \\
		& 2 &  424187 &   952274 &  1968568 &  266017 \\
		& 3 &  354575 &   771100 &  1968273 &  223589 \\
		& 4 &  297029 &   656233 &  1922964 &  193872 \\
		unsolved & 1 &      52 &      110 &      292 &      43 \\
		& 2 &      23 &       73 &      178 &      26 \\
		& 3 &      15 &       56 &      178 &      22 \\
		& 4 &       9 &       45 &      173 &      19 \\
		\bottomrule
	\end{tabular}
\end{table}

\paragraph{Exact Search and Beam Search}

\begin{figure*}[t]
	\centering
	\subfloat[\emph{PAR2} objective function.]{
		\includegraphics[width=0.98\columnwidth]{plots/objective-PAR2.pdf}
		\label{fig:objective-PAR2}
	}
	\hfil
	\subfloat[\emph{Unsolved} objective function.]{
		\includegraphics[width=0.98\columnwidth]{plots/objective-solved.pdf}
		\label{fig:objective-unsolved}
	}
	\caption{Comparison of different solution approaches.}
	\label{fig:objective}
\end{figure*}

Figure~\ref{fig:objective} shows the objective value of the exact solution, beam search with $w=1$, and the upper bound from Equation~\ref{eq:upper-bound}.
For all approaches, objective value improves rapidly for the first few $k$, but marginal gains become smaller with increasing $k$.
For the objective function \emph{PAR2}, the best $k=1$-portfolio has a penalized runtime 5.51 times as high as the best
$k=48$-portfolio.
This ratio reduces to 1.98 for $k=5$ and 1.30 for $k=10$.
For the objective function \emph{Unsolved}, a portfolio of ten solvers suffices to solve all instances, while the best $k=5$-portfolio leaves seven instances unsolved and the best $k=1$ leaves 52 instances unsolved.

As Figure~\ref{fig:objective} shows, the objective value of beam search is very close to the exact solution.
In contrast, the theoretical, submodularity-based upper bound for beam search is clearly higher and therefore too loose to give a good estimate.
For the objective function \emph{PAR2}, the best beam-search portfolio for $k=2$ has a 15.4\% higher penalized runtime than the exact solution.
For all other $k$, the best beam-search portfolio has at most a 4\% higher penalized runtime than the exact solution.
For the objective function \emph{Unsolved}, the best beam-search portfolio for $k=2$ solves only seven instances less than the exact solution, and this difference decreases further with $k$.
These results are particularly impressive considering that the runtime of beam search is linear in $k$ as well as the number of instances $n$, while the underlying problem is NP-complete.

To bring the beam-search solution even closer to the exact solution, one can increase $w$.
E.g., for $w=10$, the PAR2 score is never 1\% higher than the best exact solution and the number of solved instances is the same as for the exact solution for all $k$.
Regarding the set of $w$ portfolios $\mathbb{P}^i$ within each iteration of beam search, a similar phenomenon as for exhaustive search occurs:
While portfolios in $\mathbb{P}^i$ show a larger variation in objective value for small $k$, this variation becomes smaller in latter iterations, i.e., the top $w$ portfolios become more similar.

\subsubsection{Solvers in Portfolios}

While beam search iteratively adds solvers, the exact solution might differ in more than one solver from $k-1$ to $k$, i.e., existing solvers from the portfolio can be replaced.
Indeed, we observe this phenomenon frequently in our results.
For example, the best $k=2$-portfolio does not contain the best $k=1$-portfolio for both objective functions.
However, as seen above, beam search still yields close-to-optimal objective value.
This indicates that there are several good solver combinations, so solvers can substitute each other in portfolios to some extent.

This substitution effect seems to be particularly present for large $k$, as we can observe for beam search with $w=100$:
For the objective function \emph{PAR2}, one solver appears in 19\% of the $w$ portfolios for $k=2$.
However, for $k=10$, no solver appears in more than 10\% of the portfolios and for $k=20$, no solver appears in more than 5\% of the portfolios.
A similar effect shows for the objective function \emph{Unsolved}.
This shows that a few solvers might be crucial for good objective value for small $k$, but there is more flexibility in building a good portfolio for large $k$.

A correlation analysis on $k=4$-exhaustive-search results also underlines the interaction between solvers.
First, we encode the absence or presence of each solver in a portfolio with 0 or 1 respectively.
Next, we compute the Spearman rank correlation between this occurrence vector with the objective value.
For the objective function \emph{PAR2}, all correlations are between -0.38 und 0.15.
For the objective function \emph{Unsolved}, all correlations are between -0.38 und 0.14.
Thus, the presence of all individual solvers is at most weakly correlated to the objective value.
This means one needs a combination of solvers to clearly influence the objective value in either direction.

\subsection{Prediction Results}

\subsubsection{MCC}

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{plots/mcc.pdf}
	\caption{Test-set MCC for beam-search portfolios with $w=100$. Decision trees of unlimited depth.}
	\label{fig:mcc}
\end{figure}

Figure~\ref{fig:mcc} shows test-set classification performance with decision trees of unlimited depth, using the portfolios from beam-search with $w=100$.
We focus on these search results instead of exact search to have multiple portfolios for each $k$.
We do not show training-set performance, since it always is 1.0.
Test-set prediction performance is rather low for all $k$.
This could cause a bad objective value for these model-based portfolios, as we will analyze latter.
For low $k$, the prediction performance varies strongly between portfolios, while for larger $k$, the variation becomes smaller.

With a perfect training-set performance, but a low test set performance, the model seems to be overfitting to the training set.
As a counter-measure, one can limit the model size, i.e., decrease the depth of the decision trees.
This indeed decreases the gap between training-set performance and test-set performance.
However, the decrease mainly results from a decrease in training-set performance, while average test-set performance is rather similar for all tree depths apart from a depth of two.
Thus, decreasing tree depth does not improve generalization capabilities of the models.

\subsubsection{Objective Value}

\begin{figure*}[t]
	\centering
	\subfloat[\emph{PAR2} objective function.]{
		\includegraphics[width=0.98\columnwidth]{plots/objective-prediction-PAR2.pdf}
		\label{fig:objective-prediction-PAR2}
	}
	\hfil
	\subfloat[\emph{Unsolved} objective function.]{
		\includegraphics[width=0.98\columnwidth]{plots/objective-prediction-solved.pdf}
		\label{fig:objective-prediction-unsolved}
	}
	\caption{Objective value for beam-search-portfolios with $w=100$. Decision trees of unlimited depth.}
	\label{fig:objective-prediction}
\end{figure*}

Figure~\ref{fig:objective-prediction} shows test-set objective value with decision trees of unlimited depth, using the portfolios from beam-search with $w=100$.
For comparison, we also displays VBS and VWS score for the same portfolios.
As shown before, VBS score decreases with $k$.
This also allows portfolios with prediction model to potentially improve their performance.
In contrast, VWS score increases with $k$.
This is because larger portfolios contain more solvers which are only good on some instances, instead of being good on average.
If a prediction model mainly predicts these solvers, objective value can become a lot worse.
Overall, the range of the potential objective value of a portfolio with prediction model grows with $k$.
Depending on the prediction quality, objective value can develop in either direction.
In our case, objective value remains rather stable with $k$.
This implies that the prediction model cannot help to leverage the larger amount of solvers in the portfolio.
Given the low prediction performance in terms of MCC, this is expected.
As a consequence, the gap between VBS and the predicted solver grows with $k$.

\subsubsection{Feature Importance}

Averaging importance over all trained models, the most important feature has an importance of 5.69\%, the least important feature has an importance of 0\%.
To reach an cumulated average importance of 50\%, one needs 29 out of 143 features.
This indicates that not just a small set of features is important, but many features can contribute towards predictions.
Indeed, prediction models use 47 features on average.
Note that this figure includes decision trees of all depths, i.e., also shallow trees that are not even able to include that many features.
If we limit our analysis to decision trees of unlimited depth, the models use 116 features on average.
The large number of features used by the models could also be a sign that our current feature set is not suitable to discriminate between solvers.

\section{Conclusions and Future Work}
\label{sec:conclusion}

SAT solvers are often complementary, i.e, they are particularly good on some instances and less good on other instances.
Thus, one combines several solvers into portfolios.
We analyzed such portfolios with runtime data from the SAT~Competition~2020.
In particular, we focused on portfolios with a limited number of solvers, so-called \emph{small portfolios}.
We presented an integer-problem to find small portfolios exactly as well as a beam-search approach to find small portfolios fast.
Regarding portfolio size, we saw a strong improvement in objective value for adding solvers to small portfolios, but a small impact once the portfolio reaches a certain size.
Regarding the solution approach, we found that beam-search yields close-to-optimal solutions.
After determining portfolios, we combined them with prediction models that made instance-specific solver recommendations.
However, these models performed rather poorly on the given dataset, such that the objective value of portfolios with predictions did not improve with the portfolio size.

In future work, we want to improve prediction performance, in particular, by adapting the set of instance features.
Also, we want to integrate our portfolio-search functionality into the Python package~\emph{gbd-tools}, so it can be directly used in queries to the database GBD.

\section*{Acknowledgments}

%TODO check if Jakob and/or Markus need to add stuff here

\balance % according to documentation, command might not work if issued to late in document, so hopefully placing it here suffices

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
