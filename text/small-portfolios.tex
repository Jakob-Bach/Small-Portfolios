\documentclass[conference]{IEEEtran}

%\usepackage[style=ieee, backend=bibtex]{biblatex} % handled by IEEE template at the moment
\usepackage[linesnumbered,vlined]{algorithm2e} % pseudo-code; use of package discouraged by IEEE, but hacked in in a (hopefully acceptable) way
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{amsthm} % mathematical theorems
\usepackage{balance} % balance columns on the last page
\usepackage{booktabs} % nicely formatted tables
\usepackage{graphicx} % plots
\usepackage[caption=false,font=footnotesize]{subfig} % figures with multiple sub-figures and sub-captions; use of (newer) package "subcaption" discouraged by IEEE
\usepackage{xcolor} % colored text (for TODOs)
\usepackage{hyperref} % links and URLs; should be loaded last

%\addbibresource{references.bib} % handled by IEEE template at the moment

\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\todo}[1]{{\color{red}TODO: #1}}

\begin{document}

\title{An Extensive Study of k-Portfolios\\ of Recent SAT Solvers}
\author{\IEEEauthorblockN{Jakob Bach}
\IEEEauthorblockA{\textit{KIT Department of Informatics} \\
\textit{Karlsruhe Institute of Technology}\\
Karlsruhe, Germany \\
jakob.bach@kit.edu}
\and
\IEEEauthorblockN{Markus Iser}
\IEEEauthorblockA{\textit{KIT Department of Informatics} \\
\textit{Karlsruhe Institute of Technology}\\
Karlsruhe, Germany \\
markus.iser@kit.edu}
}

\maketitle

\begin{abstract}
Hard combinatorial problems such as propositional satisfiability are ubiquitous. 
In general, one looks for stable methods that show good performance on all problem instances. 
However, new approaches emerge regularly some of which behave complementary to stable solvers in that they only run faster on some instances, but not on many others. 
In the presence of continuous improvements of individual solvers, putting together portfolios of solvers needs to be efficient. 
In particular, it remains an open question how well portfolios can exploit the complementarity of existing solvers. 
This paper features a comprehensive analysis of portfolios of recent SAT solvers, the ones from the SAT Competitions~2020 and~2021. 
We determine optimal portfolios with exact and approximate approaches and study the impact of the portfolio size $k$ on its performance. 
Subsequently, we investigate how effective simple prediction models are for instance-specific solver recommendations.
One result of ours is that the solutions found by an approximate approach proposed in this current article is just as good as the optimal solution in practice. 
We also observe that marginal returns decrease very quickly when adding more solvers, and our simple predictive models do not gain further performance beyond very small portfolio sizes. 
\end{abstract}

\begin{IEEEkeywords}
Propositional Satisfiability, Solver Portfolios, Runtime Prediction, Machine Learning
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

\paragraph{Motivation} %KB: DAs Layout mit a), b) usw. zur Gliederung der Einleitung ist mMn sehr unüblich, nicht?

SAT solving is the archetypal NP-complete problem.
Its practical applications abound, e.g., verification of hardware and software~\cite{Kaufmann:2021:Amulet,Buning:2020:QPRVerify}, product configuration~\cite{Janota:2014:Configuration}, cryptanalysis~\cite{Nejati:2020:CDCLCrypto}, or planning~\cite{Schreiber:2021:Lilotane}.
SAT solvers are not only used to solve hard combinatorial problems in industry, but also previously open problems in mathematics~\cite{Heule:2016:Pyth,Heule:2018:Schur}. 

Nowadays, one can observe continuous progress regarding SAT solving methods, heuristics, and their implementations in SAT solvers. 
Evaluation of solvers is commonly based on compilations of benchmark instances which represent diverse interesting application scenarios. 
An important design objective behind new SAT solvers is stability, i.e., good performance on many types of instances. 

At the same time, we observe a recurrent emergence of new heuristics and methods which improve performance on only a narrow subset of instances. 
Such approaches have a merit nevertheless -- one can see them as complementary to solvers with good average behavior. 
To make such narrow progress explicit, one tends to evaluate the respective solvers on specific problem types and instances.

\begin{example}[Crypto Track in SAT Competition 2021]
Recently, an application track where the participating solvers are evaluated on a large benchmark set from only one specific domain has been introduced in SAT Competitions.
This has uncovered the following phenomenon:
The SAT Solver \emph{SLIME} has not been competitive in the evaluations on the diverse set of instances in the Main Track of SAT Competition~2021. 
But \emph{SLIME} achieved the best score in the Crypto Track where SAT solvers were evaluated on a set of $200$ instances exclusively from cryptographic applications. 
\end{example}

Another possibility to quantify the progress in SAT solving beyond measuring the average performance of solvers exists as well; it is based on portfolios. 
Portfolios are sets of solvers.  
One can combine a solver portfolio with a perfect oracle which then selects the fastest solver from the portfolio for each instance in the benchmark set. 
This construction is commonly referred to as the \emph{Virtual Best Solver} (VBS). 
One can then derive a ranking of solvers based on their marginal contribution to the VBS, cf.~\cite{Xu:2012:EvalContribVBS}. 

\begin{example}[Special Price in SAT Competition 2021]
In SAT Competition 2021, the performance of the VBS of all portfolios of size two over the entirety of participating solvers has been evaluated. 
The best pair of solvers in terms of their VBS contains the solver \emph{CaDiCaL\_PriPro}.
This solver has not been competitive in the overall evaluation.
But due to that achievement, it has been awarded the new \emph{Special Innovation Price}.
\end{example}

Yet another tweak when it comes to performance gains from sets of solvers is as follows:
Combining a portfolio with a solver-election process which includes runtime predictions can outperform individual best solvers by much~\cite{xu2008satzilla}. 
However, a downside is that such constructions are complex and quite time-consuming. 
More often than not, comparative studies in SAT solving do not include implementations applying this principle to state-of-the-art solvers.

Given the circumstances sketched so far, the following questions are important when it comes to the design of portfolios and their evaluation. 
First, one must decide how many solvers to put in a portfolio, and which ones. 
A $k$-portfolio is a set of $k$ solvers drawn from a given set of $n \geq k$ solvers $S$. 
A $k$-portfolio is optimal if its VBS outperforms that of all other $k$-portfolios which one can draw from $S$. 
The \emph{K-Portfolio Problem} is to determine an optimal $k$-portfolio. 
Searching for a solution to the \emph{K-Portfolio Problem} is an NP-hard optimization problem~\cite{nof2020real}. 

Second, only after answering the previous question, one can design a solver-selection process based on performance predictions. 
In particular, this includes the selection of instance features used to learn a prediction model as well as of the model type. 

\paragraph{Contributions}

This current article features a study which captures and evaluates the state-of-the-art in SAT solving in terms of portfolio performance. 
Our study is unique in that it addresses all points raised so far, as follows:
We systematically construct $k$-portfolios and analyze the impact of the portfolio size $k$ on portfolio performance. 
For exact search, we encode the \emph{K-Portfolio Problem} as an integer optimization problem. 
This has been faster than alternative approaches, according to preliminary experiments of ours. 
Spurred by the complexity of the problem, we also analyze the quality of various approximate and random solutions. 
An important takeaway of our study is that it gives way to the creation of optimal portfolios in an ad-hoc manner. %KB: Hier fragt man sich beim Lesen ja schon, wie also die ad-hoc creation-Methode aussieht. Der Results-Absatz, der folgt, erklärt das aber nicht wirklich. (Der Rumpf des Artikels sagt es eigentlich auch nicht explizit.) Da ich die Einleitung jetzt deswegen nicht großartig umbauen will, würde ich vorschlagen, hier zu schreiben "... in an ad-hoc manner, as explained momentarily", und Sie fügen dann in Results nich einen Satz ein, der das dann explizit sagt.
In a subsequent step, we answer the question whether the quality of predictions of solver runtimes which one can achieve with simple models is sufficient. %KB: Was war doch gleich das Problem bei den anderen Ansätzen? Dass die Modelle von Hand getunt waren? Vielleicht sollte das hier dann deutlich werden: "... with simple models, i.e., off-the-shelf model types and instance features, without any tuning" oder so ähnlich.
For each portfolio size $k$, we train models using random forests, using the widely known instance feature records of SATzilla~2012~\cite{xu2012satzilla2012}, and then use the predictions to select portfolio members automatically in a straightforward manner. 
Our study is based on very recent datasets which contain the runtimes of solvers having participated in SAT~Competitions~2020 and~2021. 
Our code and data are available online.

\paragraph{Results}

Our evaluation yields insightful results regarding the various questions. 
While it is expected that the limit utility decreases with portfolio size, we did not expect the decrease to be so early. 
In particular, for portfolios of size 5 and larger, the marginal gain in VBS performance decreases to less than 100 seconds by adding more solvers. 
Another interesting observation is that well-known \emph{beam search} has given way to the best approximate solutions to the \emph{K-Portfolio Problem}. 
These solutions happen to perform very similarly to the optimal ones, even for a minimal beam width of one, in contrast to other approaches, which have been used in previous studies (\emph{k-best} in particular). 
According to our experiments however, relying on a simple prediction model has been a mixed bag:
On the negative side, prediction performance has been rather low. 
The actual runtime of portfolios with predictive models is not even close to the theoretical improvement one could have with a perfect oracle.
We take this as an indication that more research is necessary; our hypothesis is that the instance features available in our dataset are not sufficiently conclusive. 
On the positive side, we have observed considerable performance gains over single-best solver performance with runtime prediction for $3$-portfolios in both competition datasets. 
With the SAT Competition~2021 dataset, performance is even better with $k = 4$. %MI: need to find better formulation 
For larger $k$, the runtimes of portfolios with predictive models does not improve further. 
We take this as a further indication that it might not be beneficial to combine very many solvers to a portfolio. 

\paragraph{Outline}

In Section~\ref{sec:related-work}, we review related work. 
Preliminaries are given in Section~\ref{sec:preliminaries}. 
In Section~\ref{sec:approach}, we introduce the \emph{K-Portfolio Problem} and several solution approaches. 
We describe the experimental design in Section~\ref{sec:experimental-design} and present the experimental results in Section~\ref{sec:evaluation}. 
We conclude with Section~\ref{sec:conclusion}. 

\section{Related Work}
\label{sec:related-work}

In this section, we provide an overview of approaches that deal with portfolios of SAT solvers. 
We begin with studies that use portfolios to evaluate the state-of-the-art. 
%JB: state-of-the-art of what?
We then review instance-specific solver selection techniques, including approaches to automatically configure and analyze such techniques. 
Finally, we discuss related work on systematic investigation of $k$-portfolios. 

Solver complementarity regarding the runtime of solver portfolios has been examined in Xu et al.~\cite{Xu:2012:EvalContribVBS}. 
They show that significant contributions to the runtime of a portfolio often come from solvers which are not competitive when evaluated as a standalone solver. 
This however depends on the dataset under evaluation. 
In fact, the best $k$-portfolios ($k \in [1,3]$) drawn from solvers participating in SAT~Competition~2020 contain solvers which actually received an award in the standalone evaluation \cite{SC2020:AIJ}.
In particular, the best performing $2$-portfolio of solvers consists of the winners of the SAT and UNSAT Tracks. 
This has been markedly different in SAT~Competition~2021.
Here, a solver which was not competitive in the standalone evaluation has been part of the best performing $2$-portfolio (drawn from the solvers in that competition). 
All this underlines the importance of portfolio analysis for good stable solvers. Here, progress often manifests itself through complex hybridization of methods. 

Instance-specific solver selection approaches bring in another perspective on solver complementarity.
One of the best-known complex solver-selection approaches is SATzilla, of which multiple versions exist~\cite{xu2008satzilla, xu2012satzilla2012}. 
They combine instance classification with runtime prediction models in a so-called mixture-of-experts approach in order to select a solver in their portfolio. 
Additional components of SATzilla are so-called \emph{pre-solvers}, which are only run for a short time to solve easy instances, and a \emph{backup solver}, which is responsible for instances where computing features for the prediction models takes too long.
Overall, portfolio approaches can become quite complex, and it is unclear which elements of the portfolio are really necessary to achieve a good performance 
Other instance-specific solver selection approaches are ISAC, which is based on clustering in the instance feature space~\cite{Kadioglu:2010:ISAC}, and SNNAP, which combines the ISAC approach with runtime prediction models~\cite{Collautti:2013:SNNAP}.
%Besides complex portfolio approaches, there are also simple ones, e.g., selecting a solver nearest-neighbor-based, without training a prediction model~\cite{malitsky2011non, nikolic2013simple, samulowitz2013snappy}.%MI: what are they doing?
A recent survey on algorithm-selection approaches -- also beyond SAT -- is provided by Kerschke et al.~\cite{kerschke2019automated}.

Many portfolio approaches have several stages and configuration options, which makes configuring and selecting the best portfolio approach difficult.
To cope with this, Autofolio by Lindauer et al.\ provides an automatic configurator~\cite{lindauer2015autofolio}.
One can also analyze the impact of configuration options with tools such as CAVE~\cite{biedenkapp2018cave}.

A systematic analysis of portfolios with varying size~$k$, which has some similarities to our study, is conducted by Amadini et al.~\cite{amadini2014empirical, amadini2016extensive}.
They evaluate $k$-portfolios of CSP solvers in terms of their average runtime and solved instances. 
They compare several classification methods within portfolios, next to adapting complex SAT portfolio approaches like SATzilla. 
Amadini et al.\ focus on different instances and solvers than this current study, though the evaluation procedure has some similarities.
%Our evaluation is less broad in terms of classification models, but instead we analyze feature importances to find out what drives the models' predictions.MI: really?
While \cite{amadini2014empirical} only uses heuristic search algorithms, our study features a broad comparison of different search strategies for portfolios, including an \emph{exact} solution, two \emph{approximate} solutions, and \emph{random sampling}. 

Nof and Strichman formalize the \emph{K-Portfolio Problem} in the form of two maximization problems with different objective functions and prove their submodularity and NP-completeness~\cite{nof2020real}. 
They solve the \emph{K-Portfolio Problem} optimally with an SMT solver and use \emph{greedy search} to generate approximate solutions.
They evaluate their approach on anytime search algorithms for an allocation problem, focusing on solution quality after a timeout of one second or less. 
While we build on the theoretical results of Nof and Strichman, our evaluation is broader. 
In addition to the high variety of search algorithms, we also analyze portfolios with prediction models.

A feature of our current study, which sets it apart from the existing ones, is that it is based on two most recent datasets of SAT~Competitions~2020 and~2021. 
They contain runtimes of actual state-of-the-art SAT solvers for a very diverse set of hard SAT instances, which are measured with a timeout of 5000~seconds.

\section{Preliminaries}
\label{sec:preliminaries}

Let a set of solvers $S = \{s_1, \dots, s_n\}$, a set of SAT instances $I = \{i_1, \dots, i_m\}$ and solver runtimes $r:~I \times S \rightarrow [0, T]$ with a fixed timeout $T$ be given.
A scoring function $c_T : S \rightarrow \mathbb{N}$ is used to estimate solver performance. 
To score a solver, we use the frequently used penalized average runtime with a penalization factor of two (PAR-2 Score). 
This score is a trade-off between solver runtime and the number of solved instances.
It is defined as follows.%
\begin{align}
r_T(i,s) &:= \begin{cases}
	2 \cdot T & \text{if }r(i,s) = T\\
	r(i,s) & \text{otherwise}
\end{cases} \tag*{Penalized Runtimes}\\[.5em]
c_T(s) &:= \frac{1}{|I|} \sum_{i \in I}{r_T(i,s)} \tag*{PAR-2 Score}
\end{align}

A portfolio $P \subseteq S$ is a \emph{non-empty} set of solvers.
To score a solver portfolio $P$, we assume to have an oracle that always selects the fastest solver for each instance. 
This construction is commonly referred to as the virtual best solver (VBS) for $P$. 
Accordingly, we extend the scoring function as follows.%
$$
	c_{T}(P) := \frac{1}{|I|} \sum\limits_{i \in I}{\min\{r_T(i,s) \mid s \in P\}}
$$
In the following, we refer to $c_{T}(P)$ as the \emph{cost} of $P$. 

In reality, one may train a prediction model $m : I \rightarrow P$ for a solver portfolio $P \subseteq S$ which recommends a solver for each instance, using features of the instance. 
The \emph{cost} of such a prediction model for solvers $P$ is given by the following function.%
$$
	c_{T}'(m,P) := \frac{1}{|I|} \sum\limits_{i \in I}{r_T(m(i),i)}
$$

Clearly, the portfolio cost $c_{T}(P)$ is a lower bound for the actual cost of a portfolio $P$ which uses a prediction model.

\section{Optimal \texorpdfstring{$k$}{k}-Portfolios} % hyperref does not want a math symbol here
\label{sec:approach}

The \emph{K-Portfolio Problem} is to find a portfolio $P$ of size $|P| = k$ with minimum costs.%
$$
\argmin\limits_{P \subseteq S, |P| = k} c_{T}(P)
$$
Note that the portfolio cost function decreases monotonically under the addition of solvers: $\forall s \in S, c_{T}(P \cup \{s\}) \leq c_{T}(P)$. 
In the following, we outline four approaches for either \emph{exact} or \emph{approximate} determination of solutions to that function. 

\subsection{Exhaustive Search (exact)}

The simplest way to solve the \emph{K-Portfolio Problem} is by exhaustively searching all portfolios with $k$ solvers. 
Since there are $\binom{n}{k}$ possible portfolios to search, this becomes infeasible for sufficiently large $n$ and $k \gg 1$ as well as $k \ll n$.
For example, with $n=48$ solvers in the SAT~Competition~2020, there are $1128$ portfolios for $k=2$, but roughly $6.54 * 10^9$ portfolios for $k=10$.

\subsection{Integer Programming (exact)}

The \emph{K-Portfolio Problem} is not linear due to the use of the $\min$ function.
However, one can make it an integer linear problem by introducing additional variables.
This allows to obtain an exact solution for the problem with an off-the-shelf integer-programming solver.

We introduce two sets of binary decision variables. 
The binary variables $y_s$ indicate whether a solver $s \in S$ is in the portfolio, and 
the binary variables $x_{i,s}$ indicate whether solver $s \in S$ is selected for instance $i \in I$. 
Equation~\ref{eq:ip1} specifies the cardinality constraint on the number of solvers. 
Equation~\ref{eq:ip2} stipulates that exactly one solver is chosen for each instance. 
Equation~\ref{eq:ip3} ensures that a solver can only be chosen for an instance if it is part of the portfolio. 
Ultimately, Equation~\ref{eq:ip4} specifies the optimization target.%
\begin{align}
	\sum_{s \in S} y_s &\leq k \label{eq:ip1}\\
	\forall_{i\in I} \sum_{s \in S} x_{i,s} &= 1 \label{eq:ip2}\\
	\forall_{s \in S} \sum_{i \in I} x_{i,s} &\leq |I| \cdot y_s \label{eq:ip3}\\
	\min_{x,y} \quad & \frac{1}{|I|} \cdot \sum_{i \in I} \sum_{s \in S} r_T(i,s) \cdot x_{i,s} \label{eq:ip4}
\end{align}

\subsection{Beam Search (approximate)}

% IEEE wants algorithms as figures, not separate floats (as "algorithm2e" does), so we need some hacking: https://tex.stackexchange.com/questions/147598/how-to-use-the-algorithm2e-package-with-ieeetran-class
\begin{figure}[t]
\makeatletter
\let\@latex@error\@gobble
\makeatother
\begin{algorithm}[H]
\DontPrintSemicolon
	\KwIn{Solvers $S$, Portfolio Size $k$, Portfolio Cost $c_T$}
	\KwIn{Beam Width $w$}
	\KwOut{Portfolio $P$ with $|P|=k$}
	\KwData{Sets of $i$-Portfolios $T_i$}
	\BlankLine
	$T_0 \leftarrow \{\emptyset\}$\;
	\For{$i \leftarrow 1$ \KwTo $k$}{
		\tcp{Candidate $i$-Portfolios:}
		$U \leftarrow \emptyset$\;
		\ForEach{$P \in T_{i-1}$}{
			\ForEach{$s \in S \setminus P$}{
				$U \leftarrow U \cup \{ P \cup \{ s \} \}$\;
			}
		}
		\tcp{Select $w$ best $i$-Portfolios:}
		$T_i \leftarrow \emptyset$\;
		\For{$j \leftarrow 1$ \KwTo $w$}{
			$T_i \leftarrow T_i \cup \{\argmin\limits_{P \in U \setminus T_i}{c_T(P)}\}$\;
		}
	}
	\Return $\argmin\limits_{P \in T_k}{c_{T}(P)}$\;
\end{algorithm}
\caption{\emph{Beam search} algorithm.}
\label{al:beam-search}
\end{figure}

\emph{Beam search} is a greedy algorithm that finds portfolios in an iterative manner.
Figure~\ref{al:beam-search} specifies the approach. 
In each iteration, the algorithm combines the portfolios from the previous iteration with individual solvers which are not part of these portfolios. 
In other words, the algorithm expands existing portfolios by adding individual solvers. 
Before the next iteration, only the $w$ portfolios with the lowest cost are retained.
The beam width $w$ is an input parameter.
For $w=1$, only one portfolio remains at the end of each iteration.
We refer to this special case as \emph{greedy search}.
For $w = |S|$, the algorithm degrades to exhaustive search. 
For $w \ll |S|$, it yields a runtime advantage compared to exhaustive search, as it only evaluates $O(|S| \cdot w)$ portfolios per iteration. 

The algorithm is a greedy heuristic that does not necessarily find the optimal solution to the \emph{K-Portfolio Problem}. 
But there is a bound on the costs of a portfolio found by \emph{greedy search}. 
To this end, one can use a result from~\cite{nemhauser1978analysis}, which applies to greedy algorithms on non-negative monotone submodular set functions.
Nof and Strichman show that their \emph{K-Algorithms Max-Sum Problem} for portfolios is submodular, and thus a bound on greedy algorithms holds~\cite{nof2020real}. 
We can transform the \emph{K-Portfolio Problem} (minimization) into the \emph{K-Algorithms Max-Sum Problem} (maximization) by replacing the costs with utilities as follows.%
$$
u_{T}(P) := c_W - c_{T}(P)
$$
In this transformation, $c_W$ is an upper bound on portfolio performance, the single worst solver.%
$$
c_W := \max\{c_T(s) \mid s \in S\}
$$
As $u_{T}(P)$ is non-negative, monotone and submodular, we get the following bound on a greedy-search result $P_{greedy}^k$~\cite{nemhauser1978analysis, krause2014submodular}.%
$$
	u_{T}(P_{greedy}^k) \geq (1 - \frac{1}{e}) \cdot \max_{|P| \leq k}{u_{T}(P)}
$$
I.e., there is a lower bound on the utility of a portfolio found by \emph{greedy search}.
This can be transformed into an upper bound on the cost of a portfolio found by \emph{greedy search}.%
\begin{equation}
	c_{T}(P_{greedy}^k) \leq (1 - \frac{1}{e}) \cdot \min_{|P| \leq k}{c_{T}(P)} + \frac{1}{e} \cdot c_W
	\label{eq:upper-bound}
\end{equation}

\subsection{K-Best (approximate)}

This is a baseline used in~\cite{nof2020real}.
It sorts all solvers by their individual performance and then picks the top $k$ from this list.
Unlike \emph{beam search}, it does not consider how solvers within a portfolio interact, i.e., if they complement each other.

\section{Experimental Design}
\label{sec:experimental-design}

In our experiments, we evaluate the solution approaches just presented and analyze their relationship to prediction approaches for $k$-portfolios. 
In particular, we are interested in the performance improvements achievable with increasing values of Parameter $k$. 

\subsection{Solution Approaches}

We employ the four solution approaches from Section~\ref{sec:approach} to search for portfolios:

\begin{itemize}
	\item \emph{Random sampling}:
	To get an idea how the performance of arbitrary portfolios is distributed, we randomly sample 1000 portfolios for each $k$.
	Exhaustively evaluating all portfolios for all sizes $k$ would have been too expensive in our scenario.
	\item \emph{Optimal solution}:
	We solve the \emph{K-Portfolio Problem} as an integer optimization problem to exactly determine the best portfolio for each $k \in \{1, \dots, |S|\}$.
	\item \emph{Beam search}: 
	We use this approach to search for good portfolios heuristically.
	We evaluate all $k \in \{1, \dots, |S|\}$ and also vary the beam width $w \in \{1, 2, \dots, 10, 20, \dots, 100\}$.
	\item \emph{K-best}:
	We use this approach to have a simple baseline for \emph{beam search}.
	We evaluate all $k \in \{1, \dots, |S|\}$.
\end{itemize}

The optimization goal for all approaches is the PAR-2 score $c_T(P)$.
In preliminary experiments, we also analyzed two slightly different objectives, namely the number of unsolved instances and the PAR-2 score normalized for each instance.
However, overall trends in the results were similar to those with PAR-2 score, so we stick to the latter.

To test generalization of search and prediction approaches, we conduct five-fold cross-validation over SAT instances and average evaluation metrics over these folds. 
We search for portfolios and train prediction models only on the training sets. %KB: Ist es nicht klar, dass man nur auf den Trainingsdaten trainiert?
%KB: Kann das "generalization of" nicht vielleicht weg? (Ich frage mich, was das bedeuten soll.)

\subsection{Prediction Approaches}

We also analyze the performance of portfolios which use a prediction model to select a solver. 
We train such prediction models for all portfolios found by our solution approaches described earlier. %KB: "for" --Y "with", ist das gemeint?

For each instance, the prediction target is the best solver out of the given $k$-portfolio. %KB: Das Folgende ist kein Korrekturvorschlag zum Artikel: Ich habe mich gefragt, warum wir das so machen. Wäre es nicht vielleicht sinnvoll, die Laufzeiten vorherzusagen, d. h. das Ganze als Regressionsproblem zu modellieren? Der Vorteil wäre, dass man stets eine Zahl bekommt, wie weit man von der tatsächlichen Lösung weg ist.
As models, we use random forests~\cite{breiman2001random}, which are ensembles of decision trees. 
Preliminary experiments with individual decision trees~\cite{breiman1984classification} of different depths have yielded worse prediction performance. %KB: Irreführende Citations, hat nichts mit SAT Solvern zu tun. Vielleicht möchten Sie den Satz auch einfach weglassen. Entweder weiß jemand, was Ensembles sind, oder wir können ihm oder ihr hier jetzt halt auch nicht helfen.
Random forests are also used in the well-known portfolio approach SATzilla2012~\cite{xu2012satzilla2012}. 
To analyze if prediction performance improves due to ensembling, we train models with one, ten, and 100 trees.
The prediction approach in SATzilla2012 has two differences compared ours:
First, it trains a classifier for each pair of solvers in the portfolio, while we only train one classifier, making a multi-class prediction.
Second, it weights instances based on the runtime difference between solvers, while we go for an unweighted approach.
However, none of these changes has helped to improve prediction performance according to preliminary experiments of ours, so we opt for the simpler approach instead. %KB: Nicht ganz klar, was also der simpler approach ist.

We evaluate prediction performance in two ways:

\begin{itemize}
	\item \emph{Objective value}:
	We evaluate the prediction models with respect to the objective value, i.e., the runtime cost function $c_{T}'(m,P)$ (cf. Section~\ref{sec:preliminaries}). 
	\item \emph{MCC}:
	We evaluate the predictions with Matthews correlation coefficient (MMC)~\cite{matthews1975comparison, gorodkin2004comparing}.
	This does not take into account how fast the recommended solvers actually are, but only if the fastest solver is recommended or not.
	We use MCC instead of simpler metrics like accuracy, as the class labels might be imbalanced, i.e., one solver might be the fastest for most of the instances, and always predicting this solver would already yield a high accuracy.
	MCC has a range of $[-1,1]$. 
	The value zero occurs with both random guessing and always guessing the same solver.
\end{itemize}

\subsection{Dataset}

In our experiments, we use two datasets.
The first one, \texttt{SC2020}, contains $|S| = 48$ solvers and $400$ instances from the Main Track of the SAT~Competition~2020\footnote{\url{https://satcompetition.github.io/2020/}}~\cite{balyo2020proceedings, SC2020:AIJ}.
We filtered out those instances where no solver finished in time, resulting in a dataset of $|I| = 316$ instances. 
The second dataset, \texttt{SC2021}, contains $|S| = 46$ solvers and $400$ instances from the Main Track of the SAT~Competition~2021\footnote{\url{https://satcompetition.github.io/2021/}}.
Again, we filtered out those instances where no solver finished in time, resulting in a dataset of $|I| = 325$ instances.

For predictions, we make use of 143 features to characterize instances, of which 
$138$ features are from the feature extractor of SATzilla~2012~\cite{xu2008satzilla, xu2012satzilla2012} and five features are from a gate-recognition approach~\cite{Iser:2015:GateRecognition}. 
Feature values which are missing due to the feature extractor having exceeded time- or memory limits are replaced with the mean of their training set. %KB: Unter uns -- am Artikel selbst würde ich nichts ändern: Ist das eigentlich eine gute Idee? Offensichtlich sind diese Merkmale ja extrem ausgeprägt, sonst könnte man sie ja berechnen. Stattdessen den Mittelwert zu nehmen ist doch irreführend?!? Wie oft ist das eigentlich passiert?

\subsection{Implementation}

We implement our experimental design in Python and make our code available online\footnote{\url{https://github.com/Jakob-Bach/Small-Portfolios}}.
\todo{make repo public, adapt URL if necessary}
The code also allows to download and prepare the datasets.
Additionally, we publish the full experimental data, including results\footnote{\url{https://www.ipd.kit.edu/mitarbeiter/bach/k-portfolio-data.tar.gz}}.
\todo{put data on server, adapt URL if necessary}

To create the datasets, we use the package~\emph{gbd-tools}~\cite{iser2020collaborative}.
For predictions, we use the package \emph{scikit-learn}~\cite{scikit-learn}.
To solve the \emph{K-Portfolio Problem} exactly, we use the package \emph{mip}~\cite{python-mip}.

\section{Evaluation}
\label{sec:evaluation}

In the following, we evaluate the performance of the exact and approximate solutions to the \emph{K-Portfolio Problem}. 
We then relate the performance of such optimal $k$-portfolios to the performance of an actual prediction model for instance-specific solver selection. 
In particular, we are interested in the influence of the portfolio size $k$ on performance. 

\subsection{Optimization Results}

The datasets seem promising for portfolios. %KB: Den Satz verstehe ich nicht. Was wollen Sie sagen?
In both datasets, there is no single solver which is fastest for all or even for a majority of the instances.
For the $316$ instances in \texttt{SC2020}, the three overall fastest solvers win on only 46, 38, and 26 instances, respectively.
For the $325$ instances in \texttt{SC2021}, the three overall fastest solvers win on only 25, 22, and 20 instances, respectively.
This indicates that combining solvers in portfolios can improve overall runtime.

\subsubsection{General Trend}

\begin{figure*}[t]
	\centering
	\subfloat[\texttt{SC2020} dataset.]{
		\includegraphics[width=0.98\columnwidth]{plots/search-train-objective-2020.pdf}
		\label{fig:search-train-objective-2020}
	}
	\hfil
	\subfloat[\texttt{SC2021} dataset.]{
		\includegraphics[width=0.98\columnwidth]{plots/search-train-objective-2021.pdf}
		\label{fig:search-train-objective-2021}
	}
	\caption{
		Training-set portfolio performance for different $k$ and search algorithms.
	}
	\label{fig:search-train-objective}
\end{figure*}

Figure~\ref{fig:search-train-objective} displays the cost in terms of the PAR-2 function of the best $k$-portfolios for the different solution approaches. 
The \emph{optimal solution} is the exact optimum.
\emph{Greedy search} denotes the performance of the best portfolios found by a \emph{beam search} algorithm with the smallest beam width $w=1$. 
\emph{k-best} stands for the performance of portfolios which are comprised by the top $k$ single-best solvers. 
For \emph{random sampling}, we average over repeated samples.
The \emph{upper bound} limits costs of \emph{greedy search} according to Equation~\ref{eq:upper-bound}.
If we report numbers in the following, we refer to the training dataset for which the optimization problems are solved. %KB: Den Satz verstehe ich nicht.

For all approaches and both datasets, the PAR-2 score improves rapidly for the first few $k$, but marginal gains become smaller with increasing $k$.
For the dataset \texttt{SC2020}, the optimal $1$-portfolio has a penalized runtime $5.51$ times as high as the $48$-portfolio.
This ratio reduces to $1.89$ for the best $5$-portfolio and $1.25$ for the best $10$-portfolio.
For \texttt{SC2021}, the respective ratios are $4.74$ for $k=1$, $1.58$ for $k=5$ and $1.17$ for $k=10$.

\subsubsection{Beam Search}

Figure~\ref{fig:search-train-objective} also shows the cost of the best $k$-portfolios found by \emph{greedy search} to be very close to that of the \emph{optimal solution}.
These results are remarkable considering that the runtime of \emph{greedy search} is linear in $k$ as well as the number of instances $n$, while finding the \emph{optimal solution} is NP-complete.
In contrast, the theoretical, submodularity-based \emph{upper bound} for \emph{greedy search} clearly is higher and too loose to serve as a good estimate.

To bring the \emph{beam search} solution even closer to the \emph{optimal solution}, one can increase $w$.
For example, the best $2$-portfolio found by \emph{greedy search} has a $14.7\%$ higher cost than the \emph{optimal solution} for \texttt{SC2020}.
For all other $k$, the \emph{greedy search} portfolio has less than $4\%$ higher penalized runtime than the \emph{optimal solution}.
In comparison, for $w=10$ and for all $k$, the PAR-2 score is never more than $2\%$ higher than the \emph{optimal solution}.

Regarding the set of the $w$ best portfolios in each iteration of \emph{beam search} we observe a convergence of portfolio performance with increasing $k$. 
There is a large variance in the PAR-2 score of portfolios in the beam for small $k$, and this variance becomes smaller in later iterations, i.e., the top $w$ portfolios become more similar in performance.
A similar phenomenon also occurs for \emph{random sampling} portfolios:
With increasing $k$, the standard deviation of the PAR-2 score in a sample of portfolios decreases.
The expected value of portfolio performance improves as well.
Thus, carefully selecting the solvers for a portfolio makes the biggest difference for small $k$.

\subsubsection{K-Best}

The baseline \emph{k-best} is worse than \emph{greedy search} on the training data, but better than \emph{random sampling}.
While \emph{greedy search} always is quite close to the \emph{optimal solution}, the performance gap of \emph{k-best} widens after the first few $k$ and only becomes smaller for large $k$ later. %KB: gap to what? Say it explicitly.
The performance of \emph{k-best} relative to the other approaches also differs between \texttt{SC2020} and \texttt{SC2021}.
For \texttt{SC2020}, \emph{k-best} is closer to the optimal solution, while for \texttt{SC2021}, \emph{k-best} is closer to the expected value of \emph{random sampling}.
This indicates that building a portfolio of complementary solvers, rather than picking the best individual solvers, may be more important.
%is more important for \texttt{SC2021}. (Habe das hier geändert -- diese Schlussfolgerung ist zu eng, würde ich sagen.)

\subsubsection{Test-Set Performance} %KB: Vermutlich sollten Sie den Unterschied zwischen training-set performance und test-set performance zu Beginn dieser Section explizit erklären.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{plots/search-test-objective-2020.pdf}
	\caption{
		Test-set portfolio performance for different $k$ and search algorithms for the \texttt{SC2020} dataset.
	}
	\label{fig:search-test-objective-2020}
\end{figure}

Figure~\ref{fig:search-test-objective-2020} shows test-set portfolio cost for the different solution approaches.
The overall trends are the same as on the training set in Figure~\ref{fig:search-train-objective}.
A notable exception is that \emph{beam search}, \emph{k-best}, and the \emph{optimal solution} show similar performance on the test data of \texttt{SC2020}.
In particular, there is no clear winner, and the \emph{optimal solution} can even perform worse than those found by the approximate approaches. 
This is because optimization is run on the training instances only, and the best portfolio on the training data is not necessarily the best portfolio on the test data.
For the test data of \texttt{SC2021}, which we do not display here, \emph{k-best} is markedly worse than \emph{beam search} and the \emph{optimal solution}. %KB: Das war gemeint, oder?
The larger performance gap already on the training set might have caused this effect.

\subsubsection{Portfolio Composition}

While \emph{beam search} adds solvers iteratively, the \emph{optimal solution} might differ in more than one solver from $k-1$ to $k$, i.e., existing solvers from the portfolio can be replaced.
Indeed, we observe this phenomenon in our results.
For example, the optimal $2$-portfolios do not contain the optimal $1$-portfolios for both datasets.
However, this non-monotonous behavior is not strong.
On average, one or two new solvers become part of the optimal portfolio when increasing $k$ by one.
I.e., zero or one solver are replaced on average when increasing $k$ by one.
In addition, the replaced solver might only be slightly worse than its substitute.
This might explain the good performance of \emph{beam search}, which pursues a monotonous approach when building portfolios.

\subsubsection{Impact of Single Solvers on Portfolios}

We have carried out a correlation analysis on the 1000 portfolios which are the result of \emph{random sampling} for a certain value of $k$. 
First, we encode the absence or presence of each solver in a portfolio with 0 or 1 respectively.
Next, we compute the Spearman rank correlation between this occurrence vector with the PAR-2 score.
Our analysis highlights the interaction between solvers. 
For $k=5$, all correlations are in $[-0.40,0.17]$ for \texttt{SC2020}, and $[-0.24,0.23]$ for \texttt{SC2021}.
The mean correlation is zero in both cases.
Similar correlation behavior occurs for other $k$.

These results indicate that only the presence of some solvers has a moderately negative correlation to the PAR-2 score of the whole portfolio, i.e., only some solvers can improve the portfolio performance of our minimization problem on their own.
This effect is stronger for \texttt{SC2021}.
The positive correlations are even weaker, i.e., there are no solvers which influence portfolio performance in a strongly negative manner.
Overall, this means that the influence of single solvers on portfolio performance is limited; one needs a combination of solvers to clearly influence the PAR-2 score in either direction.

\subsection{Prediction Results}

In this section, we evaluate the prediction performance for instance-specific solver predictions in the portfolios computed earlier. 
We also evaluate the PAR-2 score of these portfolios with prediction model. %KB: Mir nicht ganz klar, was hier gemacht wird.

\subsubsection{Matthews Correlation Coefficient}

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{plots/prediction-test-mcc.pdf}
	\caption{
		Test-set MCC for \emph{random sampling} portfolios, using random forests with 100 trees.
		(MCC is 0 for random or constant prediction and it is 1 for perfect prediction.)
		For readability reasons, we do not plot outlier points.
	}
	\label{fig:prediction-test-mcc}
\end{figure}

Figure~\ref{fig:prediction-test-mcc} graphs the test-set classification performance of random forests with 100 trees, using the portfolios from \emph{random sampling}.
Performance with fewer trees is worse, on the training set as well as on the test set.
We use \emph{random sampling} results here, to show the variation of prediction performance over many portfolios.
However, classification results are similar for \emph{beam search} with $w=100$ and for the \emph{optimal solution}.
We do not display training-set performance, since it is close to 1.0.

As Figure~\ref{fig:prediction-test-mcc} shows, test-set prediction performance is rather low for all $k$.
It is clearly higher than with random guessing, which would yield an MCC of 0.0, but clearly lower than the optimal MCC of 1.0.
With a nearly perfect training-set performance, but a low test set performance, the model seems to overfit to the training set.
This could cause a bad PAR-2 value for model-based portfolios, and we will analyze this next.
For small $k$, the prediction performance varies stronger between portfolios than for larger $k$.

\subsubsection{Portfolio Performance}

\begin{figure*}[t]
	\centering
	\subfloat[\texttt{SC2020} dataset.]{
		\includegraphics[width=0.98\columnwidth]{plots/prediction-test-objective-beam-2020.pdf}
		\label{fig:prediction-test-objective-beam-2020}
	}
	\hfil
	\subfloat[\texttt{SC2021} dataset.]{
		\includegraphics[width=0.98\columnwidth]{plots/prediction-test-objective-beam-2021.pdf}
		\label{fig:prediction-test-objective-beam-2021}
	}
	\caption{
		Portfolio performance for \emph{beam search} portfolios with $w=100$, comparing the portfolio's virtual best solver (VBS), single best solver (SBS), and score with prediction model.
		Prediction model: random forests with 100 trees.
		The performance of the the global SBS, i.e., not only considering solvers from the portfolio, is shown as horizontal line.
	}
	\label{fig:prediction-test-objective-beam}
\end{figure*}

Figure~\ref{fig:prediction-test-objective-beam} shows test-set PAR-2 scores for portfolios from \emph{beam search} with $w=100$.
The plot compares the PAR-2 score of portfolios with prediction model to two competitors without prediction model, computed on the same portfolios.
The virtual best solver (VBS) provides a lower bound, as its portfolio performance can only be achieved with perfect prediction.
The single best solver (SBS) of each portfolio serves as a baseline, corresponding to a constant prediction. %KB: Ich sehe nicht, wieso SBS einer 'constant prediction' entspricht.
Note that the portfolio performance with prediction models can even be worse than the SBS, e.g., when always predicting the slowest solver for each instance.
However, we do not show this upper bound on PAR-2 score here.

As discussed earlier, the VBS score decreases with $k$.
In theory, this also allows portfolios with prediction model to improve their performance.
However, in our case, the PAR-2 score of model-based portfolios remains rather stable with increasing values of $k$, with the biggest improvement from $k=2$ to $k=3$.
This implies that the prediction model cannot help to benefit the larger amount of solvers in the portfolio with increasing $k$. %KB: Was ist gemeint? Dass die Vorhersagen nicht helfen bei der Auswahl der Solver, wenn k größer wird? Wenn ja, schreiben Sie das doch bitte so.
Given the low prediction performance in terms of MCC, as seen in Figure~\ref{fig:prediction-test-mcc}, this has been expected.
As a consequence, the gap between VBS and the predicted solver grows with $k$.
However, model-based portfolios tend to be better than the single best solver from these portfolios, i.e., the prediction models can discriminate between solvers to some extent.
In addition, for $k > 2$ the model-based portfolios are better than the global single-best solver, which might not be part of the portfolio, at least on average.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{plots/prediction-test-objective-optimal-2020.pdf}
	\caption{
		Portfolio performance for \emph{optimal solution} portfolios, comparing the portfolio's virtual best solver (VBS), single best solver (SBS), and score with prediction model for the \texttt{SC2020} dataset.
		Prediction model: random forests with 100 trees.
		The performance of the the global SBS, i.e., not only considering solvers from the portfolio, is shown as horizontal line.
	}
	\label{fig:prediction-test-objective-optimal-2020}
\end{figure}

Figure~\ref{fig:prediction-test-objective-optimal-2020} repeats the same comparison as before for the \emph{optimal solution} portfolios.
The overall trends remain the same.
In contrast to Figure~\ref{fig:prediction-test-objective-beam}, here we have only five portfolios for each $k$, one for each fold of cross-validation.
We still see a considerable variance for each $k$, i.e., the test set performance also depends on the current split of instances.

\subsubsection{Feature Importance}

Averaging importance over all trained models, the most important feature has an importance of roughly 2\%, the least important one an importance of 0\%.
To reach a cumulated average importance of 50\%, one needs 40 out of 143 features.
On average, prediction models use 109 features.
If we limit our analysis to random forests with just one tree, the models still use 59 features on average.
We conclude that no single feature or even small set of features drives prediction performance.
Together with the low prediction performance, this indicates that our dataset lacks discriminating features which are suitable to decide which solver should be used on an instance.

\section{Conclusions and Future Work}
\label{sec:conclusion}

%KB: Hier würde sich ein einleitender Satz gut machen, also dass über das Zusammenwirken von Solvern bisher noch vergleichsweise wenig bekannt ist, obwohl es sich bei SAT Solving um ein reifes Gebiet handelt, oder so ähnlich.
SAT solvers are often complementary, i.e, some solvers are good on some instances, and others are on other instances.
This has lead to the notion of portfolios, i.e., combination of several solvers. %KB: Ab hier haben die Conclusions in etwa das gleiche Problem wie die Einleitung. Die einzelnen Punkte werden etwas lieblos (so wirkt es halt) aufgezählt, ohne dass die Wichtigkeit der einzelnen Einsichten erklärt wird. Ich würde erst einmal sagen, dass der ARtikel eine vergleichende Studie zu Portfolios ist, die in mehrerlei Hinsicht über das hinausgegangen ist, was bisherige Arbeiten geleistet haben. Es muss auch nicht unbedingt alles aufgezählt werden -- hier reichen dann die Highlights (aus Ihrer Sicht).
We have analyzed such portfolios with runtime data from the SAT~Competitions~2020 and 2021. %KB: Vielleicht: "One feature of our study is that it includes the latest ..."
In particular, we focused on portfolios with the number of solvers limited to $k$. %KB: Nicht gut formuliert. "We have studied the important question how large portfolios should actually be, and have observed that ..." vielleicht stattdessen.

We analyzed an integer optimization problem %KB: Wording passt hier nicht.
to find $k$-portfolios exactly as well as a \emph{beam search} approach to find $k$-portfolios fast. %KB: Da Sie integer optimization mit nichts anderem (zum Finden der exakten Lösung) verglichen haben, wäre ich hier diesbezüglich zurückhaltender.
%KB: Dass Beam search so gut funktioniert, wid hier gsar nicht klar.
%KB: Die folgenden Aussagen dann etwas nach oben. Ich würde nicht zweimal anfangen mit einzelnen Aspekten der Studie.
Regarding portfolio size, we saw a strong improvement in objective value for adding solvers to portfolios with small $k$, but a small impact once the portfolio reaches a certain size.

Regarding the solution approach, we found that \emph{beam search} yields close-to-optimal solutions.
After determining portfolios, we combined them with prediction models that made instance-specific solver recommendations.
However, these models performed not satisfactorily on the given dataset, such that the objective value of portfolios with predictions did not continuously improve with the portfolio size.

In future work, we want to improve prediction performance, in particular, by adapting the set of instance features. %KB: Nicht "we want to", das klingt zu wenig objektiv. Stattdessen: "Regarding future work, there is room for improvement regarding ..." oder "As future work, it is necessary to improve ..."
Recent results in the empirical analysis of solver portfolios indicate that features which describe the community structure of graph representations of SAT instances correlate well with solver performance. %KB: Dieser Satz liest sich, als hätten wir dem Leser die ganze Zeit gewisse Forschungsergebnisse vorenthalten. Was sind das für recent results?
Among those are features describing their hierarchical community structure, such as community leaf size, community degree, and the number of inter-community edges~\cite{Li:2021:HCS}. 
Also, we want to integrate our portfolio-search functionality %KB: Nicht ganz klar welche. Beam search?
into the Python package~\emph{gbd-tools}, so it can be directly used in queries to the database GBD. %KB: Das hier klingt nicht nach Forschung. Sie sollten also mindestens sagen, was der Rest der Welt von dieser Integration hat.

\section*{Acknowledgments}

\todo{check if Jakob and/or Markus need to add stuff here}

\balance % according to documentation, command might not work if issued to late in document, so hopefully placing it here suffices

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
