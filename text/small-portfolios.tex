\documentclass[conference]{IEEEtran}

%\usepackage[style=ieee, backend=bibtex]{biblatex} % handled by IEEE template at the moment
\usepackage[linesnumbered,vlined]{algorithm2e} % pseudo-code; use of package discouraged by IEEE, but hacked in in a (hopefully acceptable) way
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{amsthm} % mathematical theorems
\usepackage{balance} % balance columns on the last page
\usepackage{booktabs} % nicely formatted tables
\usepackage{graphicx} % plots
\usepackage[caption=false,font=footnotesize]{subfig} % figures with multiple sub-figures and sub-captions; use of (newer) package "subcaption" discouraged by IEEE
\usepackage{xcolor} % colored text (for TODOs)
\usepackage{hyperref} % links and URLs; should be loaded last

%\addbibresource{references.bib} % handled by IEEE template at the moment

\newtheorem{definition}{Definition}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\todo}[1]{{\color{red}TODO: #1}}

\begin{document}

\title{Analyzing k-Portfolios of SAT Solvers}

\author{\IEEEauthorblockN{Jakob Bach}
\IEEEauthorblockA{\textit{KIT Department of Informatics} \\
\textit{Karlsruhe Institute of Technology}\\
Karlsruhe, Germany \\
jakob.bach@kit.edu}
\and
\IEEEauthorblockN{Markus Iser}
\IEEEauthorblockA{\textit{KIT Department of Informatics} \\
\textit{Karlsruhe Institute of Technology}\\
Karlsruhe, Germany \\
markus.iser@kit.edu}
}

\maketitle

\begin{abstract}
Successful approaches that tackle hard combinatorial problems such as propositional satisfiability are often complementary, i.e., an approach only works best on some problem instances, but not on a great many of them. 
%Parallel portfolios and instance-specific algorithm selection take advantage of this. %KB: 'take advantage' ist vielleicht zu positiv, ich hätte hier eher erwartet "versuchen, das auszubügeln" oder so. Der Satz liest sich auch, als sei das Problem jetzt gelöst.
Parallel portfolios and instance-specific algorithm selection try to exploit this.
%In this paper, we present a systematic analysis of solver portfolios using runtime measurements from the SAT~Competition~2020. %KB: Warum? Es fehlt ein motivierender Satz.
%At the same time, we observe continuous advances in the runtime of individual solvers, which are often achieved through better heuristics and hybridization of approaches. 
However, portfolio approaches are in competition with the progressive improvements of individual solvers.
It therefore remains an open question to what extent complementarity can be exploited in recent solvers. 
In this paper, we present a systematic analysis of portfolios of recent SAT solvers using runtime measurements from the SAT~Competitions~2020 and~2021. 
First, we present and compare search approaches to find good portfolios of limited size~$k$.
Second, we use prediction models to make instance-specific solver recommendations within the portfolios and evaluate this approach as well. %KB: Es fehlt eine abschließende Aussage, dass das ganz gut funktioniert.
We show that greedy solutions for finding good portfolios in practice compete well with optimal solutions. 
We also show that the marginal returns of adding more solvers diminish with increasing portfolios size, which becomes particularly evident in connection with prediction models. 
\end{abstract}

\begin{IEEEkeywords}
Propositional Satisfiability, Solver Portfolios, Runtime Prediction Models, Machine Learning
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

\paragraph{Motivation}

SAT solving is the archetypal NP-complete problem. It has lots of practical applications, e.g., verification of hard- and software~\cite{Kaufmann:2021:Amulet,Buning:2020:QPRVerify}, product configuration~\cite{Janota:2014:Configuration}, cryptanalysis~\cite{Nejati:2020:CDCLCrypto}, or planning~\cite{Schreiber:2021:Lilotane}. 
Today, SAT solvers are not only used to solve hard combinatorial problems in industry but also to solve previously open problems in mathematics~\cite{Heule:2016:Pyth,Heule:2018:Schur}. 

One can observe continuous progress in SAT solving methods, heuristics, and their implementations in SAT solvers. 
The SAT Competition series, which is organized yearly as an international open event, aims to support and provide further incentives for maintaining this progress~\cite{balyo2020proceedings}. %KB: Ich halte den Satz hier für unpassend. Es sollte rein inhaltlich argumentiert werden.

Solvers are usually evaluated based on compilations of benchmark instances representing diverse interesting application scenarios of SAT solvers. 
Researchers who propose new SAT solvers tend to strive for \emph{stable} SAT solvers, i.e., with a good overall performance on many types of instances. 
At the same time, new heuristics and methods which contribute to the performance on only a narrow subset of instances continue to emerge. 
This diversity has motivated \emph{portfolios}, which are sets of SAT solvers.
One can define the runtime of a portfolio on an instance as the runtime of the fastest solver from the portfolio. %KB: Vielleicht "It is common and plausible to define the runtime ..."
In that regard, a special innovation price was given to a solver based on its contributions to the best portfolio in the SAT Competition 2021\footnote{\url{https://satcompetition.github.io/2021/}}. %KB: Das ist auch keine inhaltliche Aussage, ich würde Derartiges weglassen. Es entsteht auch der Eindruck, dass Portfolios reif, etabliert usw. seien. Das passt aber nicht zu Ihrem Pitch.
Solver portfolios can also be used in combination with runtime prediction models for instance-specific algorithm selection~\cite{xu2008satzilla}. %KB: Wieso brauche ich dafür Portfolios? Diese Art Vorhersage kann ich vermutlich mit kleinem Aufwand für viele (fast alle) Solver machen, oder nicht? Und dann weiter: Könnte ich nicht, basierend auf diesen Vorhersagen, 'on the fly' ein Portfolio für ein bestimmtes Problem zusammenstellen?
%KB: Anscheinend funktioniert das noch nicht optimal, sonst würden Sie es in der Studie nicht aufgreifen. Hier sollte gesagt werden, was also unklar ist, und warum es wichtig ist, das zu klären.
%KB: Hier fehlt ein Satz, dass das Konzept zwar im Raum steht, dass aber nicht genügend darüber bekannt ist, wie Portfolios also wirken (in quantitativer Hinsicht). (Ist doch so, oder? Sonst bedürfte es Ihrer Studie ja nicht.)

\paragraph{Problem Statement}

In this paper, we study $k$-portfolios of SAT solvers, i.e., portfolios composed of $k$ SAT solvers.
In particular, we analyze the impact of the portfolio size $k$ on portfolio performance.
We want to find out whether portfolios with just a few solvers are already sufficient to achieve a good solving performance. %KB: Hier sollte die Reihenfolge der Aussagen vertauscht sein. Zuerst das hier sagen, dann das, was gemacht wird.
%KB: Hier fehlt mir auch etwas Eindringlichkeit. Warum ist es wichtig, darüber Bescheid zu wissen, wie das k sich auswirkt?
%KB: Gibt es noch weitere Antworten, die die Studie gibt? Das würde ich hier dann als Frage(n) formulieren. Nur diese eine Frage wirkt ein bisschen mickrig.
%KB: So, wie es derzeit geschrieben ist, entsteht ein bisschen der Eindruck, dass die Studie in ihrer Durchführung höchst einfach war. Ein paar Sätze, mit welchen Schwierigkeiten Sie zu kämpfen hatten, würden sich hier gut machen, denke ich.

\paragraph{Contributions}

We empirically analyze the performance of $k$-portfolios on runtime data for 48 solvers and 400 instances from the SAT~Competition~2020. 
In our experiments, we compare possible portfolio sizes $k \in \{1, \dots, 48\}$. 
For approximate determination of optimal portfolios with size $k$, we apply \emph{beam search} with varying beam widths. %KB: Was bedeutet 'optimal portfolio' eigentlich? Bitte explizit sagen.
We compare the quality of such approximations to the results of an exact search.
For the exact search, we encode the \emph{K-Portfolio Problem} as an integer optimization problem. 

We rely on two baselines, one deploying random sampling and the other one ranking by individual solvers' performances. %KB: Die zweite Baseline habe ich hier nicht verstanden.
For the evaluation, we consider both the theoretically optimal performance of portfolios %KB: Was soll das 'theoretically' hier?
as well as the performance if a prediction model makes instance-specific solver recommendations. %KB: Abhängig von k, oder was ist gemeint? -- Wie machen Sie es eigentlich? Nehmen Sie dann die k schnellsten Solver? Oder achten Sie auch (wie auch immer genau) darauf, dass diese Solver irgendwie unterschiedlich sind?

\paragraph{Results}

We observe that adding solvers to a portfolio has diminishing returns, i.e., portfolio performance improves most when adding the first few solvers. %KB: Das ist sehr erwartet und sollte vielleicht nicht als erstes kommen. Ich habe mich gefragt, wie sich das k denn jetzt auswirkt. Wie groß sollte es sein? Sind die unterschiedlichen Werte von k sehr problem- und Portfolio-abhängig? Wenn ja, lässt sich das erklären?
Additionally, the portfolios found heuristically by \emph{beam search} have very similar performance to the optimal solution, even for minimal beam width $w=1$. %KB: Wie verhalten sich die Laufzeiten der Suchen zueinander?
We also observe that the prediction models exhibit a rather low prediction performance in our experiments. %KB: Hier würde man sich als Leser vermutlich eine kurze Erklärung wünschen.
Thus, actual portfolio performance cannot benefit from the theoretical improvement over $k$. 

\paragraph{Outline}

In Section~\ref{sec:related-work}, we review related work. 
Preliminaries are given in Section~\ref{sec:preliminaries}. 
In Section~\ref{sec:approach}, we introduce the \emph{K-Portfolio Problem} and multiple solution approaches for it. 
We describe the experimental design in Section~\ref{sec:experimental-design} and present the experimental results in Section~\ref{sec:evaluation}. 
We conclude with Section~\ref{sec:conclusion}. 

\section{Related Work}
\label{sec:related-work}

\todo{conduct deeper literature research}
\todo{expand a bit, if necessary; currently, description of references is rather short}
\todo{describe "SAT Competition 2020" paper (if published), which does a lot of analysis on single solvers and some analysis on portfolios; citation already exists: \cite{SC2020:AIJ}}
\todo{some references might be shortened if space is needed}

A recent survey on algorithm-selection approaches is provided by Kerschke et al.~\cite{kerschke2019automated}.
Portfolios of SAT solvers are a sub-category of this. 
Many portfolio approaches have several stages and configuration options, which makes configuring and selecting the best portfolio approach difficult.
To this end, Autofolio by Lindauer et al. provides an automatic configurator~\cite{lindauer2015autofolio}.
Also, one can analyze the impact of configuration options, e.g., with the tool CAVE~\cite{biedenkapp2018cave}. 

One of the most well-known complex portfolio approaches is SATzilla, of which multiple versions exist~\cite{xu2008satzilla, xu2012satzilla2012}. 
Additional components of SATzilla are so-called \emph{pre-solvers}, which are only run for a short time to solve easy instances, or a \emph{backup-solver}, which is responsible for instances where computing features for the prediction models takes too long. 
Overall, portfolio approaches can become quite complex and it becomes unclear which parts of the portfolio solution are really necessary to achieve a good performance.
Besides complex portfolio approaches, there are also simple ones, e.g., selecting a solver nearest-neighbor-based, without training a prediction model~\cite{malitsky2011non, nikolic2013simple, samulowitz2013snappy}.

Adamini et al. analyze portfolios of CSP solvers in terms of average runtime and solved instances~\cite{amadini2014empirical, amadini2016extensive}. 
Thus, they focus on different instances and solvers than we, though the evaluation procedure has some similarities.
Similar to us, they vary the portfolio size $k$.

They compare several classification methods within portfolios, besides adapting complex SAT portfolio approaches like SATzilla.
Our evaluation is less broad in terms of classification models, but instead we analyze feature importances to find out what drives the models' predictions.
Also, we evaluate additional search strategies for portfolios.
For example, we systematically analyze parametrization of \emph{beam search}.
Further, we compare against random portfolios of size $k$ as a baseline.
Finally, we solve the \emph{K-Portfolio Problem} exactly, while \cite{amadini2014empirical} only uses heuristic search algorithms.

Nof and Strichman formalize the \emph{K-Portfolio Problem} in the form of two maximization problems with different objective functions~\cite{nof2020real}.
They show submodularity and NP-completeness of the problems.
While we can build on the theoretical results of \cite{nof2020real}, our evaluation is different.
In their evaluation, they use \emph{beam search} with a beam width of one.
In contrast, we analyze parametrization of \emph{beam search}.
Further, we compare against random portfolios of size $k$ as a baseline.
Also, \cite{nof2020real} does not analyze portfolios with prediction models, but only the optimization problems.
For the exact solution, we provide an encoding as integer optimization problem.
In comparison, \cite{nof2020real} provides an SMT encoding.
Finally, their domain is different than ours, as they focus on instances of an allocation problem whose solution is evaluated already after 0.1~s.
In comparison, solvers in our dataset have a much larger timeout of 5000~s.

\section{Preliminaries}
\label{sec:preliminaries}

Given is a set of solvers $S = \{s_1, \dots, s_n\}$, a set of SAT instances $I = \{i_1, \dots, i_m\}$ and solver runtimes $r~:~S \times I \rightarrow [0, T]$ with a fixed timeout $T$.
A scoring function $c_T : S \rightarrow \mathbb{N}$ is used to estimate solver performance. 
To score a solver, we use the frequently used penalized average runtime with a penalization factor of two (PAR-2 Score). 
That score yields a trade-off between solver runtime and the number of solved instances.
It is defined as follows.%
\begin{align}
r_T(s,i) &:= \begin{cases}
	2 \cdot T & \text{if }r(s,i) = T\\
	r(s,i) & \text{otherwise}
\end{cases} \tag*{Penalized Runtimes}\\[.5em]
c_T(s) &:= \frac{1}{|I|} \sum_{i \in I}{r_T(s,i)} \tag*{PAR-2 Score}
\end{align}

A portfolio $P \subseteq S$ is a \emph{non-empty} set of solvers.
To score a solver portfolio $P$, we assume to have an oracle that always selects the fastest solver for each instance. 
This construction is commonly referred to as the virtual best solver (VBS) for $P$. 
Accordingly, we extend the scoring function as follows.%
$$
	c_{T}(P) := \frac{1}{|I|} \sum\limits_{i \in I}{\min\{r_T(s,i) \mid s \in P\}}
$$
In the following, we will refer to $c_{T}(P)$ as the \emph{cost} of $P$. 

In reality, one may train a prediction model $m : I \rightarrow P$ for a solver portfolio $P \subseteq S$ which recommends a solver for each instance, using features of the instance. 
The \emph{cost} of such a prediction model for solvers $P$ is given by the following function.%
$$
	c_{T}'(m,P) := \frac{1}{|I|} \sum\limits_{i \in I}{r_T(m(i),i)}
$$

Clearly, the portfolio cost $c_{T}(P)$ serves as a lower bound for the actual cost of a portfolio $P$ which uses a prediction model.

\section{Optimal $k$-Portfolios}
\label{sec:approach}

The \emph{K-Portfolio Problem} is to find a portfolio $P$ of size $|P| = k$ with minimum costs.%
$$
\argmin\limits_{P \subseteq S, |P| = k} c_{T}(P)
$$
Note that the portfolio cost function decreases monotonically under the addition of solvers: $\forall s \in S, c_{T}(P \cup \{s\}) \leq c_{T}(P)$. 
In the following, we outline four approaches for either \emph{exact} or \emph{approximate} determination of solutions to that function. 

\subsection{Exhaustive Search (exact)}

The simplest way to solve the \emph{K-Portfolio Problem} is by exhaustively searching all portfolios with $k$ solvers. 
Since there are $\binom{n}{k}$ possible portfolios to search, this becomes infeasible for sufficiently large $n$ and $k \gg 1$ as well as $k \ll n$.
For example, with the $n=48$ solvers in our dataset, there are $1128$ portfolios for $k=2$, but roughly $6.54 * 10^9$ portfolios for $k=10$.

\subsection{MIP Search (exact)}

The \emph{K-Portfolio Problem} is not linear due to the use of the $\min$ function.
However, one can make it an integer linear problem by introducing additional variables.
This allows to obtain an exact solution for the problem with an off-the-shelf integer-programming solver.

We introduce two sets of binary decision variables. 
The binary variables $y_s$ denote whether a solver $s \in S$ is in the portfolio, and 
the binary variables $x_{s,i}$ denote whether solver $s \in S$ is selected for instance $i \in I$. 
Equation~\ref{eq:ip1} specifies the cardinality constraint on the number of solvers. 
Equation~\ref{eq:ip2} stipulates that exactly one solver is chosen for each instance. 
Equation~\ref{eq:ip3} ensures that a solver can only be chosen for an instance if it is part of the portfolio. 
Ultimately, the optimization target is specified by Equation~\ref{eq:ip4}.%
\begin{align}
	\sum_{s \in S} y_s &\leq k \label{eq:ip1}\\
	\forall_{i\in I} \sum_{s \in S} x_{s,i} &= 1 \label{eq:ip2}\\
	\forall_{s \in S} \sum_{i \in I} x_{s,i} &\leq |I| \cdot y_s \label{eq:ip3}\\
	\min_{x,y} \quad & \frac{1}{|I|} \cdot \sum_{i \in I} \sum_{s \in S} r_T(s,i) \cdot x_{s,i} \label{eq:ip4}
\end{align}

\subsection{Beam Search (approximate)}

% IEEE wants algorithms as figures, not separate floats (as "algorithm2e" does), so we need some hacking: https://tex.stackexchange.com/questions/147598/how-to-use-the-algorithm2e-package-with-ieeetran-class
\begin{figure}[t]
\makeatletter
\let\@latex@error\@gobble
\makeatother
\begin{algorithm}[H]
\DontPrintSemicolon
	\KwIn{Solvers $S$, Portfolio Size $k$, Portfolio Cost $c_T$}
	\KwIn{Beam Width $w$}
	\KwOut{Portfolio $P$ with $|P|=k$}
	\KwData{Sets of $i$-Portfolios $T_i$}
	\BlankLine
	$T_0 \leftarrow \{\emptyset\}$\;
	\For{$i \leftarrow 1$ \KwTo $k$}{
		\tcp{Candidate $i$-Portfolios:}
		$U \leftarrow \emptyset$\;
		\ForEach{$P \in T_{i-1}$}{
			\ForEach{$s \in S \setminus P$}{
				$U \leftarrow U \cup \{ P \cup \{ s \} \}$\;
			}
		}
		\tcp{Select $w$ best $i$-Portfolios:}
		$T_i \leftarrow \emptyset$\;
		\For{$j \leftarrow 1$ \KwTo $w$}{
			$T_i \leftarrow T_i \cup \{\argmin\limits_{P \in U \setminus T_i}{c_T(P)}\}$\;
		}
	}
	\Return $\argmin\limits_{P \in T_k}{c_{T}(P)}$\;
\end{algorithm}
\caption{\emph{Beam search} algorithm.}
\label{al:beam-search}
\end{figure}

\emph{Beam search} is a greedy algorithm that finds portfolios in an iterative manner.
Figure~\ref{al:beam-search} depicts the approach. 
In each iteration, the algorithm combines the portfolios from the previous iteration with individual solvers which are not part of these portfolios. 
I.e., the algorithm expands existing portfolios by adding single solvers. 
Before going to the next iteration, only the $w$ portfolios with the lowest cost are retained.
The beam-width $w$ is an input parameter.
For $w=1$, only one portfolio remains at the end of each iteration.
We refer to this special case as \emph{greedy search}.
For $w = |S|$, the algorithm degrades to exhaustive search. 
For $w \ll |S|$, it yields a runtime advantage compared to exhaustive search, as it only evaluates $O(|S| \cdot w)$ portfolios per iteration. 

The algorithm is a greedy heuristic that does not necessarily find the optimal solution to the \emph{K-Portfolio Problem}. 
But there is a bound on the costs of a portfolio found by \emph{greedy search}. 
To this end, one can use a result from~\cite{nemhauser1978analysis}, which applies to greedy algorithms on non-negative monotone submodular set functions.
Nof and Strichman show that their \emph{K-Algorithms Max-Sum Problem} for portfolios is submodular and thus a bound on greedy algorithms holds~\cite{nof2020real}. 
We can transform the \emph{K-Portfolio Problem} (minimization) into the \emph{K-Algorithms Max-Sum Problem} (maximization) by replacing the costs with utilities as follows.%
$$
u_{T}(P) := c_W - c_{T}(P)
$$
In this transformation $c_W$ denotes an upper bound on portfolio performance, the virtual worst solver.%
$$
c_W := \frac{1}{|I|} \sum_{i \in I}{\max\{{r_T(s,i) \mid s \in S}}\}
$$
As $u_{T}(P)$ is non-negative, monotone and submodular, we get the following bound on a greedy-search result $P_{greedy}^k$~\cite{nemhauser1978analysis, krause2014submodular}.%
$$
	u_{T}(P_{greedy}^k) \geq (1 - \frac{1}{e}) \cdot \max_{|P| \leq k}{u_{T}(P)}
$$
I.e., there is a lower bound on the utility of a portfolio found by \emph{greedy search}.
This can be transformed into an upper bound on the cost of a portfolio found by \emph{greedy search}.%
\begin{equation}
	c_{T}(P_{greedy}^k) \leq (1 - \frac{1}{e}) \cdot \min_{|P| \leq k}{c_{T}(P)} + \frac{1}{e} \cdot c_W
	\label{eq:upper-bound}
\end{equation}

\subsection{K-Best (approximate)}

This is a baseline used in~\cite{nof2020real}.
It sorts all solvers by their individual performance and then picks the top $k$ from this list.
Thus, opposed to \emph{beam search}, it does not consider how solvers within a portfolio interact, e.g., if they complement each other.

\section{Experimental Design}
\label{sec:experimental-design}

In our experiments, we evaluate the presented optimization approaches, and analyze their relation to prediction approaches for $k$-portfolios. 
In particular, we are interested in the achievable performance improvements with increasing values for the parameter $k$. 

\subsection{Optimization Approaches}

We employ the four solution approaches from Section~\ref{sec:approach} to search for portfolios:

\begin{itemize}
	\item \emph{Random sampling}:
	To get an idea how the performance of arbitrary portfolios is distributed, we randomly sample 1000 portfolios for each $k$.
	Exhaustively evaluating all portfolios for all sizes $k$ is too expensive in our scenario.
	\item \emph{Optimal solution}:
	We solve the \emph{K-Portfolio Problem} as an integer optimization problem to exactly determine the best portfolio for each $k \in \{1, \dots, |S|\}$.
	\item \emph{Beam search}: 
	We use this approach to search for good portfolios heuristically.
	We evaluate all $k \in \{1, \dots, |S|\}$ and also vary the beam width $w \in \{1, 2, \dots, 10, 20, \dots, 100\}$.
	\item \emph{K-best}:
	We use this approach to have a simple baseline for \emph{beam search}.
	We evaluate all $k \in \{1, \dots, |S|\}$.
\end{itemize}

The optimization goal for all approaches is the PAR-2 score $c_T(P)$.
In preliminary experiments, we also analyzed two slightly different objectives, i.e., number of unsolved instances and PAR-2 score normalized for each instance.
However, overall trends in the results were similar to those with PAR-2 score, so we stick to the latter.

To test generalization of search and prediction approaches, we conduct five-fold cross-validation over SAT instances, and average evaluation metrics over these folds. 
We search for portfolios and train prediction models only on the training sets.

\subsection{Prediction Approaches}

We also analyze the performance of portfolios which use a prediction model to select a solver. 
We train such prediction models for all the portfolios found by our previously described optimization approaches. 

For each instance, the prediction target is the best solver out of the given $k$-portfolio.
As models, we use random forests~\cite{breiman2001random}, which are ensembles of decision trees. 
Preliminary experiments with single decision trees~\cite{breiman1984classification} of different depths yielded worse prediction performance. 
Random forests are also used in the well-known portfolio approach SATzilla2012~\cite{xu2012satzilla2012}. 
To analyze if prediction performance improves due to ensembling, we train models with one, ten, and 100 trees.
We evaluate prediction performance in two ways:

\begin{itemize}
	\item \emph{Objective value}:
	We evaluate the prediction models with respect to the objective value, i.e., the runtime cost function $c_{T}'(m,P)$ (cf. Section~\ref{sec:preliminaries}). 
	\item \emph{MCC}:
	We evaluate the predictions with Matthews correlation coefficient (MMC)~\cite{matthews1975comparison, gorodkin2004comparing}.
	This does not take into account how fast the recommended solvers actually are, but only if the fastest solver is recommended or not.
	We use MCC instead of simpler metrics like accuracy, as the class labels might be imbalanced, i.e., one solver might be the fastest for most of the instances, and always predicting that solver would already yield a high accuracy.
	MCC has a range of $[-1,1]$, being zero for both random guessing and always guessing the same solver.
\end{itemize}

\subsection{Dataset}

In our experiments, we use a dataset of $|S| = 48$ solvers and $400$ instances from the Main track of the SAT~Competition~2020\footnote{\url{https://satcompetition.github.io/2020/}}~\cite{balyo2020proceedings, SC2020:AIJ}. 
We filtered out those instances where no solver finished in time, resulting in a dataset of $|I| = 316$ instances. 

For predictions, we make use of 143 features to characterize instances, of which 
$138$ features stem from the feature extractor of SATzilla~2012~\cite{xu2008satzilla, xu2012satzilla2012} and five features stem from a gate-recognition approach~\cite{Iser:2015:GateRecognition}. 
Feature values which are missing due to the feature extractor having exceeded time- or memory limits are replaced with their training-set mean. 

\subsection{Implementation}

We implement our experimental design in Python and make our code available online\footnote{\url{https://github.com/Jakob-Bach/Small-Portfolios}}.
To create the dataset of solver runtimes, we use the package~\emph{gbd-tools}~\cite{iser2020collaborative}.
For predictions, we use the package \emph{scikit-learn}~\cite{scikit-learn}.
To solve the \emph{K-Portfolio Problem} exactly, we use the package \emph{mip}~\cite{python-mip}.

\section{Evaluation}
\label{sec:evaluation}

\todo{update to removal of alternative objectives}

In the following, we evaluate the performance of the exact and approximate solutions to the \emph{K-Portfolio Problem}. 
We subsequently relate the performance of such optimal $k$-portfolios to the performance of an actual prediction model for instance-specific solver selection. 
In particular, we are interested in the influence of the portfolio size $k$ on performance. 

\subsection{Optimization Results}

The dataset seems promising for portfolios.
Regarding \mbox{PAR-2} score, there is no single solver which is fastest for all or even for a majority of the $316$ instances. 
The three solvers which are fastest on average (in terms of their PAR-2 Score), `win' on only 46, 38, and 26 instances, respectively. 
Regarding the number of solved instances, the three best single solvers still leave 52, 55, and 61 instances unsolved. 
This indicates that combing solvers in portfolios can improve overall runtime.

\subsubsection{Optimizing the PAR-2 Score}

\begin{figure*}[t]
	\centering
	\subfloat[Training data.]{
		\includegraphics[width=0.98\columnwidth]{plots/search-train-objective-PAR2.pdf}
		\label{fig:search-train-objective-PAR2}
	}
	\hfil
	\subfloat[Test data.]{
		\includegraphics[width=0.98\columnwidth]{plots/search-test-objective-PAR2.pdf}
		\label{fig:search-test-objective-PAR2}
	}
	\caption{
		Portfolio performance for different $k$ and search algorithms for objective \emph{PAR-2}.
		Optimized on training set (left), evaluated on test set (right). 
	}
\label{fig:search-train-objective}
\end{figure*}

Figure~\ref{fig:search-train-objective} displays the cost in terms of the \emph{PAR-2} function of the best $k$-portfolios for $1 \leq k \leq 48$ as they are found by different solution approaches. 
The \emph{optimal solution} is the exact optimum.
\emph{Beam search} denotes the performance of the best portfolios found by a \emph{greedy search} algorithm, i.e., executing the \emph{beam search} algorithm with the smallest beam width $w=1$. 
The approach \emph{k-best} displays the performance of portfolios which are comprised by the top $k$ single-best solvers. 
For \emph{random sampling}, we average over the repetitions of sampling.
The \emph{upper bound} limits costs of {greedy search} according to Equation~\ref{eq:upper-bound}.
If we report numbers in the following, we refer to the training dataset for which the optimization problems are solved.

For all approaches, the objective value improves rapidly for the first few $k$, but marginal gains become smaller with increasing $k$.
For the \emph{optimal solution}, the best $1$-portfolio has a penalized runtime $5.51$ times as high as the $48$-portfolio. 
This ratio reduces to $1.89$ for the best $5$-portfolio and $1.25$ for the best $10$-portfolio. 

As Figure~\ref{fig:search-train-objective-PAR2} also shows, the cost of the best $k$-portfolios found by \emph{greedy search} is very close to that of the \emph{optimal solution}.
These results are particularly impressive considering that the runtime of \emph{greedy search} is linear in $k$ as well as the number of instances $n$, while the finding the \emph{optimal solution} is NP-complete.
In contrast, the theoretical, submodularity-based \emph{upper bound} for \emph{greedy search} is clearly higher and therefore too loose to give a good estimate. 

The baseline \emph{k-best} is worse than \emph{greedy search} on the training data, but still clearly better than \emph{random sampling}.
As Figure~\ref{fig:search-test-objective-PAR2} shows, \emph{beam search}, \emph{k-best}, and the \emph{optimal solution} show similar performance on the test dataset.
In particular, there is no clear winner, and the \emph{optimal solution} can even perform worse than those found by the approximate approaches. 
This is because optimization is run on the training instances only, and the best portfolio on the training data is not necessarily the best portfolio on the test data.
Still, training and test performances are rather similar.

On the training data, the best $2$-portfolio found by \emph{greedy search} has a $14.7\%$ higher cost than the \emph{optimal solution}, while the $2$-portfolio of \emph{k-best} has a $28.4\%$ higher cost than the \emph{optimal solution}. 
For all other $k$, the greedy-search portfolio has less than $4\%$ higher penalized runtime than the \emph{optimal solution}.

In contrast, the performance gap of \emph{k-best} portfolios to the \emph{optimal solution} widens for the first few $k$, before becoming smaller again later.
For $k=8$, where the relative performance gap is the largest, the \emph{k-best} portfolio has a 51.1\% higher penalized runtime than the \emph{optimal solution}.

\subsubsection{Alternative Objective Functions}

Since the objective functions \emph{PAR-2\_norm} and \emph{Unsolved} follow a similar pattern as \emph{PAR-2}, we omitted them in our plots. 
For the objective function \emph{Unsolved}, a portfolio of ten solvers suffices to solve all instances, while the best $5$-portfolio leaves $1.7\%$ of instances unsolved and the best $1$-portfolio leaves $16.4\%$ of instances unsolved.

For the objective function \emph{Unsolved}, the greedy-search portfolio for $k=2$ solves only $1.9\%$ out of all instances less than the \emph{optimal solution}, and this difference decreases further with $k$.
\emph{K-best} solves up to $4.3\%$ of all instances less than the \emph{optimal solution}.

\subsubsection{Beam Search Width}

To bring the \emph{beam search} solution even closer to the \emph{optimal solution}, one can increase $w$.
E.g., for $w=10$ and for all $k$, the PAR-2 score is never more than $2\%$ higher than the \emph{optimal solution} and the difference in unsolved instances is never more than $0.1\%$ of all instances.
Regarding the set of the $w$ best portfolios within each iteration of \emph{beam search}, we observe convergence with increasing $k$. 
We observe a large variance in the PAR-2 score of portfolios in the beam for small $k$, and this variance becomes smaller in later iterations, i.e., the top $w$ portfolios become more similar. 

A similar phenomenon also occurs for randomly sampled portfolios. 
With increasing $k$, the standard deviation of the objective value in a sample of portfolios decreases.
At the same time, the expected objective value converges towards the objective value of the optimal $k$-portfolio.
This is clearly visible in Figure~\ref{fig:search-train-objective}.
Thus, the benefit of searching for good portfolios is the greatest for small $k$.

\subsubsection{Monotonicity of $k$-Portfolios}

While \emph{beam search} iteratively adds solvers, the \emph{optimal solution} might differ in more than one solver from $k-1$ to $k$, i.e., existing solvers from the portfolio can be replaced.
Indeed, we observe this phenomenon frequently in our results.
For example, the best $2$-portfolios usually do not contain the best $1$-portfolio in our experiments.
However, despite this non-monotonous behavior of the \emph{optimal solution}, \emph{beam search} still yields a close-to-optimal objective value.
This indicates that there are several good solver combinations, so solvers can substitute each other in portfolios to some extent.

This substitution effect seems to be particularly present for large $k$, as we can observe for \emph{beam search} with $w=100$. 
For the objective function \emph{PAR-2}, one solver appears in 19\% of the beam of $2$-portfolios.
However, for $k=10$, no solver appears in more than 10\% of the portfolios and for $k=20$, no solver appears in more than 5\% of the portfolios.
A similar effect shows for the objective functions \emph{PAR-2\_norm} and \emph{Unsolved}.
This shows that a few solvers might be crucial for good objective value for small $k$, but there is more flexibility in building a good portfolio for large $k$.

\subsubsection{Random Portfolios}

A correlation analysis on \emph{random sampling} results, of which we have 1000 portfolios for each $k$, also underlines the interaction between solvers.
First, we encode the absence or presence of each solver in a portfolio with 0 or 1 respectively.
Next, we compute the Spearman rank correlation between this occurrence vector with the objective value.
For $k=5$, all correlations are in $[-0.40,0.17]$ for objective function \emph{PAR-2}, $[-0.42,0.17]$ for \emph{PAR-2\_norm}, and $[-0.41,0.15]$ for \emph{Unsolved}.
The mean correlation is zero for all objective functions.
Similar correlation behavior occurs for other $k$.

These results indicate that only the presence of some solvers has a moderately negative correlation to the objective value of the whole portfolio, i.e., only some solvers can clearly improve the portfolio performance of our minimization problem on their own.
The positive correlations are even weaker, i.e., there are no solvers which influence portfolio performance in a strongly negative manner.
Overall, this means that the influence of single solvers on portfolio performance is limited; one needs a combination of solvers to clearly influence the objective value in either direction.

\subsection{Prediction Results}

In this section, we evaluate the prediction performance for the previously determined portfolios. 
We also evaluate the actual performance of the resulting portfolios in terms of their penalized average runtimes. 

\subsubsection{Matthews Correlation Coefficient}

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{plots/prediction-test-mcc.pdf}
	\caption{
		Test-set MCC for \emph{random sampling} portfolios, using random forests with 100 trees.
		(MCC is 0 for random or constant prediction and it is 1 for perfect prediction.)
		For readability reasons, we do not plot outlier points.
	}
	\label{fig:prediction-test-mcc}
\end{figure}

Figure~\ref{fig:prediction-test-mcc} shows test-set classification performance of random forests with 100 trees, using the portfolios from \emph{random sampling}.
Classification performance with less trees is worse, on training set as well as test set.
We use \emph{random sampling} results here, but classification results are similar for \emph{beam search} with $w=100$.
We do not display training-set performance, since it always is 1.0.
As Figure~\ref{fig:prediction-test-mcc} shows, test-set prediction performance is rather low for all $k$.
It is clearly higher than random guessing, which would yield an MCC of 0.0, but clearly lower than the optimal MCC of 1.0.
With a perfect training-set performance, but a low test set performance, the model seems to be overfitting to the training set.
This could cause a bad objective value for model-based portfolios, as we will analyze latter.
For low $k$, the prediction performance varies stronger between portfolios than for larger $k$.

\subsubsection{Portfolio Performance}

\begin{figure*}[t]
	\centering
	\subfloat[\emph{PAR-2} objective function.]{
		\includegraphics[width=0.98\columnwidth]{plots/prediction-test-objective-PAR2.pdf}
		\label{fig:prediction-test-objective-PAR2}
	}
	\hfil
	\subfloat[\emph{Unsolved} objective function.]{
		\includegraphics[width=0.98\columnwidth]{plots/prediction-test-objective-unsolved.pdf}
		\label{fig:prediction-test-objective-unsolved}
	}
	\caption{
		Portfolio performance for \emph{beam search} portfolios with $w=100$, comparing VBS (orange) to portfolios with prediction model (blue).
		Random forests with 100 trees for the predictions.
	}
	\label{fig:prediction-test-objective}
\end{figure*}

Figure~\ref{fig:prediction-test-objective} shows test-set objective value for portfolios from \emph{beam search} with $w=100$.
The plot compares the objective value of portfolios with an oracle, the VBS, to the objective value of portfolios with random forests with 100 trees
As discussed before, VBS score decreases with $k$.
In theory, this also allows portfolios with prediction model to potentially improve their performance.

However, in our case, the objective value of model-based portfolios remains rather stable with increasing values of $k$.
This implies that the prediction model cannot help to leverage the larger amount of solvers in the portfolio.
Given the low prediction performance in terms of MCC, as previously seen in Figure~\ref{fig:prediction-test-mcc}, this is expected.
As a consequence, the gap between VBS and the predicted solver grows with $k$.
Also, the objective value of model-based portfolios varies more than that of VBS-portfolios.
This probably is related to the considerably variance in prediction performance, which is shown in Figure~\ref{fig:prediction-test-mcc}.

\subsubsection{Feature Importance}

Averaging importance over all trained models, the most important feature has an importance of roughly 2\%, the least important feature has an importance of 0\%.
To reach an cumulated average importance of 50\%, one needs 49 out of 143 features.
On average, prediction models use 106 features.
If we limit our analysis to random forests with just one tree, the models still use 52 features on average.
In conclusion, no single feature or even small set of features drives prediction performance.
Considering the low prediction performance, this indicates that our dataset lacks discriminating features which are suitable to decide which solver should be used on an instance.

\todo{get rid of alternate objective functions (PAR-2 Norm and Unsolved) as in this context we cannot learn anything more from them}

\todo{evaluate prediction performance also for the optimal k-portfolios}

\section{Conclusions and Future Work}
\label{sec:conclusion}

SAT solvers are often complementary, i.e, they are particularly good on some instances and less good on other instances.
Thus, one combines several solvers into portfolios.
We analyzed such portfolios with runtime data from the SAT~Competition~2020.
In particular, we focused on portfolios with the number of solvers limited to $k$.

We analyzed an integer optimization problem to find $k$-portfolios exactly as well as a \emph{beam search} approach to find $k$-portfolios fast.
Regarding portfolio size, we saw a strong improvement in objective value for adding solvers to portfolios with small $k$, but a small impact once the portfolio reaches a certain size.

Regarding the solution approach, we found that \emph{beam search} yields close-to-optimal solutions.
After determining portfolios, we combined them with prediction models that made instance-specific solver recommendations.
However, these models performed rather poorly on the given dataset, such that the objective value of portfolios with predictions did not improve with the portfolio size.

In future work, we want to improve prediction performance, in particular, by adapting the set of instance features.
Recent results in the empirical analysis of solver portfolios indicate that features which describe the community structure of graph representations of SAT instances correlate well with solver performance. 
Among those are features describing their hierarchical community structure, such as community leaf size, community degree, and the number of inter-community edges~\cite{Li:2021:HCS}. 
Also, we want to integrate our portfolio-search functionality into the Python package~\emph{gbd-tools}, so it can be directly used in queries to the database GBD.

\section*{Acknowledgments}

\todo{check if Jakob and/or Markus need to add stuff here}

\balance % according to documentation, command might not work if issued to late in document, so hopefully placing it here suffices

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
