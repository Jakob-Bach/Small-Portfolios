\documentclass[conference]{IEEEtran}

%\usepackage[style=ieee, backend=bibtex]{biblatex} % handled by IEEE template at the moment
\usepackage[linesnumbered,vlined]{algorithm2e} % pseudo-code; use of package discouraged by IEEE, but hacked in in a (hopefully acceptable) way
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{amsthm} % mathematical theorems
\usepackage{balance} % balance columns on the last page
\usepackage{booktabs} % nicely formatted tables
\usepackage{graphicx} % plots
\usepackage[caption=false,font=footnotesize]{subfig} % figures with multiple sub-figures and sub-captions; use of (newer) package "subcaption" discouraged by IEEE
\usepackage{xcolor} % colored text (for TODOs)
\usepackage{hyperref} % links and URLs; should be loaded last

%\addbibresource{references.bib} % handled by IEEE template at the moment

\newtheorem{definition}{Definition}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\todo}[1]{{\color{red}TODO: #1}}

\begin{document}

\title{Analyzing Small Portfolios of SAT Solvers}

\author{\IEEEauthorblockN{Jakob Bach}
\IEEEauthorblockA{\textit{KIT Department of Informatics} \\
\textit{Karlsruhe Institute of Technology}\\
Karlsruhe, Germany \\
jakob.bach@kit.edu}
\and
\IEEEauthorblockN{Markus Iser}
\IEEEauthorblockA{\textit{KIT Department of Informatics} \\
\textit{Karlsruhe Institute of Technology}\\
Karlsruhe, Germany \\
markus.iser@kit.edu}
}

\maketitle

\begin{abstract}
Successful approaches that tackle hard combinatorial problems such as propositional satisfiability are often in some sense complementary, i.e., each approach is only the best on some problem instances. 
Parallel portfolios and instance-specific algorithm selection take advantage of this. 
In this paper, we present a systematic analysis of solver portfolios using runtime measurements from the SAT~Competition~2020. 
First, we present and compare search approaches to find good portfolios of limited size.
Second, we use prediction models to make instance-specific solver recommendations within the portfolios and evaluate this approach as well.
\end{abstract}

\begin{IEEEkeywords}
Propositional Satisfiability, Solver Portfolios, Runtime Prediction Models, Machine Learning
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

\paragraph{Motivation}

SAT solving is the archetypal NP-complete problem and has lots of practical applications, e.g., verification of hard- and software~\cite{Kaufmann:2021:Amulet,Buning:2020:QPRVerify}, product configuration~\cite{Janota:2014:Configuration}, cryptanalysis~\cite{Nejati:2020:CDCLCrypto}, or planning~\cite{Schreiber:2021:Lilotane}. 
Today, SAT solvers are not only used to solve hard combinatorial problems in industry but also to solve previously open problems in mathematics~\cite{Heule:2016:Pyth,Heule:2018:Schur}. 

We can observe continuous progress in SAT solving methods, heuristics, and their implementations in SAT solvers. 
The SAT Competition series, which is organized yearly as an international open event, aims to support and provide further incentives for maintaining this progress~\cite{balyo2020proceedings}.

Solvers are usually evaluated based on compilations of benchmark instances representing diverse interesting application scenarios of SAT solvers. 
Researchers who implement SAT solvers usually strive for so-called \emph{stable} SAT solvers with a good overall performance on many types of instances. 
Still, we regularly observe the emergence of new heuristics and methods which contribute to the performance on only a specific subsets of instances. 
Such insular improvements can subsequently serve as the subject for research on improvements for stable solvers. 

In order to also highlight such insular developments, recent SAT Competitions introduced \emph{application tracks}, where SAT solvers are evaluated on instances which stem only from a specific application. 
In SAT Competition 2021, for the first time a special innovation price was given to a solver based on its contributions to the best portfolio.\footnote{\url{https://satcompetition.github.io/2021/}} 

Solvers portfolios can also be used in combination with runtime prediction models for instance-specific algorithm selection. 
It has been shown in the past, that such approaches can improve performance of SAT solvers.~\cite{xu2008satzilla}

\paragraph{Problem Statement}

In this paper, we study small portfolios of SAT solvers.
\emph{Small} refers to the number of solvers in the portfolio, which we limit to certain values~$k$.
Also, we strive to keep the overall approach simple by only using one prediction model per portfolio, and no further portfolio components. 
Instead, we analyze the impact of the portfolio size $k$. 
In particular, we want to find out whether small portfolios are already sufficient to achieve a good solving performance. 

\paragraph{Contributions}

We empirically analyze the performance of small portfolios on runtime data for 48 solvers and 400 instances from the SAT~Competition~2020. 
In our experiments, we compare possible portfolio sizes $k \in \{1, \dots, 48\}$. 
For approximate deterimination of optimal portfolios if size $k$, we apply \emph{beam search} with varying beam widths. 
We compare the quality of such approximations to the results of an exact search, for which we encode the $k$-portfolio problem as an integer optimization problem. 

Moreover, we include two baselines, one employing random sampling and the other one ranking by individual solvers' performances. 
We analyze three variants of the $k$-portfolio problem, based on different transformations of solver runtimes.
For evaluation, we consider both the theoretically optimal performance of portfolios as well as the performance if a prediction model makes instance-specific solver recommendations. 

\paragraph{Results}

We observe that adding solvers to a portfolio has diminishing returns, i.e., portfolio performance improves most when adding the first few solvers. 
Additionally, the portfolios found heuristically by \emph{beam search} have very similar performance to the optimal solution, even for minimal beam width $w=1$.
Unfortunately, we find that the prediction models exhibit a rather low prediction performance in our experiments. 
Thus, actual portfolio performance cannot benefit from the theoretical improvement over $k$. 

\paragraph{Outline}

In Section~\ref{sec:related-work}, we review related work. 
Preliminaries are given in Section~\ref{sec:preliminaries}. 
In Section~\ref{sec:approach}, we introduce the small-portfolio problem and multiple solution approaches for it. 
We describe the experimental design in Section~\ref{sec:experimental-design} and present the experimental results in Section~\ref{sec:evaluation}. 
We conclude with Section~\ref{sec:conclusion}. 

\section{Related Work}
\label{sec:related-work}

\todo{conduct deeper literature research}
\todo{expand a bit, if necessary; currently, description of references is rather short}
\todo{describe "SAT Competition 2020" paper (if published), which does a lot of analysis on single solvers and some analysis on portfolios; citation already exists: \cite{SC2020:AIJ}}
\todo{some references have very long booktitles at the moment; if space is neeed, can use short conference/workshop name instead}

A recent survey on algorithm-selection approaches is provided by Kerschke et al.~\cite{kerschke2019automated}.
Portfolios of SAT solvers are a sub-category of this. 
Many portfolio approaches have several stages and configuration options, which makes configuring and selecting the best portfolio approach difficult.
To this end, Autofolio by Lindauer et al. provides an automatic configurator~\cite{lindauer2015autofolio}.
Also, one can analyze the impact of configuration options, e.g., with the tool CAVE~\cite{biedenkapp2018cave}. 

One of the most well-known complex portfolio approaches is SATzilla, of which multiple versions exist~\cite{xu2008satzilla, xu2012satzilla2012}. 
Additional components of SATzilla are so-called \emph{pre-solvers}, which are only run for a short time to solve easy instances, or a backup-solver, which is responsible for instances where computing features for the prediction models takes too long. 
Overall, portfolio approaches can become quite complex and it becomes unclear which parts of the portfolio solution are really necessary to achieve a good performance.
Besides complex portfolio approaches, there are also simple ones, e.g., selecting a solver nearest-neighbor-based, without training a prediction model~\cite{malitsky2011non, nikolic2013simple, samulowitz2013snappy}.

Adamini et al. analyze portfolios of CSP solvers in terms of average runtime and solved instances~\cite{amadini2014empirical, amadini2016extensive}. 
Thus, they focus on different instances and solvers than we, though the evaluation procedure has some similarities.
Similar to us, they vary the portfolio size $k$.

They compare several classification methods within portfolios, besides adapting complex SAT portfolio approaches like SATzilla.
Our evaluation is less broad in terms of classification models, but instead we analyze feature importances to find out what drives the models' predictions.
Also, we evaluate additional search strategies for portfolios.
For example, we systematically analyze parametrization of beam search.
Further, we compare against random portfolios of size $k$ as a baseline.
Finally, we solve the small-portfolio-problem exactly, while \cite{amadini2014empirical} only uses heuristic search algorithms.

Nof and Strichman formalize the small-portfolio problem in the form of two maximization problems with different objective functions~\cite{nof2020real}.
They show submodularity and NP-completeness of the problems.
While we can build on the theoretical results of \cite{nof2020real}, our evaluation is different.
In their evaluation, they use \emph{beam search} with a beam width of one.
In contrast, we analyze parametrization of \emph{beam search}.
Further, we compare against random portfolios of size $k$ as a baseline.
Also, \cite{nof2020real} does not analyze portfolios with prediction models, but only the optimization problems.
For the exact solution, we provide an encoding as integer optimization problem.
In comparison, \cite{nof2020real} provides an SMT encoding.
Finally, their domain is different than ours, as they focus on instances of an allocation problem whose solution is evaluated already after 0.1~s.
In comparison, solvers in our dataset have a much larger timeout of 5000~s.

\section{Preliminiaries}
\label{sec:preliminaries}

Given is a set of solvers $S = \{s_1, \dots, s_n\}$, a set of SAT instances $I = \{i_1, \dots, i_m\}$ and discrete solver runtimes $r~:~S \times I \rightarrow [0, \dots, T]$ with a fixed timeout $T$. 
A scoring function $c_T : S \rightarrow \mathbb{N}$ is used to estimate solver performance. 
To score a solver, we use the frequently used penalized average runtime with a penalization factor of two (PAR-2 Score) which is defined as follows.%
\begin{align}
r_T(s,i) &:= \begin{cases}
	2 \cdot T & \text{if }r(s,i) = T\\
	r(s,i) & \text{otherwise}
\end{cases} \tag*{Penalized Runtimes}\\[.5em]
c_T(s) &:= \frac{1}{|I|} \sum_{i \in I}{r_T(s,i)} \tag*{PAR-2 Score}
\end{align}

A portfolio $P \subseteq S$ is a \emph{non-empty} set of solvers.
To score a solver portfolio $P$, we assume to have an oracle that always selects the fastest solver for each instance. 
This construction is commonly referred to as the virtual best solver (VBS) for $P$. 
Accordingly, we extend the scoring function as follows.%
\begin{align*}
	c_{T}(P) &:= \frac{1}{|I|} \sum\limits_{i \in I}{\min\{r_T(s,i) \mid s \in P\}}
\end{align*}%
%
In the following, we will refer to $c_{T}(P)$ as the \emph{cost} of $P$. 

In reality, one may train a prediction model $m : I \rightarrow P$ for a solver portfolio $P \subseteq S$ which recommends a solver for each instance. 
The \emph{cost} of such a prediction model for solvers $P$ is given by the following function.%
\begin{align*}
	c_{T}'(P) &:= \frac{1}{|I|} \sum\limits_{i \in I}{r_T(m(i),i)}
\end{align*}%
%Such prediction models use feature records $r(I)$ of the SAT instances to make its recommendations. 

Clearly, the portfolio cost $c_{T}(P)$ serves as a lower bound for the actual cost of a portfolio $P$ which uses a prediction model.

%In the worst case, the prediction model always recommends the slowest solver out of $P$ for each instance.
%This serves as an upper bound for the actual portfolio score, which we will refer to as the \emph{virtual worst solver} (VWS) for $P$.

\section{Optimal $k$-Portfolios}
\label{sec:approach}

The $k$-Portfolio Problem is to find a portfolio $P$ of size $|P| = k$ with minimum costs.%
%For each solver out of $S$, one needs to decide whether it becomes part of the portfolio $P$ or not.
\begin{align*}
\argmin\limits_{P \subseteq S, |P| = k} c_{T}(P)
\end{align*}%
%
Note that the portfolio cost function decreases monotonically under the addition of solvers: $\forall s \in S, c_{T}(P \cup \{s\}) \leq c_{T}(P)$. 
In the following, we outline four approaches for either \emph{exact} or \emph{approximate} determination of solutions to that function. 

\subsection{Exhaustive Search (exact)}

The simplest way to solve the $k$-Portfolio Problem is by exhaustively searching all portfolios with at most $k$ solvers. 
%Due to the use of a VBS, adding a solver to the portfolio cannot increase costs.
%Thus, it is sufficient to search over all portfolios with exactly $k$ solvers, ignoring smaller portfolios.
Since there are $\binom{n}{k}$ possible portfolios to search, this becomes infeasible for sufficiently large $n$ and $k \gg 1$ as well as $k \ll n$.
For example, with the $n=48$ solvers in our dataset, there are $1128$ portfolios for $k=2$, but roughly $6.54 * 10^9$ portfolios for $k=10$.

\subsection{Integer Programming (exact)}

The $k$-Portfolio Problem is not linear due to the use of the $\min$ function.
However, one can make it an integer linear problem by introducing additional variables.
This allows to obtain an exact solution for the problem with an off-the-shelf integer-programming solver.

We introduce two sets of binary decision variables. 
The variables $y_s$ denote whether a solver $s \in S$ is in the portfolio, and 
variables $x_{s,i}$ denotes whether solver $s \in S$ is selected for instance $i \in I$. 
Equation~\ref{eq:ip1} specifies the cardinality constraint on the number of solvers. 
Equation~\ref{eq:ip2} stipulates that exactly one solver is chosen for each instance. 
Equation~\ref{eq:ip3} ensures that a solver can only be chosen for an instance if it is part of the portfolio. 
Ultimately, the optimization target is specified by Equation~\ref{eq:ip4}. 

\begin{align}
%	\forall_{s \in S}~y_s &\in \{0,1\}\\
%	\forall_{i\in I} \forall_{s \in S}~x_{s,i} &\in \{0, 1\}\\
	\sum_{s \in S} y_s &\leq k \label{eq:ip1}\\
	\forall_{i\in I} \sum_{s \in S} x_{s,i} &= 1 \label{eq:ip2}\\
	\forall_{s \in S} \sum_{i \in I} x_{s,i} &\leq |I| \cdot y_s \label{eq:ip3}\\
	\min_{x,y} \quad & \frac{1}{|I|} \cdot \sum_{i \in I} \sum_{s \in S} r_T(s,i) \cdot x_{s,i} \label{eq:ip4}
%\label{eq:small-portfolio-integer-problem}
\end{align}

\subsection{Beam Search (approximate)}

% IEEE wants algorithms as figures, not separate floats (as "algorithm2e" does), so we need some hacking: https://tex.stackexchange.com/questions/147598/how-to-use-the-algorithm2e-package-with-ieeetran-class
\begin{figure}[b]
\makeatletter
\let\@latex@error\@gobble
\makeatother
\begin{algorithm}[H]
\DontPrintSemicolon
	\KwIn{Solvers $S$, Portfolio Size $k$, Portfolio Cost $c_T$}
	\KwIn{Beam Width $w$}
	\KwOut{Portfolio $P$ with $|P|=k$}
	\KwData{Sets of $i$-Portfolios $T_i$}
	\BlankLine
	$T_0 \leftarrow \{\emptyset\}$\;
	\For{$i \leftarrow 1$ \KwTo $k$}{
		\tcp{Candidate $i$-Portfolios:}
		$U \leftarrow \emptyset$\;
		\ForEach{$P \in T_{i-1}$}{
			\ForEach{$s \in S \setminus P$}{
				$U \leftarrow U \cup \{ P \cup \{ s \} \}$\;
			}
		}
		\tcp{Select $w$ best $i$-Portfolios:}
		$T_i \leftarrow \emptyset$\;
		\For{$j \leftarrow 1$ \KwTo $w$}{
			$T_i \leftarrow T_i \cup \{\argmin\limits_{P \in U \setminus T_i}{c_T(P)}\}$\;
		}
		%$T_i \leftarrow \mathsf{sort}(U, P_0 < P_1 \leftrightarrow c_T(P_0) < c_T(P_1))$\;
		%$T_i \leftarrow \{P_j \in U \mid j \leq w \}$\;
	}
	\Return $\argmin\limits_{P \in T_k}{c_{T}(P)}$\;
\end{algorithm}
\caption{Beam Search Algorithm}
\label{al:beam-search}
\end{figure}

\emph{Beam search} is a greedy algorithm that finds portfolios in an iterative manner.
Figure~\ref{al:beam-search} depicts the approach. 
In each iteration, the algorithm combines the portfolios from the previous iteration with individual solvers which are not part of these portfolios. 
I.e., the algorithm expands existing portfolios by adding single solvers. 
Before going to the next iteration, only the $w$ portfolios with the lowest cost are retained.
The beam-width $w$ is an input parameter.
For $w=1$, only one portfolio remains at the end of each iteration.
We refer to this special case as \emph{greedy search}.
For $w = |S|$, the algorithm degrades to exhaustive search. 
For $w << |S|$, it yields a runtime advantage compared to exhaustive search, as it only evaluates $O(|S| \cdot w)$ portfolios per iteration. 
%Note that selection of $w$ best portfolios in Lines~7-9 can be implemented with sorting. 

The algorithm is a greedy heuristic that does not necessarily find the optimal solution to the $k$-Portfolio Problem. 
But there is a bound on the costs of a portfolio found by \emph{greedy search}. 
To this end, one can use a result from~\cite{nemhauser1978analysis}, which applies to greedy algorithms on non-negative monotone submodular set functions.
Nof and Strichman show that their \emph{K-Algorithms Max-Sum Problem} for portfolios is submodular and thus a bound on greedy algorithms holds~\cite{nof2020real}. 
We can transform the $k$-Portfolio Problem (minimization) into the \emph{K-Algorithms Max-Sum Problem} (maximization) by replacing the costs with utilities as follows.%
$$
u_{T}(P) := c_W - c_{T}(P)
$$
In this transformation $c_W$ denotes an upper bound on portolio performance, the virtual worst solver.%
$$
c_W := \frac{1}{|I|} \sum_{i \in I}{\max\{{r_T(s,i) \mid s \in S}}\}
$$
As $u_{T}(P)$ is non-negative, monotone and submodular, we get the following bound on a greedy-search result $P_{greedy}^k$~\cite{nemhauser1978analysis, krause2014submodular}.%
$$
	u_{T}(P_{greedy}^k) \geq (1 - \frac{1}{e}) \cdot \max_{|P| \leq k}{u_{T}(P)}
$$
I.e., there is a lower bound on the utility of a portfolio found by \emph{greedy search}.
This can be transformed into an upper bound on the cost of a portfolio found by \emph{greedy search}.%
$$
	c_{T}(P_{greedy}^k) \leq (1 - \frac{1}{e}) \cdot \min_{|P| \leq k}{c_{T}(P)} + \frac{1}{e} \cdot c_W
$$

\subsection{K-Best (approximate)}

This is a baseline used in~\cite{nof2020real}.
It sorts all solvers by their individual performance and then picks the top $k$ from this list.
Thus, opposed to \emph{beam search}, it does not consider how solvers within a portfolio interact, e.g., if they complement each other.

\section{Experimental Design}
\label{sec:experimental-design}

In our experiments, we evaluate the presented optimization approaches, and analyze their relation to prediction approaches for $k$-Porfolios. 
In particular, we are interested in the achievable performance improvements with increasing values for the parameter $k$. 

\subsection{Optimization Approaches}

We analyze the $k$-Portfolio Problem for three definitions cost-functions in the objective function .

\begin{itemize}
	\item \emph{PAR-2}: The penalized runtimes $r_T$ as described in Section~\ref{sec:preliminaries}.
	\item \emph{PAR2\_norm}: The penalized runtimes, but normalized to $[0,1]$ per instance. This counterbalances variations in instance hardness, i.e., that some instances are easier to solve than others.
	\begin{align*}
		\overline{r_T}(s,i) := \frac{r_T(s,i) - \min\limits_{s' \in S}{r_T(s',i)}}{\max\limits_{s' \in S}{r_T(s',i)} - \min\limits_{s' \in S}{r_T(s',i)}}
	\end{align*}
	\item \emph{Unsolved}: A heavily discretized version of the runtimes, only stating whether a solver did not finish within $T$.
	\begin{align*}
		r_{0/1}(s,i) &:= \begin{cases}
			1 & \text{if }r_T(s,i) = 2 \cdot T\\
			0 & \text{otherwise}
		\end{cases}
	\end{align*}
\end{itemize}

To test generalization of search and prediction approaches, we conduct five-fold cross-validation, and average evaluation metrics over the folds. 
%I.e., we split the instances of each of the three problems five times into a training and test set.
We search for portfolios and train prediction models only on the training sets.

We employ the four solution approaches from Section~\ref{sec:approach} to search for portfolios.

\begin{itemize}
	\item \emph{Random search}:
	%Exhaustively evaluating all portfolios for all sizes $k$ is too expensive in our scenario, in particular, when training prediction models for each portfolio, as there are $2^{48}$ portfolios overall. (ergibt keinen Sinn, das in dem Zusammenhang zu erwÃ¤hnen)
	To get an idea how the performance of arbitrary portfolios is distributed, we randomly sample 1000 portfolios for each $k$.
	\item \emph{Exact search}:
	We solve the $k$-Portfolio Problem to exactly determine the best portfolio for each $k \in \{1, \dots, |S|\}$.
	\item \emph{Beam search}: 
	We use this approach to search for good portfolios heuristically.
	We evaluate all $k \in \{1, \dots, |S|\}$ and also vary the beam width $w \in \{1, 2, \dots, 10, 20, \dots, 100\}$.
	\item \emph{K-best}:
	We use this approach to have a simple baseline for \emph{beam search}.
	We evaluate all $k \in \{1, \dots, |S|\}$.
\end{itemize}

\subsection{Prediction Approaches}

%All the solution approaches from the previous section use an oracle, the VBS, to choose a solver from the portfolio for an instance.
We also analyze the performance of portfolios which use a prediction model to select a solver. 
We train such prediction models for the portfolios found by our previously devised solutions for determining optimal $k$-portfolios. 

For each instance, the prediction target is the best solver out of the given $k$-portfolio.
As models, we use random forests~\cite{breiman2001random}, which are ensembles of decision trees. 
Preliminary experiments with single decision trees~\cite{breiman1984classification} of different depths yielded worse prediction performance. 
Random forests are also used in the well-known portfolio approach SATzilla2012~\cite{xu2012satzilla2012}. 
To analyze if prediction performance improves due to ensembling, we train models with one, ten, and 100 trees.
We evaluate prediction performance in two ways:

\begin{itemize}
	\item \emph{Objective value}:
	We evaluate the prediction models with respect to the objective value, i.e., the runtime cost function $c_{T}'(P)$ (cf. Section~\ref{sec:preliminaries}). 
	\item \emph{MCC}:
	We evaluate the predictions with Matthews correlation coefficient (MMC)~\cite{matthews1975comparison, gorodkin2004comparing}.
	This does not take into account how fast the recommended solvers actually are, but only if the fastest solver is recommended or not.
	We use MCC instead of simpler metrics like accuracy, as the class labels might be imbalanced, i.e., one solver might be the fastest for most of the instances, and always predicting that solver would already yield a high accuracy.
	MCC has a range of $[-1,1]$, being zero for both random guessing and always guessing the same solver.
\end{itemize}

\subsection{Dataset}

In our experiments, we use a dataset of $|S| = 48$ solvers and $|I| = 400$ instances from the Main track of the SAT~Competition~2020\footnote{\url{https://satcompetition.github.io/2020/}}~\cite{balyo2020proceedings, SC2020:AIJ}. 
We filtered out those instances where no solver finished in time, resulting in a dataset of $316$ instances. 

For predictions, we make use of 143 features to characterize instances, of which 
$138$ features stem from the feature extractor of SATzilla~2012~\cite{xu2008satzilla, xu2012satzilla2012} and five features stem from a gate-recognition approach~\cite{iser2020recognition}. 
Feature values which are missing due to the feature extractor having exceeded time- or memory limits are replaced with their training-set mean. 

\subsection{Implementation}

We implement our experimental design in Python and make our code available online\footnote{\url{https://github.com/Jakob-Bach/Small-Portfolios}}.
To create the dataset of solver runtimes, we use the package~\emph{gbd-tools}~\cite{iser2020collaborative}.
For predictions, we use the package \emph{scikit-learn}~\cite{scikit-learn}.
To solve the small portfolio-problem exactly, we use the package \emph{mip}~\cite{python-mip}.

\section{Evaluation}
\label{sec:evaluation}

In the following, we evaluate the performance of the optimal and approximate solutions to the $k$-Portfolio Problem. 
We subsequently relate the performance of such optimal $k$-Portfolios to the performance of an actual prediction model for instance-specific solver selection. 
In particular, we are interested in the influence of the portfolio size $k$ on performance. 

\subsection{Optimization Results}

The dataset seems promising for portfolios.
Regarding \mbox{PAR-2} score, there is no single solver which is fastest for all or even for a majority of the $316$ instances. 
The three solvers which are fastest on average (in terms of their PAR-2 Score), `win' on only 46, 38, and 26 instances, respectively. 
Regarding the number of solved instances, the three best single solvers still leave 52, 55, and 61 instances unsolved. 
This indicates that combing solvers in portfolios can improve overall runtime.

\subsubsection{Optimizing the PAR-2 Score}

\begin{figure*}[t]
	\centering
	\subfloat[Training data.]{
		\includegraphics[width=0.98\columnwidth]{plots/search-train-objective-PAR2.pdf}
		\label{fig:search-train-objective-PAR2}
	}
	\hfil
	\subfloat[Test data.]{
		\includegraphics[width=0.98\columnwidth]{plots/search-test-objective-PAR2.pdf}
		\label{fig:search-test-objective-PAR2}
	}
	\caption{Comparison of different solution approaches for objective \emph{PAR2}. Portfolio VBS Performance for different k and search algorithms. Optimized on Training (left), Evaluated on Test (right), five-fold cross-validation. 
\todo{use log-scale for y-axis}
\todo{rename k-best search to k-best, it's not \emph{search}}
\todo{remove upper-bound}
\todo{rename mip-search to optimal solution}
\todo{add beam search results for w=100}
\todo{rename random search to random sampling}}
\label{fig:search-train-objective}
\end{figure*}

Figure~\ref{fig:search-train-objective} displays the cost in terms of the \emph{PAR-2} function of the best $k$-portfolios for $1 \leq k \leq 48$ as they are found by different solution approaches. 
The performance of the optimal solution is given by \emph{MIP search}. 
\emph{Beam search} denotes the performance of the best portfolios found by a \emph{greedy search} algorithm, i.e., executing the \emph{beam search} algorithm with the smallest beam width $w=1$. 
The approach \emph{k-best} displays the performance of portfolios which are comprised by the top $k$ single-best solvers. 
In \emph{random\_search}, we average over the repetitions of random sampling.
If we report numbers in the following, we refer to the training dataset for which the optimization problems are solved.

For all approaches, the objective value improves rapidly for the first few $k$, but marginal gains become smaller with increasing $k$.
For the optimal solution, the best $1$-portfolio has a penalized runtime $5.51$ times as high as the $48$-portfolio. 
This ratio reduces to $1.89$ for the best $5$-portfolio and $1.25$ for the best $10$-portfolio. 

As Figure~\ref{fig:search-train-objective-PAR2} also shows, the cost of the best $k$-portfolios found by \emph{greedy search} is very close to that of the exact solutions.
These results are particularly impressive considering that the runtime of \emph{greedy search} is linear in $k$ as well as the number of instances $n$, while the finding the exact solution is NP-complete.
In contrast, the theoretical, submodularity-based upper bound for \emph{greedy search} is clearly higher and therefore too loose to give a good estimate. 
\todo{reference upper bound again}

The baseline \emph{k-best} is worse than \emph{greedy search} on the training data, but still clearly better than \emph{random search}.
As Figure~\ref{fig:search-test-objective-PAR2} shows, \emph{beam search}, \emph{k-best}, and \emph{MIP search} show similar performance on the test dataset.
In particular, there is no clear winner, and the exact solution can even perform worse than those found by the approximate approaches. 
This is because optimization is run on the training instances only, and the best portfolio on the training data is not necessarily the best portfolio on the test data.
Still, training and test performances are rather similar.

On the training data, the best $2$-portfolio found by \emph{greedy search} has a $14.7\%$ higher cost than that found by \emph{MIP search}, while the $2$-portfolio of \emph{k-best} has a $28.4\%$ higher cost than that found by \emph{MIP search}. 
For all other $k$, the greedy-search portfolio has less than $4\%$ higher penalized runtime than the exact solution. 

In contrast, the performance gap of \emph{k-best} portfolios to the exact solution widens for the first few $k$, before becoming smaller again later.
For $k=8$, where the relative performance gap is the largest, the \emph{k-best} portfolio has a 51.1\% higher penalized runtime than the exact solution.

\subsubsection{Alternative Objective Functions}

Since the objective functions \emph{PAR2\_norm} and \emph{Unsolved} follow a similar pattern as \emph{PAR-2}, we omitted them in our plots. 
For the objective function \emph{Unsolved}, a portfolio of ten solvers suffices to solve all instances, while the best $5$-portfolio leaves $1.7\%$ of instances unsolved and the best $1$-portfolio leaves $16.4\%$ of instances unsolved.

For the objective function \emph{Unsolved}, the greedy-search portfolio for $k=2$ solves only $1.9\%$ out of all instances less than the exact solution, and this difference decreases further with $k$.
\emph{K-best} solves up to $4.3\%$ of all instances less than the exact solution.

\subsubsection{Beam Search Width}

To bring the \emph{beam search} solution even closer to the exact solution, one can increase $w$.
E.g., for $w=10$ and for all $k$, the PAR-2 score is never more than $2\%$ higher than the best exact solution and the difference in unsolved instances is never more than $0.1\%$ of all instances.
Regarding the set of the $w$ best portfolios within each iteration of \emph{beam search}, we observe convergence with increasing $k$. 
We observe a large variance in the PAR-2 score of portfolios in the beam for small $k$, and this variance becomes smaller in later iterations, i.e., the top $w$ portfolios become more similar. 

A similar phenomenon also occurs for randomly sampled portfolios. 
With increasing $k$, the standard deviation of the objective value in a sample of portfolios decreases.
At the same time, the expected objective value converges towards the objective value of the optimal k-portfolio.
This is clearly visible in Figure~\ref{fig:search-train-objective}.
Thus, the benefit of searching for good portfolios is the greatest for small $k$.

\subsubsection{Monotonicity of $k$-Portfolios}

While \emph{beam search} iteratively adds solvers, the exact solution might differ in more than one solver from $k-1$ to $k$, i.e., existing solvers from the portfolio can be replaced.
Indeed, we observe this phenomenon frequently in our results.
For example, the best $2$-portfolios usually do not contain the best $1$-portfolio in our experiments.
However, despite this non-monotonous behavior of the exact solution, \emph{beam search} still yields a close-to-optimal objective value.
This indicates that there are several good solver combinations, so solvers can substitute each other in portfolios to some extent.

This substitution effect seems to be particularly present for large $k$, as we can observe for \emph{beam search} with $w=100$. 
For the objective function \emph{PAR-2}, one solver appears in 19\% of the beam of $2$-portfolios.\todo{how? there are only two iterations}
However, for $k=10$, no solver appears in more than 10\% of the portfolios and for $k=20$, no solver appears in more than 5\% of the portfolios.
A similar effect shows for the objective functions \emph{PAR2\_norm} and \emph{Unsolved}.
This shows that a few solvers might be crucial for good objective value for small $k$, but there is more flexibility in building a good portfolio for large $k$.

\subsubsection{Random Portfolios}

A correlation analysis on random-search results, of which we have 1000 portfolios for each $k$, also underlines the interaction between solvers.
First, we encode the absence or presence of each solver in a portfolio with 0 or 1 respectively.
Next, we compute the Spearman rank correlation between this occurrence vector with the objective value.
For $k=5$, all correlations are in $[-0.40,0.17]$ for objective function \emph{PAR2}, $[-0.42,0.17]$ for \emph{PAR2\_norm}, and $[-0.41,0.15]$ for \emph{Unsolved}.
The mean correlation is zero for all objective functions.
Similar correlation behavior occurs for other $k$.

These results indicate that only the presence of some solvers has a moderately negative correlation to the objective value of the whole portfolio, i.e., only some solvers can clearly improve the portfolio performance of our minimization problem on their own.
The positive correlations are even weaker, i.e., there are no solvers which influence portfolio performance in a strongly negative manner.
Overall, this means one needs a combination of solvers to clearly influence the objective value in either direction.\todo{last statement is trivial. what do you want to say?}

\subsection{Prediction Results}

In this section, we evaluate the prediction performance for the previously determined portfolios. 
We also evaluate the actual performance of the resulting portfolios in terms of their penalized average runtimes. 

\subsubsection{Matthews Correlation Coefficient}

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{plots/prediction-test-mcc.pdf}
	\caption{Test-set MCC for random-search portfolios. Random forests with 100 trees. (mcc: 0 - random/constant, 1 - perfect prediction)}
	\label{fig:prediction-test-mcc}
\end{figure}

Figure~\ref{fig:prediction-test-mcc} shows test-set classification performance of random forests with 100 trees, using the portfolios from \emph{random search}.
Classification performance with less trees is worse, on training set as well as test set.
We use random-search results here, but classification results are similar for \emph{beam search} with $w=100$.
We do not display training-set performance, since it always is 1.0.
As Figure~\ref{fig:prediction-test-mcc} shows, test-set prediction performance is rather low for all $k$.
It is clearly higher than random guessing, which would yield an MCC of 0.0, but clearly lower than the optimal MCC of 1.0.
With a perfect training-set performance, but a low test set performance, the model seems to be overfitting to the training set.
This could cause a bad objective value for model-based portfolios, as we will analyze latter.
For low $k$, the prediction performance varies stronger between portfolios than for larger $k$.

\subsubsection{Portfolio Performance}

\begin{figure*}[t]
	\centering
	\subfloat[\emph{PAR2} objective function.]{
		\includegraphics[width=0.98\columnwidth]{plots/prediction-test-objective-PAR2.pdf}
		\label{fig:prediction-test-objective-PAR2}
	}
	\hfil
	\subfloat[\emph{Unsolved} objective function.]{
		\includegraphics[width=0.98\columnwidth]{plots/prediction-test-objective-unsolved.pdf}
		\label{fig:prediction-test-objective-unsolved}
	}
	\caption{Objective value for \emph{beam search} portfolios with $w=100$. Random forests with 100 trees. Portfolio performance for test set vbs (orange) and prediction model (blue)}
	\label{fig:prediction-test-objective}
\end{figure*}

Figure~\ref{fig:prediction-test-objective} shows test-set objective value for portfolios from \emph{beam search} with $w=100$.
The plot compares the objective value of portfolios with an oracle, the VBS, to the objective value of portfolios with random forests with 100 trees
As discussed before, VBS score decreases with $k$.
In theory, this also allows portfolios with prediction model to potentially improve their performance.

However, in our case, the objective value of model-based portfolios remains rather stable with increasing values of $k$.
This implies that the prediction model cannot help to leverage the larger amount of solvers in the portfolio.
Given the low prediction performance in terms of MCC, as previously seen in Figure~\ref{fig:prediction-test-mcc}, this is expected.
As a consequence, the gap between VBS and the predicted solver grows with $k$.
Also, the objective value of model-based portfolios varies more than that of VBS-portfolios.
This probably is related to the considerably variance in prediction performance, which is shown in Figure~\ref{fig:prediction-test-mcc}.

\subsubsection{Feature Importance}

Averaging importance over all trained models, the most important feature has an importance of roughly 2\%, the least important feature has an importance of 0\%.
To reach an cumulated average importance of 50\%, one needs 49 out of 143 features.
On average, prediction models use 106 features.
%Note that these figures includes random forests of all sizes, i.e., even the largest ones with 100 trees.
If we limit our analysis to random forests with just one tree, the models still use 52 features on average.
In conclusion, no single feature or even small sets of features drives prediction performance.
Considering the low prediction performance, this indicates that our dataset lacks discriminating features which are suitable to decide which solver should be used on an instance.

\todo{result for k=3 is best}

\todo{get rid of alternate objective functions (PAR-2 Norm and Unsolved) as in this context we cannot learn anything more from them}

\todo{evaluate prediction performance also for the optimal k-portfolios}

\section{Conclusions and Future Work}
\label{sec:conclusion}

SAT solvers are often complementary, i.e, they are particularly good on some instances and less good on other instances.
Thus, one combines several solvers into portfolios.
We analyzed such portfolios with runtime data from the SAT~Competition~2020.
In particular, we focused on portfolios with a limited number of solvers, so-called \emph{small portfolios}.

We analyzed an integer optimization problem to find small portfolios exactly as well as a \emph{beam search} approach to find small portfolios fast.
Regarding portfolio size, we saw a strong improvement in objective value for adding solvers to small portfolios, but a small impact once the portfolio reaches a certain size.

Regarding the solution approach, we found that \emph{beam search} yields close-to-optimal solutions.
After determining portfolios, we combined them with prediction models that made instance-specific solver recommendations.
However, these models performed rather poorly on the given dataset, such that the objective value of portfolios with predictions did not improve with the portfolio size.
We found, that prediction performance does not improve beyond portfolio sizes of $k=3$. 
\todo{check validity of previous statement}

In future work, we want to improve prediction performance, in particular, by adapting the set of instance features.
Recent results in the empirical analysis of solver portfolios indicate that features which describe the community structure of graph representations of SAT instances correlate well with solver performance. 
Among those are features describing their hierarchical community structure, such as community leaf size, community degree, and the number of inter-community edges~\cite{Li:2021:HCS}. 
Also, we want to integrate our portfolio-search functionality into the Python package~\emph{gbd-tools}, so it can be directly used in queries to the database GBD.

\section*{Acknowledgments}

\todo{check if Jakob and/or Markus need to add stuff here}

\balance % according to documentation, command might not work if issued to late in document, so hopefully placing it here suffices

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
