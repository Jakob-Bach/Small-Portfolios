\documentclass[conference]{IEEEtran}

%\usepackage[style=ieee, backend=bibtex]{biblatex} % handled by IEEE template at the moment
\usepackage[linesnumbered,vlined]{algorithm2e} % pseudo-code; use of package discouraged by IEEE, but hacked in in a (hopefully acceptable) way
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{amsthm} % mathematical theorems
\usepackage{balance} % balance columns on the last page
\usepackage{booktabs} % nicely formatted tables
\usepackage{graphicx} % plots
\usepackage[caption=false,font=footnotesize]{subfig} % figures with multiple sub-figures and sub-captions; use of (newer) package "subcaption" discouraged by IEEE
\usepackage{xcolor} % colored text (for TODOs)
\usepackage{hyperref} % links and URLs; should be loaded last

%\addbibresource{references.bib} % handled by IEEE template at the moment

\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\todo}[1]{{\color{red}TODO: #1}}

\begin{document}

\title{Analyzing k-Portfolios of SAT Solvers}

\author{\IEEEauthorblockN{Jakob Bach}
\IEEEauthorblockA{\textit{KIT Department of Informatics} \\
\textit{Karlsruhe Institute of Technology}\\
Karlsruhe, Germany \\
jakob.bach@kit.edu}
\and
\IEEEauthorblockN{Markus Iser}
\IEEEauthorblockA{\textit{KIT Department of Informatics} \\
\textit{Karlsruhe Institute of Technology}\\
Karlsruhe, Germany \\
markus.iser@kit.edu}
}

\maketitle

\begin{abstract}
Successful approaches that tackle hard combinatorial problems such as propositional satisfiability are often complementary, i.e., an approach only works best on some problem instances, but not on a great many of them. 
Parallel portfolios and instance-specific algorithm selection try to exploit this.
However, portfolio approaches are in competition with the progressive improvements of individual solvers.
It therefore remains an open question to what extent complementarity can be exploited in recent solvers. 
In this paper, we present a systematic analysis of portfolios of recent SAT solvers using runtime measurements from the SAT~Competitions~2020 and~2021. 
First, we present and compare search approaches to find good portfolios of limited size~$k$.
Second, we use prediction models to make instance-specific solver recommendations within the portfolios and evaluate this approach as well.
We show that greedy solutions for finding good portfolios in practice compete well with optimal solutions. 
We also show that the marginal returns of adding more solvers diminish with increasing portfolio size, which becomes particularly evident in combination with prediction models.
\end{abstract}

\begin{IEEEkeywords}
Propositional Satisfiability, Solver Portfolios, Runtime Prediction Models, Machine Learning
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

\paragraph{Motivation}

SAT solving is the archetypal NP-complete problem.
Its practical applications abound, e.g., verification of hardware and software~\cite{Kaufmann:2021:Amulet,Buning:2020:QPRVerify}, product configuration~\cite{Janota:2014:Configuration}, cryptanalysis~\cite{Nejati:2020:CDCLCrypto}, or planning~\cite{Schreiber:2021:Lilotane}.
SAT solvers are not only used to solve hard combinatorial problems in industry, but also previously open problems in mathematics~\cite{Heule:2016:Pyth,Heule:2018:Schur}. 

Nowadays, one can observe continuous progress regarding SAT solving methods, heuristics, and their implementations in SAT solvers. 
Evaluation of solvers is commonly based on compilations of benchmark instances which represent \emph{diverse} interesting application scenarios. 
An important design objective behind new SAT solvers is \emph{stability}, i.e., good performance on many types of instances. 

At the same time, we observe a recurrent emergence of new heuristics and methods which yield improved performance on only a narrow subset of instances. 
Such approaches have a merit nevertheless -- one can see them as complementary to solvers with good average behavior. %KB: Hier ist mir die Akzentuierung, die Sie wollten, nicht ganz klar. Passt das so?
To make such narrow progress explicit, one tends to evaluate solvers exclusively on specific problem types and instances.

\begin{example}[Crypto Track in SAT Competition 2021]
Recently, an application track in SAT Competitions where the participating solvers are evaluated on a large benchmark set from purely one specific application has been introduced. This has uncovered the following phenomenon: The SAT Solver \emph{SLIME} has not been competitive in the evaluations on the diverse set of instances in the Main Track of SAT Competition~2021. 
But \emph{SLIME} achieved the best score in the Crypto Track where SAT solvers were evaluated on a set of $200$ instances exclusively from cryptographic applications. 
\end{example}

However, another possibility to quantify the progress in SAT solving beyond measuring the average performance of each solver exists as well, namely one based on portfolios. 
Portfolios are sets of solvers.  
One can combine a solver portfolio with a perfect oracle which then selects the fastest solver for each instance in the benchmark set. 
This construction is commonly referred to as the \emph{Virtual Best Solver} (VBS). 
One can then derive a ranking of solvers based on their marginal contribution to the VBS, cf.~\cite{Xu:2012:EvalContribVBS}. 

\begin{example}[Special Price in SAT Competition 2021]
In SAT Competition 2021, the performance of the VBS of all portfolios of size two over the entirety of participating solvers has been evaluated. 
The best performing pair of solvers in terms of their VBS contains the solver \emph{CaDiCaL\_PriPro}. This solver has not been competitive in the overall evaluation, but, based on that achievement has been awarded the new \emph{Special Innovation Price}.
\end{example}

Yet another tweak when it comes to performance gains from sets of solvers is as follows: Combining a portfolio with a solver-election process which includes runtime prediction models can outperform individual best solvers by much~\cite{xu2008satzilla}. 
However, a downside is that respective constructions are complex and quite time-consuming, so comparative studies in SAT solving more often than not do not implementations applying this principle to state-of-the-art solvers.

Given the situation sketched so far, the following questions are important when it comes to the design of portfolios and their evaluation. 
First, one must decide how many solvers to put in a portfolio, and which ones. 
A $k$-portfolio is a set of $k$ solvers drawn from a given set of $n \geq k$ solvers $S$. 
A $k$-portfolio is optimal if its VBS outperforms that of all other $k$-portfolios which can be drawn from $S$. 
The \emph{$K$-Portfolio Problem}, which is to determine an optimal $k$-portfolio, is an NP-hard optimization problem~\cite{nof2020real}. %KB: Vielleicht sollte hier das Wort 'search' explizit auftauchen, damit weiter unten der Bezug klar ist. Auch der Begriff 'solution', den Sie nachher benutzen, sollte hier in gleicher Weise verankert werden.
Second, putting together a dataset which contains instance features as well as runtime data, the training of instance classifiers and runtime prediction models requires a considerable amount of computation time. %KB: Hier vielleicht sprechend. Mir ist unklar, warum es diesen Satz gibt, bzw. was Sie hier sagen wollen.


\paragraph{Contributions}

This current article features a study which \emph{captures and evaluates} the state-of-the-art in SAT solving in terms of portfolio performance. It is based on very recent datasets which contain the runtimes of solvers having participated in SAT~Competitions~2020 and~2021. Our study is unique in that it addresses all points raised so far, as follows:
We systematically construct $k$-portfolios and analyze the impact of the portfolio size $k$ on portfolio performance. 
For exact search, we encode the K-Portfolio Problem as an \emph{integer optimization problem}. %KB: Das scheint neu zu sein, wenn ich Sie richtig verstanden habe. Wenn ja, sollten Sie das hier explizit sagen, und auch kurz, warum das besser ist als die Alternativen.
Spurred by the complexity of the problem, we also analyze the quality of various approximate and random solutions.  
An important takeaway of our study is that it gives way to the \emph{creation of optimal portfolios} in an ad-hoc manner. 
In a subsequent step, we study the prediction quality of solver runtimes which one can achieve with \emph{simple} models.
For each portfolio size $k$, we train such models using random forests, use the widely known instance feature records of SATzilla~2012~\cite{xu2012satzilla2012} to this end, and then use the predictions to select portfolio members automatically in a straightforward manner. 
Our code and data are available online.


\paragraph{Results}
While it is expected that the limit utility decreases with portfolio size, we did not expect the decrease to be so early -- values of $k$ larger than $3$ did not yield any benefit in our experiments. 
Another interesting observation is that well-known \emph{beam search} has given way to the best approximate solutions to the $K$-Portfolio Problem. 
These solutions happen to perform very similarly to the optimal ones, even for a minimal beam width of one. 
On the other side, relying on a simple prediction model has been a mixed bag according to our experiments: On the negative side, \emph{prediction performance} has been rather low. 
The \emph{actual runtime} of portfolios with predictive models is not even close to the theoretical improvement one could have with a perfect oracle.
We take this as an indication that more research is necessary; our hypothesis is that the instance features available in our dataset are not sufficiently conclusive. 
On the positive side, we have observed considerably performance improvements with runtime prediction for $k$-portfolios up to portfolios of size $k = 4$. %KB: Verglichen womit? Bitte explizit sagen.
For larger $k$, the runtimes of portfolios with predictive models does not improve further. 
We take this as a further indication that it might not be beneficial to combine very many solvers to a portfolio. 


\paragraph{Outline}

In Section~\ref{sec:related-work}, we review related work. 
Preliminaries are given in Section~\ref{sec:preliminaries}. 
In Section~\ref{sec:approach}, we introduce the \emph{K-Portfolio Problem} and several solution approaches. 
We describe the experimental design in Section~\ref{sec:experimental-design} and present the experimental results in Section~\ref{sec:evaluation}. 
We conclude with Section~\ref{sec:conclusion}. 

\section{Related Work}
\label{sec:related-work}

%KB: Hier würde sich ein einleitender Satz gut machen, wie das Folgende aufgebaut/angeordnet ist.
Solver complementarity with respect to their contributions to the runtime of a solver portfolio has been evaluated by Xu et al.~\cite{Xu:2012:EvalContribVBS}. 
They show that large contributions to the runtime of a portfolio often come from solvers which are not competitive when evaluated as a standalone solver. 
This however depends on the dataset under evaluation. 
In fact, the organizers of SAT~Competition~2020 showed in~\cite{SC2020:AIJ} that the best $k$-portfolios ($k \in [1,3]$) drawn from participating solvers are comprised of solvers which actually received an award in the standalone evaluation.
In particular, the best performing $2$-portfolio of solvers consists of the winners of the SAT and UNSAT tracks. 
This has been fundamentally different in SAT~Competition~2021. Here, a solver which was not competitive in the standalone evaluation has been part of the best performing $2$-portfolio (drawn from the solvers in that particular competition). 
All this underlines the importance of portfolio analysis for good stable solvers. Here, progress often manifests itself through complex hybridization of methods. 

Instance-specific solver selection approaches bring in another perspective on solver complementarity.
One of the best-known complex solver-selection approaches is SATzilla, of which multiple versions exist~\cite{xu2008satzilla, xu2012satzilla2012}. 
They combine instance classification with runtime prediction models in a mixture-of-experts approach in order to select a solver in their portfolio. 
\todo{JB: "mixture-of-experts" is not clear to me} %KB: "in a so-called mixture-of-experts approach" würde hier schon reichen.
Additional components of SATzilla are so-called \emph{pre-solvers}, which are only run for a short time to solve easy instances, and a \emph{backup solver}, which is responsible for instances where computing features for the prediction models takes too long.
Overall, portfolio approaches can become quite complex, and it is unclear which parts of the portfolio solution are really necessary to achieve a good performance %KB: "elements ofthe portfolio" anstelle von "parts of the portfolio solution"?

Other well known instance-specific solver selection approaches are ISAC, which is based on clustering in the instance feature space~\cite{Kadioglu:2010:ISAC}, and SNNAP, which combines the ISAC approach with runtime prediction models~\cite{Collautti:2013:SNNAP}.
%Besides complex portfolio approaches, there are also simple ones, e.g., selecting a solver nearest-neighbor-based, without training a prediction model~\cite{malitsky2011non, nikolic2013simple, samulowitz2013snappy}.%MI: what are they doing?
A recent survey on algorithm-selection approaches -- also beyond SAT -- is provided by Kerschke et al.~\cite{kerschke2019automated}.

Many portfolio approaches have several stages and configuration options, which makes configuring and selecting the best portfolio approach difficult.
To cope with this, Autofolio by Lindauer et al.\ provides an automatic configurator~\cite{lindauer2015autofolio}.
One can also analyze the impact of configuration options, e.g., with the tool CAVE~\cite{biedenkapp2018cave}.

One systematic analysis of portfolios with varying size~$k$, which has some similarities to our study, is conducted by Amadini et al.~\cite{amadini2014empirical, amadini201 6extensive}.
They evaluate $k$-portfolios of CSP solvers in terms of their average runtime and solved instances. 
They compare several classification methods within portfolios, besides adapting complex SAT portfolio approaches like SATzilla. 
Amadini et al. focus on different instances and solvers than this current study, though the evaluation procedure has some similarities.
%Our evaluation is less broad in terms of classification models, but instead we analyze feature importances to find out what drives the models' predictions.MI: really?
Our study features a comparison of different search strategies for portfolios. %KB: Die folgenden Sätze würde ich persönlich etwas kürzen.
For example, we systematically analyze parametrization of \emph{beam search}. 
Further, we compare against random portfolios of size $k$ as a baseline.
Finally, our comparisons also cover exact solutions of the \emph{K-Portfolio Problem}, 
while \cite{amadini2014empirical} only uses heuristic search algorithms. 

Nof and Strichman formalize the \emph{K-Portfolio Problem} in the form of two maximization problems with different objective functions, and prove their submodularity and NP-completeness~\cite{nof2020real}. 
They solve the \emph{K-Portfolio Problem} optimally with an SMT solver and use \emph{greedy search} to generate approximate solutions.
They evaluate their approach on anytime search algorithms for an allocation problem, focusing on solution quality after a timeout of 1~s or less.

While we build on the theoretical results of Nof and Strichman, our evaluation is different. %KB: Das Folgende wurde eigentlich bereits gesagt. Vielleicht sollte hier anstelle von 'different' so etwas stehen wie 'breiter'?! 
In order to determine $k$-portfolios, we evaluate a \emph{beam search} algorithm with varying widths, random portfolios of size $k$ as a baseline, and for optimal solutions we propose an encoding as an integer optimization problem.
\cite{nof2020real} also does not analyze portfolios with prediction models, but only the optimization problems.

Our case study is based on two most recent datasets of SAT~Competitions~2020 and~2021. %KB: Diese Aussage grenzt Ihre Studie doch von _allen_ bis hierhin genannten Studien ab. Vielleicht möchten Sie den Satz entsprechend einleiten, z. B. "A feature of our current study which sets it apart from the existing ones, is that it is based on ..."
They contain runtimes of actual state-of-the-art SAT solvers for a very diverse set of hard SAT instances, which are measured with a timeout of 5000~s.
We not only evaluate the theoretical lower bound of runtimes of $k$-portfolios drawn from these datasets with varying size $k$ w.r.t.\ optimal and approximate solutions to the \emph{K-Portfolio Problem}. 
\todo{JB: something is missing: I expect some "but" to complement the "not only"} 
We rather evaluate the runtimes of an instance-specific algorithm selector which is trained for the such determined $k$-portfolios. %KB: 'the such determined' ist unklar (und auch schlechtes Englisch).

\section{Preliminaries}
\label{sec:preliminaries}

Let a set of solvers $S = \{s_1, \dots, s_n\}$, a set of SAT instances $I = \{i_1, \dots, i_m\}$ and solver runtimes $r:~S \times I \rightarrow [0, T]$ with a fixed timeout $T$ be given.
A scoring function $c_T : S \rightarrow \mathbb{N}$ is used to estimate solver performance. 
To score a solver, we use the frequently used penalized average runtime with a penalization factor of two (PAR-2 Score). 
This score is a trade-off between solver runtime and the number of solved instances.
It is defined as follows.%
\begin{align}
r_T(s,i) &:= \begin{cases}
	2 \cdot T & \text{if }r(s,i) = T\\
	r(s,i) & \text{otherwise}
\end{cases} \tag*{Penalized Runtimes}\\[.5em]
c_T(s) &:= \frac{1}{|I|} \sum_{i \in I}{r_T(s,i)} \tag*{PAR-2 Score}
\end{align}

A portfolio $P \subseteq S$ is a \emph{non-empty} set of solvers.
To score a solver portfolio $P$, we assume to have an oracle that always selects the fastest solver for each instance. 
This construction is commonly referred to as the virtual best solver (VBS) for $P$. 
Accordingly, we extend the scoring function as follows.%
$$
	c_{T}(P) := \frac{1}{|I|} \sum\limits_{i \in I}{\min\{r_T(s,i) \mid s \in P\}}
$$
In the following, we will refer to $c_{T}(P)$ as the \emph{cost} of $P$. 

In reality, one may train a prediction model $m : I \rightarrow P$ for a solver portfolio $P \subseteq S$ which recommends a solver for each instance, using features of the instance. 
The \emph{cost} of such a prediction model for solvers $P$ is given by the following function.%
$$
	c_{T}'(m,P) := \frac{1}{|I|} \sum\limits_{i \in I}{r_T(m(i),i)}
$$

Clearly, the portfolio cost $c_{T}(P)$ serves as a lower bound for the actual cost of a portfolio $P$ which uses a prediction model.

\section{Optimal \texorpdfstring{$k$}{k}-Portfolios} % hyperref does not want a math symbol here
\label{sec:approach}

The \emph{K-Portfolio Problem} is to find a portfolio $P$ of size $|P| = k$ with minimum costs.%
$$
\argmin\limits_{P \subseteq S, |P| = k} c_{T}(P)
$$
Note that the portfolio cost function decreases monotonically under the addition of solvers: $\forall s \in S, c_{T}(P \cup \{s\}) \leq c_{T}(P)$. 
In the following, we outline four approaches for either \emph{exact} or \emph{approximate} determination of solutions to that function. 

\subsection{Exhaustive Search (exact)}

The simplest way to solve the \emph{K-Portfolio Problem} is by exhaustively searching all portfolios with $k$ solvers. 
Since there are $\binom{n}{k}$ possible portfolios to search, this becomes infeasible for sufficiently large $n$ and $k \gg 1$ as well as $k \ll n$.
For example, with $n=48$ solvers in the SAT~Competition~2020, there are $1128$ portfolios for $k=2$, but roughly $6.54 * 10^9$ portfolios for $k=10$.

\subsection{Integer Programming (exact)}

The \emph{K-Portfolio Problem} is not linear due to the use of the $\min$ function.
However, one can make it an integer linear problem by introducing additional variables.
This allows to obtain an exact solution for the problem with an off-the-shelf integer-programming solver.

We introduce two sets of binary decision variables. 
The binary variables $y_s$ denote whether a solver $s \in S$ is in the portfolio, and 
the binary variables $x_{s,i}$ denote whether solver $s \in S$ is selected for instance $i \in I$. 
Equation~\ref{eq:ip1} specifies the cardinality constraint on the number of solvers. 
Equation~\ref{eq:ip2} stipulates that exactly one solver is chosen for each instance. 
Equation~\ref{eq:ip3} ensures that a solver can only be chosen for an instance if it is part of the portfolio. 
Ultimately, the optimization target is specified by Equation~\ref{eq:ip4}.%
\begin{align}
	\sum_{s \in S} y_s &\leq k \label{eq:ip1}\\
	\forall_{i\in I} \sum_{s \in S} x_{s,i} &= 1 \label{eq:ip2}\\
	\forall_{s \in S} \sum_{i \in I} x_{s,i} &\leq |I| \cdot y_s \label{eq:ip3}\\
	\min_{x,y} \quad & \frac{1}{|I|} \cdot \sum_{i \in I} \sum_{s \in S} r_T(s,i) \cdot x_{s,i} \label{eq:ip4}
\end{align}

\subsection{Beam Search (approximate)}

% IEEE wants algorithms as figures, not separate floats (as "algorithm2e" does), so we need some hacking: https://tex.stackexchange.com/questions/147598/how-to-use-the-algorithm2e-package-with-ieeetran-class
\begin{figure}[t]
\makeatletter
\let\@latex@error\@gobble
\makeatother
\begin{algorithm}[H]
\DontPrintSemicolon
	\KwIn{Solvers $S$, Portfolio Size $k$, Portfolio Cost $c_T$}
	\KwIn{Beam Width $w$}
	\KwOut{Portfolio $P$ with $|P|=k$}
	\KwData{Sets of $i$-Portfolios $T_i$}
	\BlankLine
	$T_0 \leftarrow \{\emptyset\}$\;
	\For{$i \leftarrow 1$ \KwTo $k$}{
		\tcp{Candidate $i$-Portfolios:}
		$U \leftarrow \emptyset$\;
		\ForEach{$P \in T_{i-1}$}{
			\ForEach{$s \in S \setminus P$}{
				$U \leftarrow U \cup \{ P \cup \{ s \} \}$\;
			}
		}
		\tcp{Select $w$ best $i$-Portfolios:}
		$T_i \leftarrow \emptyset$\;
		\For{$j \leftarrow 1$ \KwTo $w$}{
			$T_i \leftarrow T_i \cup \{\argmin\limits_{P \in U \setminus T_i}{c_T(P)}\}$\;
		}
	}
	\Return $\argmin\limits_{P \in T_k}{c_{T}(P)}$\;
\end{algorithm}
\caption{\emph{Beam search} algorithm.}
\label{al:beam-search}
\end{figure}

\emph{Beam search} is a greedy algorithm that finds portfolios in an iterative manner.
Figure~\ref{al:beam-search} depicts the approach. 
In each iteration, the algorithm combines the portfolios from the previous iteration with individual solvers which are not part of these portfolios. 
I.e., the algorithm expands existing portfolios by adding single solvers. 
Before going to the next iteration, only the $w$ portfolios with the lowest cost are retained.
The beam-width $w$ is an input parameter.
For $w=1$, only one portfolio remains at the end of each iteration.
We refer to this special case as \emph{greedy search}.
For $w = |S|$, the algorithm degrades to exhaustive search. 
For $w \ll |S|$, it yields a runtime advantage compared to exhaustive search, as it only evaluates $O(|S| \cdot w)$ portfolios per iteration. 

The algorithm is a greedy heuristic that does not necessarily find the optimal solution to the \emph{K-Portfolio Problem}. 
But there is a bound on the costs of a portfolio found by \emph{greedy search}. 
To this end, one can use a result from~\cite{nemhauser1978analysis}, which applies to greedy algorithms on non-negative monotone submodular set functions.
Nof and Strichman show that their \emph{K-Algorithms Max-Sum Problem} for portfolios is submodular and thus a bound on greedy algorithms holds~\cite{nof2020real}. 
We can transform the \emph{K-Portfolio Problem} (minimization) into the \emph{K-Algorithms Max-Sum Problem} (maximization) by replacing the costs with utilities as follows.%
$$
u_{T}(P) := c_W - c_{T}(P)
$$
In this transformation $c_W$ denotes an upper bound on portfolio performance, the single worst solver.%
$$
c_W := \max\{c_T(s) \mid s \in S\}
$$
As $u_{T}(P)$ is non-negative, monotone and submodular, we get the following bound on a greedy-search result $P_{greedy}^k$~\cite{nemhauser1978analysis, krause2014submodular}.%
$$
	u_{T}(P_{greedy}^k) \geq (1 - \frac{1}{e}) \cdot \max_{|P| \leq k}{u_{T}(P)}
$$
I.e., there is a lower bound on the utility of a portfolio found by \emph{greedy search}.
This can be transformed into an upper bound on the cost of a portfolio found by \emph{greedy search}.%
\begin{equation}
	c_{T}(P_{greedy}^k) \leq (1 - \frac{1}{e}) \cdot \min_{|P| \leq k}{c_{T}(P)} + \frac{1}{e} \cdot c_W
	\label{eq:upper-bound}
\end{equation}

\subsection{K-Best (approximate)}

This is a baseline used in~\cite{nof2020real}.
It sorts all solvers by their individual performance and then picks the top $k$ from this list.
Thus, opposed to \emph{beam search}, it does not consider how solvers within a portfolio interact, e.g., if they complement each other.

\section{Experimental Design}
\label{sec:experimental-design}

In our experiments, we evaluate the presented solution approaches, and analyze their relation to prediction approaches for $k$-portfolios. 
In particular, we are interested in the achievable performance improvements with increasing values for the parameter $k$. 

\subsection{Solution Approaches}

We employ the four solution approaches from Section~\ref{sec:approach} to search for portfolios:

\begin{itemize}
	\item \emph{Random sampling}:
	To get an idea how the performance of arbitrary portfolios is distributed, we randomly sample 1000 portfolios for each $k$.
	Exhaustively evaluating all portfolios for all sizes $k$ is too expensive in our scenario.
	\item \emph{Optimal solution}:
	We solve the \emph{K-Portfolio Problem} as an integer optimization problem to exactly determine the best portfolio for each $k \in \{1, \dots, |S|\}$.
	\item \emph{Beam search}: 
	We use this approach to search for good portfolios heuristically.
	We evaluate all $k \in \{1, \dots, |S|\}$ and also vary the beam width $w \in \{1, 2, \dots, 10, 20, \dots, 100\}$.
	\item \emph{K-best}:
	We use this approach to have a simple baseline for \emph{beam search}.
	We evaluate all $k \in \{1, \dots, |S|\}$.
\end{itemize}

The optimization goal for all approaches is the PAR-2 score $c_T(P)$.
In preliminary experiments, we also analyzed two slightly different objectives, i.e., number of unsolved instances and PAR-2 score normalized for each instance.
However, overall trends in the results were similar to those with PAR-2 score, so we stick to the latter.

To test generalization of search and prediction approaches, we conduct five-fold cross-validation over SAT instances, and average evaluation metrics over these folds. 
We search for portfolios and train prediction models only on the training sets.

\subsection{Prediction Approaches}

We also analyze the performance of portfolios which use a prediction model to select a solver. 
We train such prediction models for all the portfolios found by our previously described solution approaches. 

For each instance, the prediction target is the best solver out of the given $k$-portfolio.
As models, we use random forests~\cite{breiman2001random}, which are ensembles of decision trees. 
Preliminary experiments with single decision trees~\cite{breiman1984classification} of different depths yielded worse prediction performance. 
Random forests are also used in the well-known portfolio approach SATzilla2012~\cite{xu2012satzilla2012}. 
To analyze if prediction performance improves due to ensembling, we train models with one, ten, and 100 trees.
The prediction approach in SATzilla2012 has two major differences from ours:
First, it trains a classifier for each pair of solvers in the portfolio, while we only train one classifier, making a multi-class prediction.
Second, it weights instances based on the runtime difference between solvers, while we go for an unweighted approach.
However, both these changes did not help to improve prediction performance in preliminary experiments of us, so we opt for the simpler approach instead.

We evaluate prediction performance in two ways:

\begin{itemize}
	\item \emph{Objective value}:
	We evaluate the prediction models with respect to the objective value, i.e., the runtime cost function $c_{T}'(m,P)$ (cf. Section~\ref{sec:preliminaries}). 
	\item \emph{MCC}:
	We evaluate the predictions with Matthews correlation coefficient (MMC)~\cite{matthews1975comparison, gorodkin2004comparing}.
	This does not take into account how fast the recommended solvers actually are, but only if the fastest solver is recommended or not.
	We use MCC instead of simpler metrics like accuracy, as the class labels might be imbalanced, i.e., one solver might be the fastest for most of the instances, and always predicting that solver would already yield a high accuracy.
	MCC has a range of $[-1,1]$, being zero for both random guessing and always guessing the same solver.
\end{itemize}

\subsection{Dataset}

In our experiments, we use two datasets.
The first one, \texttt{SC2020}, contains $|S| = 48$ solvers and $400$ instances from the Main track of the SAT~Competition~2020\footnote{\url{https://satcompetition.github.io/2020/}}~\cite{balyo2020proceedings, SC2020:AIJ}.
We filtered out those instances where no solver finished in time, resulting in a dataset of $|I| = 316$ instances. 
The second dataset, \texttt{SC2021}, contains $|S| = 46$ solvers and $400$ instances from the Main track of the SAT~Competition~2021\footnote{\url{https://satcompetition.github.io/2021/}}.
Again, we filtered out those instances where no solver finished in time, resulting in a dataset of $|I| = 325$ instances.

For predictions, we make use of 143 features to characterize instances, of which 
$138$ features stem from the feature extractor of SATzilla~2012~\cite{xu2008satzilla, xu2012satzilla2012} and five features stem from a gate-recognition approach~\cite{Iser:2015:GateRecognition}. 
Feature values which are missing due to the feature extractor having exceeded time- or memory limits are replaced with their training-set mean. 

\subsection{Implementation}

We implement our experimental design in Python and make our code available online\footnote{\url{https://github.com/Jakob-Bach/Small-Portfolios}}.
\todo{make repo public, adapt URL if necessary}
The code also allows to download and prepare the datasets.
Additionally, we publish the full experimental data, including results\footnote{\url{https://www.ipd.kit.edu/mitarbeiter/bach/k-portfolio-data.tar.gz}}.
\todo{put data on server, adapt URL if necessary}

To create the datasets, we use the package~\emph{gbd-tools}~\cite{iser2020collaborative}.
For predictions, we use the package \emph{scikit-learn}~\cite{scikit-learn}.
To solve the \emph{K-Portfolio Problem} exactly, we use the package \emph{mip}~\cite{python-mip}.

\section{Evaluation}
\label{sec:evaluation}

In the following, we evaluate the performance of the exact and approximate solutions to the \emph{K-Portfolio Problem}. 
We subsequently relate the performance of such optimal $k$-portfolios to the performance of an actual prediction model for instance-specific solver selection. 
In particular, we are interested in the influence of the portfolio size $k$ on performance. 

\subsection{Optimization Results}

The datasets seem promising for portfolios.
In both datasets, there is no single solver which is fastest for all or even for a majority of the instances.
For the $316$ instances in \texttt{SC2020}, the three overall fastest solvers `win' on only 46, 38, and 26 instances, respectively.
For the $325$ instances in \texttt{SC2021}, the three overall fastest solvers `win' on only 25, 22, and 20 instances, respectively.
This indicates that combing solvers in portfolios can improve overall runtime.

\subsubsection{Overall Trend}

\begin{figure*}[t]
	\centering
	\subfloat[\texttt{SC2020} dataset.]{
		\includegraphics[width=0.98\columnwidth]{plots/search-train-objective-2020.pdf}
		\label{fig:search-train-objective-2020}
	}
	\hfil
	\subfloat[\texttt{SC2021} dataset.]{
		\includegraphics[width=0.98\columnwidth]{plots/search-train-objective-2021.pdf}
		\label{fig:search-train-objective-2021}
	}
	\caption{
		Training-set portfolio performance for different $k$ and search algorithms.
	}
	\label{fig:search-train-objective}
\end{figure*}

Figure~\ref{fig:search-train-objective} displays the cost in terms of the PAR-2 function of the best $k$-portfolios for the different solution approaches. 
The \emph{optimal solution} is the exact optimum.
\emph{Greedy search} denotes the performance of the best portfolios found by a \emph{beam search} algorithm with the smallest beam width $w=1$. 
The approach \emph{k-best} displays the performance of portfolios which are comprised by the top $k$ single-best solvers. 
For \emph{random sampling}, we average over the repetitions of sampling.
The \emph{upper bound} limits costs of \emph{greedy search} according to Equation~\ref{eq:upper-bound}.
If we report numbers in the following, we refer to the training dataset for which the optimization problems are solved.

For all approaches and both datasets, the PAR-2 score improves rapidly for the first few $k$, but marginal gains become smaller with increasing $k$.
For the dataset \texttt{SC2020}, the optimal $1$-portfolio has a penalized runtime $5.51$ times as high as the $48$-portfolio.
This ratio reduces to $1.89$ for the best $5$-portfolio and $1.25$ for the best $10$-portfolio.
For \texttt{SC2021}, the respective ratios are $4.74$ for $k=1$, $1.58$ for $k=5$ and $1.17$ for $k=10$.

\subsubsection{Beam Search}

As Figure~\ref{fig:search-train-objective} also shows, the cost of the best $k$-portfolios found by \emph{greedy search} is very close to that of the \emph{optimal solution}.
These results are particularly impressive considering that the runtime of \emph{greedy search} is linear in $k$ as well as the number of instances $n$, while the finding the \emph{optimal solution} is NP-complete.
In contrast, the theoretical, submodularity-based \emph{upper bound} for \emph{greedy search} is clearly higher and therefore too loose to give a good estimate.

To bring the \emph{beam search} solution even closer to the \emph{optimal solution}, one can increase $w$.
For example, the best $2$-portfolio found by \emph{greedy search} has a $14.7\%$ higher cost than the \emph{optimal solution} for \texttt{SC2020}.
For all other $k$, the \emph{greedy search} portfolio has less than $4\%$ higher penalized runtime than the \emph{optimal solution}.
In comparison, for $w=10$ and for all $k$, the PAR-2 score is never more than $2\%$ higher than the \emph{optimal solution}.

Regarding the set of the $w$ best portfolios within each iteration of \emph{beam search}, we observe convergence of portfolio performance with increasing $k$. 
There is a large variance in the PAR-2 score of portfolios in the beam for small $k$, and this variance becomes smaller in later iterations, i.e., the top $w$ portfolios become more similar in performance.
A similar phenomenon also occurs for \emph{random sampling} portfolios:
With increasing $k$, the standard deviation of the PAR-2 score in a sample of portfolios decreases.
Also, the expected value of portfolio performance improves.
Thus, carefully picking the solvers for a portfolio makes the biggest difference for small $k$.

\subsubsection{K-Best}

The baseline \emph{k-best} is worse than \emph{greedy search} on the training data, but better than \emph{random sampling}.
While \emph{greedy search} always is relatively close to the \emph{optimal solution}, the performance gap of \emph{k-best} widens after the first few $k$ and only becomes smaller for large $k$ later.
The performance of \emph{k-best} relative to the other approaches also differs between \texttt{SC2020} and \texttt{SC2021}.
For \texttt{SC2020}, \emph{k-best} is closer to the optimal solution, while for \texttt{SC2021}, \emph{k-best} is closer to the expected value of \emph{random sampling}.
This indicates that building a portfolio of complementary solvers, rather than picking just the best individual solvers, is more important for \texttt{SC2021}.

\subsubsection{Test-Set Performance}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{plots/search-test-objective-2020.pdf}
	\caption{
		Test-set portfolio performance for different $k$ and search algorithms for the \texttt{SC2020} dataset.
	}
	\label{fig:search-test-objective-2020}
\end{figure}

Figure~\ref{fig:search-test-objective-2020} shows test-set portfolio cost for the different solution approaches.
The overall trends are the same as on the training set, displayed in Figure~\ref{fig:search-train-objective}.
As a notable exception, \emph{beam search}, \emph{k-best}, and the \emph{optimal solution} show similar performance on the test data of \texttt{SC2020}.
In particular, there is no clear winner, and the \emph{optimal solution} can even perform worse than those found by the approximate approaches. 
This is because optimization is run on the training instances only, and the best portfolio on the training data is not necessarily the best portfolio on the test data.
For the test data of \texttt{SC2021}, which we do not display here, \emph{k-best} still shows clear difference to \emph{beam search} and the \emph{optimal solution}.
This might be caused by the larger performance gap already on the training set.

\subsubsection{Portfolio Composition}

While \emph{beam search} iteratively adds solvers, the \emph{optimal solution} might differ in more than one solver from $k-1$ to $k$, i.e., existing solvers from the portfolio can be replaced.
Indeed, we observe this phenomenon in our results.
For example, the optimal $2$-portfolios do not contain the optimal $1$-portfolios for both datasets.
However, this non-monotonous behavior is not strong.
On average, between one and two new solvers become part of the optimal portfolio when increasing $k$ by one.
I.e., between zero and one solvers get replaced on average when increasing $k$ by one.
Also, the replaced solver might only be slightly worse than its substitute.
This might explain the good performance of \emph{beam search}, which pursues a monotonous approach when building portfolios.

\subsubsection{Impact of Single Solvers on Portfolios}

A correlation analysis on \emph{random sampling} results, of which we have 1000 portfolios for each $k$, underlines the interaction between solvers.
First, we encode the absence or presence of each solver in a portfolio with 0 or 1 respectively.
Next, we compute the Spearman rank correlation between this occurrence vector with the PAR-2 score.
For $k=5$, all correlations are in $[-0.40,0.17]$ for \texttt{SC2020}, and $[-0.24,0.23]$ for \texttt{SC2021}.
The mean correlation is zero in both cases.
Similar correlation behavior occurs for other $k$.

These results indicate that only the presence of some solvers has a moderately negative correlation to the PAR-2 score of the whole portfolio, i.e., only some solvers can improve the portfolio performance of our minimization problem on their own.
This effect is stronger for \texttt{SC2021}.
The positive correlations are even weaker, i.e., there are no solvers which influence portfolio performance in a strongly negative manner.
Overall, this means that the influence of single solvers on portfolio performance is limited; one needs a combination of solvers to clearly influence the PAR-2 score in either direction.

\subsection{Prediction Results}

In this section, we evaluate the prediction performance for making instance-specific solver predictions in the previously determined portfolios. 
We also evaluate the PAR-2 score of these portfolios with prediction model. 

\subsubsection{Matthews Correlation Coefficient}

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{plots/prediction-test-mcc.pdf}
	\caption{
		Test-set MCC for \emph{random sampling} portfolios, using random forests with 100 trees.
		(MCC is 0 for random or constant prediction and it is 1 for perfect prediction.)
		For readability reasons, we do not plot outlier points.
	}
	\label{fig:prediction-test-mcc}
\end{figure}

Figure~\ref{fig:prediction-test-mcc} shows test-set classification performance of random forests with 100 trees, using the portfolios from \emph{random sampling}.
Classification performance with less trees is worse, on training set as well as test set.
We use \emph{random sampling} results here, to show the variation of prediction performance over a large set of portfolios.
However, classification results are similar for \emph{beam search} with $w=100$ and for the \emph{optimal solution}.
We do not display training-set performance, since it is close to 1.0.

As Figure~\ref{fig:prediction-test-mcc} shows, test-set prediction performance is rather low for all $k$.
It is clearly higher than random guessing, which would yield an MCC of 0.0, but clearly lower than the optimal MCC of 1.0.
With a nearly perfect training-set performance, but a low test set performance, the model seems to be overfitting to the training set.
This could cause a bad PAR-2 value for model-based portfolios, as we will analyze next.
For low $k$, the prediction performance varies stronger between portfolios than for larger $k$.

\subsubsection{Portfolio Performance}

\begin{figure*}[t]
	\centering
	\subfloat[\texttt{SC2020} dataset.]{
		\includegraphics[width=0.98\columnwidth]{plots/prediction-test-objective-beam-2020.pdf}
		\label{fig:prediction-test-objective-beam-2020}
	}
	\hfil
	\subfloat[\texttt{SC2021} dataset.]{
		\includegraphics[width=0.98\columnwidth]{plots/prediction-test-objective-beam-2021.pdf}
		\label{fig:prediction-test-objective-beam-2021}
	}
	\caption{
		Portfolio performance for \emph{beam search} portfolios with $w=100$, comparing the portfolio's virtual best solver (VBS), single best solver (SBS), and score with prediction model.
		Prediction model: random forests with 100 trees.
		The performance of the the global SBS, i.e., not only considering solvers from the portfolio, is shown as horizontal line.
	}
	\label{fig:prediction-test-objective-beam}
\end{figure*}

Figure~\ref{fig:prediction-test-objective-beam} shows test-set PAR-2 score for portfolios from \emph{beam search} with $w=100$.
The plot compares the PAR-2 score of portfolios with prediction model to two competitors without prediction model, computed on the same portfolios.
The virtual best solver (VBS) provides a lower bound, as its portfolio performance can only be achieved with perfect prediction.
The single best solver (SBS) of each portfolio serves as a baseline, being equal to a constant prediction.
Note that the portfolio performance with prediction models can even be worse than the SBS, e.g., when always predicting the slowest solver for each instance.
However, we do not show this upper bound on PAR-2 score here.

As discussed before, VBS score decreases with $k$.
In theory, this also allows portfolios with prediction model to potentially improve their performance.
However, in our case, the PAR-2 score of model-based portfolios remains rather stable with increasing values of $k$, with the biggest improvement from $k=2$ to $k=3$.
This implies that the prediction model cannot help to benefit the larger amount of solvers in the portfolio with increasing $k$.
Given the low prediction performance in terms of MCC, as previously seen in Figure~\ref{fig:prediction-test-mcc}, this is expected.
As a consequence, the gap between VBS and the predicted solver grows with $k$.
However, the model-based portfolios are usually at least better than the single best solver from these portfolios, i.e., the prediction models can discriminate between solvers to some extent.
Also, for $k > 2$ the model-based portfolios are on average better than the global single-best solver, which might not be part of the portfolio.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{plots/prediction-test-objective-optimal-2020.pdf}
	\caption{
		Portfolio performance for \emph{optimal solution} portfolios, comparing the portfolio's virtual best solver (VBS), single best solver (SBS), and score with prediction model for the \texttt{SC2020} dataset.
		Prediction model: random forests with 100 trees.
		The performance of the the global SBS, i.e., not only considering solvers from the portfolio, is shown as horizontal line.
	}
	\label{fig:prediction-test-objective-optimal-2020}
\end{figure}

Figure~\ref{fig:prediction-test-objective-optimal-2020} repeats the same comparison as before for the \emph{optimal solution} portfolios.
Overall trends remain the same.
As a difference to Figure~\ref{fig:prediction-test-objective-beam}, the are only five portfolios for each $k$ here, one for each fold of cross-validation.
We still see a considerable variance for each $k$, i.e., the test set performance also depends on the current split of instances.

\subsubsection{Feature Importance}

Averaging importance over all trained models, the most important feature has an importance of roughly 2\%, the least important feature has an importance of 0\%.
To reach an cumulated average importance of 50\%, one needs 40 out of 143 features.
On average, prediction models use 109 features.
If we limit our analysis to random forests with just one tree, the models still use 59 features on average.
In conclusion, no single feature or even small set of features drives prediction performance.
Together with the low prediction performance, this indicates that our dataset lacks discriminating features which are suitable to decide which solver should be used on an instance.

\section{Conclusions and Future Work}
\label{sec:conclusion}

SAT solvers are often complementary, i.e, they are particularly good on some instances and less good on other instances.
Thus, one combines several solvers into portfolios.
We analyzed such portfolios with runtime data from the SAT~Competitions~2020 and 2021.
In particular, we focused on portfolios with the number of solvers limited to $k$.

We analyzed an integer optimization problem to find $k$-portfolios exactly as well as a \emph{beam search} approach to find $k$-portfolios fast.
Regarding portfolio size, we saw a strong improvement in objective value for adding solvers to portfolios with small $k$, but a small impact once the portfolio reaches a certain size.

Regarding the solution approach, we found that \emph{beam search} yields close-to-optimal solutions.
After determining portfolios, we combined them with prediction models that made instance-specific solver recommendations.
However, these models performed not satisfactorily on the given dataset, such that the objective value of portfolios with predictions did not continuously improve with the portfolio size.

In future work, we want to improve prediction performance, in particular, by adapting the set of instance features.
Recent results in the empirical analysis of solver portfolios indicate that features which describe the community structure of graph representations of SAT instances correlate well with solver performance. 
Among those are features describing their hierarchical community structure, such as community leaf size, community degree, and the number of inter-community edges~\cite{Li:2021:HCS}. 
Also, we want to integrate our portfolio-search functionality into the Python package~\emph{gbd-tools}, so it can be directly used in queries to the database GBD.

\section*{Acknowledgments}

\todo{check if Jakob and/or Markus need to add stuff here}

\balance % according to documentation, command might not work if issued to late in document, so hopefully placing it here suffices

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
